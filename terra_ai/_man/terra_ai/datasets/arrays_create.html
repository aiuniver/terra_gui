<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>terra_ai.datasets.arrays_create API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>terra_ai.datasets.arrays_create</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import cv2
import numpy as np

from sklearn.cluster import KMeans
from gensim.models.word2vec import Word2Vec
from tqdm.notebook import tqdm

from tensorflow.keras.layers.experimental.preprocessing import Resizing
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import utils


class CreateArray(object):

    def __init__(self):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.augmentation: dict = {}

        self.file_folder = None
        self.txt_list: dict = {}

    def create_images(self, image_path: str, **options):

        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
        array = img_to_array(img, dtype=np.uint8)
        if options[&#39;net&#39;] == &#39;Linear&#39;:
            array = array.reshape(np.prod(np.array(array.shape)))

        return array

    def create_video(self, video_path, **options) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            video_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота кадра.
                width: int
                    Ширина кадра.
                max_frames: int
                    Максимальное количество кадров.
                mode: str
                    Режим обработки кадра (Сохранить пропорции, Растянуть).
                x_len: int
                    Длина окна выборки.
                step: int
                    Шаг окна выборки.

        Returns:
            array: np.ndarray
                Массив видео.

        &#34;&#34;&#34;

        def resize_frame(one_frame, original_shape, target_shape, mode):

            resized = None

            if mode == &#39;Растянуть&#39;:
                resized = resize_layer(one_frame[None, ...])
                resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
            elif mode == &#39;Сохранить пропорции&#39;:
                # height
                resized = one_frame.copy()
                if original_shape[0] &gt; target_shape[0]:
                    resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
                else:
                    black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3), dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized))
                    resized = np.concatenate((resized, black_bar))
                # width
                if original_shape[1] &gt; target_shape[1]:
                    resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
                else:
                    black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3), dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized), axis=1)
                    resized = np.concatenate((resized, black_bar), axis=1)

            # resized = resized.numpy().squeeze()

            return resized

        array = []
        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        resize_layer = Resizing(*shape)

        cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
        height = int(cap.get(4))
        width = int(cap.get(3))
        # fps = int(cap.get(5))
        frame_count = int(cap.get(7))
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                if shape != (height, width):
                    frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
                frame = frame[:, :, [2, 1, 0]]
                array.append(frame)
                if len(array) == options[&#39;max_frames&#39;]:
                    break
        finally:
            cap.release()

        array = np.array(array)
        if frame_count &lt; options[&#39;max_frames&#39;]:
            add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
            array = np.concatenate((array, add_frames), axis=0)

        return array

    def create_text(self, sample: dict, **options):

        &#34;&#34;&#34;

        Args:
            sample: dict
                - file: Название файла.
                - slice: Индексы рассматриваемой части последовательности
            **options: Параметры обработки текста:
                bag_of_words: Tokenizer object, bool
                    Перевод в формат bag_of_words.
                word_to_vec: Word2Vec object, bool
                    Перевод в векторное представление Word2Vec.
                put: str
                    Индекс входа или выхода.

        Returns:
            array: np.ndarray
                Массив текстового вектора.
        &#34;&#34;&#34;

        filepath: str = sample[&#39;file&#39;]
        slicing: list = sample[&#39;slice&#39;]
        array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

        for key, value in options.items():
            if value:
                if key == &#39;bag_of_words&#39;:
                    array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
                elif key == &#39;word_to_vec&#39;:
                    reverse_tok = {}
                    words_list = []
                    for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                        reverse_tok[index] = word
                    for idx in array:
                        words_list.append(reverse_tok[idx])
                    array = []
                    for word in words_list:
                        array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
                break

        array = np.array(array)

        return array

    def create_audio(self):

        pass

    def create_dataframe(self):

        pass

    def create_classification(self, index, **options):

        if options[&#39;one_hot_encoding&#39;]:
            index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
        index = np.array(index)

        return index

    def create_regression(self):

        pass

    def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            image_path: str
                Путь к файлу
            **options: Параметры сегментации:
                mask_range: int
                    Диапазон для каждого из RGB каналов.
                num_classes: int
                    Общее количество классов.
                shape: tuple
                    Размер картинки (высота, ширина).
                classes_colors: list
                    Список цветов для каждого класса.

        Returns:
            array: np.ndarray
                Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

        &#34;&#34;&#34;

        def cluster_to_ohe(mask_image):

            mask_image = mask_image.reshape(-1, 3)
            km = KMeans(n_clusters=options[&#39;num_classes&#39;])
            km.fit(mask_image)
            labels = km.labels_
            cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
            cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
            cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

            mask_ohe = np.zeros(options[&#39;shape&#39;])
            for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
                mask = np.zeros(options[&#39;shape&#39;])

                for j, cl_rgb in enumerate(cl_cent):
                    if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                            rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                            rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                        mask = cl_mask[:, :, j]

                if k == 0:
                    mask_ohe = mask
                else:
                    mask_ohe = np.dstack((mask_ohe, mask))

            return mask_ohe

        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
        array = img_to_array(img, dtype=np.uint8)
        array = cluster_to_ohe(array)

        return array

    def create_text_segmentation(self, sample: dict, **options):

        array = []

        for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
            tags = [0 for _ in range(options[&#39;num_classes&#39;])]
            if elem:
                for idx in elem:
                    tags[idx] = 1
            array.append(tags)
        array = np.array(array, dtype=&#39;uint8&#39;)

        return array

    def create_timeseries(self):

        pass

    def create_object_detection(self, txt_path: str, **options):

        &#34;&#34;&#34;

        Args:
            txt_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота изображения.
                width: int
                    Ширина изображения.
                num_classes: tuple
                    Количество классов.

        Returns:
            array: np.ndarray
                Массивы в трёх выходах.

        &#34;&#34;&#34;

        height: int = options[&#39;height&#39;]
        width: int = options[&#39;width&#39;]
        num_classes: int = options[&#39;num_classes&#39;]

        with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
            bb_file = txt.read()
        real_boxes = []
        for elem in bb_file.split(&#39;\n&#39;):
            tmp = []
            if elem:
                for num in elem.split(&#39; &#39;):
                    tmp.append(float(num))
                real_boxes.append(tmp)
        real_boxes = np.array(real_boxes)
        real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
        anchors = np.array(
            [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
        num_layers = 3
        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

        real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
        input_shape = np.array((height, width), dtype=&#39;int32&#39;)

        boxes_wh = real_boxes[..., 2:4] * input_shape

        cells = [13, 26, 52]
        y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
                  range(num_layers)]
        box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

        anchor_area = anchors[:, 0] * anchors[:, 1]
        for r in range(len(real_boxes)):
            correct_anchors = []
            for anchor in anchors:
                correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
            correct_anchors = np.array(correct_anchors)
            correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
            iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
            best_anchor = np.argmax(iou, axis=-1)

            for m in range(num_layers):
                if best_anchor in anchor_mask[m]:
                    h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                    j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                    k = anchor_mask[m].index(int(best_anchor))
                    c = real_boxes[r, 4].astype(&#39;int32&#39;)
                    y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                    y_true[m][j, h, k, 4] = 1
                    y_true[m][j, h, k, 5 + c] = 1
                    break

        return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            words: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="terra_ai.datasets.arrays_create.CreateArray"><code class="flex name class">
<span>class <span class="ident">CreateArray</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CreateArray(object):

    def __init__(self):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.augmentation: dict = {}

        self.file_folder = None
        self.txt_list: dict = {}

    def create_images(self, image_path: str, **options):

        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
        array = img_to_array(img, dtype=np.uint8)
        if options[&#39;net&#39;] == &#39;Linear&#39;:
            array = array.reshape(np.prod(np.array(array.shape)))

        return array

    def create_video(self, video_path, **options) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            video_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота кадра.
                width: int
                    Ширина кадра.
                max_frames: int
                    Максимальное количество кадров.
                mode: str
                    Режим обработки кадра (Сохранить пропорции, Растянуть).
                x_len: int
                    Длина окна выборки.
                step: int
                    Шаг окна выборки.

        Returns:
            array: np.ndarray
                Массив видео.

        &#34;&#34;&#34;

        def resize_frame(one_frame, original_shape, target_shape, mode):

            resized = None

            if mode == &#39;Растянуть&#39;:
                resized = resize_layer(one_frame[None, ...])
                resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
            elif mode == &#39;Сохранить пропорции&#39;:
                # height
                resized = one_frame.copy()
                if original_shape[0] &gt; target_shape[0]:
                    resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
                else:
                    black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3), dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized))
                    resized = np.concatenate((resized, black_bar))
                # width
                if original_shape[1] &gt; target_shape[1]:
                    resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
                else:
                    black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3), dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized), axis=1)
                    resized = np.concatenate((resized, black_bar), axis=1)

            # resized = resized.numpy().squeeze()

            return resized

        array = []
        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        resize_layer = Resizing(*shape)

        cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
        height = int(cap.get(4))
        width = int(cap.get(3))
        # fps = int(cap.get(5))
        frame_count = int(cap.get(7))
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                if shape != (height, width):
                    frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
                frame = frame[:, :, [2, 1, 0]]
                array.append(frame)
                if len(array) == options[&#39;max_frames&#39;]:
                    break
        finally:
            cap.release()

        array = np.array(array)
        if frame_count &lt; options[&#39;max_frames&#39;]:
            add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
            array = np.concatenate((array, add_frames), axis=0)

        return array

    def create_text(self, sample: dict, **options):

        &#34;&#34;&#34;

        Args:
            sample: dict
                - file: Название файла.
                - slice: Индексы рассматриваемой части последовательности
            **options: Параметры обработки текста:
                bag_of_words: Tokenizer object, bool
                    Перевод в формат bag_of_words.
                word_to_vec: Word2Vec object, bool
                    Перевод в векторное представление Word2Vec.
                put: str
                    Индекс входа или выхода.

        Returns:
            array: np.ndarray
                Массив текстового вектора.
        &#34;&#34;&#34;

        filepath: str = sample[&#39;file&#39;]
        slicing: list = sample[&#39;slice&#39;]
        array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

        for key, value in options.items():
            if value:
                if key == &#39;bag_of_words&#39;:
                    array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
                elif key == &#39;word_to_vec&#39;:
                    reverse_tok = {}
                    words_list = []
                    for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                        reverse_tok[index] = word
                    for idx in array:
                        words_list.append(reverse_tok[idx])
                    array = []
                    for word in words_list:
                        array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
                break

        array = np.array(array)

        return array

    def create_audio(self):

        pass

    def create_dataframe(self):

        pass

    def create_classification(self, index, **options):

        if options[&#39;one_hot_encoding&#39;]:
            index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
        index = np.array(index)

        return index

    def create_regression(self):

        pass

    def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            image_path: str
                Путь к файлу
            **options: Параметры сегментации:
                mask_range: int
                    Диапазон для каждого из RGB каналов.
                num_classes: int
                    Общее количество классов.
                shape: tuple
                    Размер картинки (высота, ширина).
                classes_colors: list
                    Список цветов для каждого класса.

        Returns:
            array: np.ndarray
                Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

        &#34;&#34;&#34;

        def cluster_to_ohe(mask_image):

            mask_image = mask_image.reshape(-1, 3)
            km = KMeans(n_clusters=options[&#39;num_classes&#39;])
            km.fit(mask_image)
            labels = km.labels_
            cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
            cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
            cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

            mask_ohe = np.zeros(options[&#39;shape&#39;])
            for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
                mask = np.zeros(options[&#39;shape&#39;])

                for j, cl_rgb in enumerate(cl_cent):
                    if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                            rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                            rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                        mask = cl_mask[:, :, j]

                if k == 0:
                    mask_ohe = mask
                else:
                    mask_ohe = np.dstack((mask_ohe, mask))

            return mask_ohe

        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
        array = img_to_array(img, dtype=np.uint8)
        array = cluster_to_ohe(array)

        return array

    def create_text_segmentation(self, sample: dict, **options):

        array = []

        for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
            tags = [0 for _ in range(options[&#39;num_classes&#39;])]
            if elem:
                for idx in elem:
                    tags[idx] = 1
            array.append(tags)
        array = np.array(array, dtype=&#39;uint8&#39;)

        return array

    def create_timeseries(self):

        pass

    def create_object_detection(self, txt_path: str, **options):

        &#34;&#34;&#34;

        Args:
            txt_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота изображения.
                width: int
                    Ширина изображения.
                num_classes: tuple
                    Количество классов.

        Returns:
            array: np.ndarray
                Массивы в трёх выходах.

        &#34;&#34;&#34;

        height: int = options[&#39;height&#39;]
        width: int = options[&#39;width&#39;]
        num_classes: int = options[&#39;num_classes&#39;]

        with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
            bb_file = txt.read()
        real_boxes = []
        for elem in bb_file.split(&#39;\n&#39;):
            tmp = []
            if elem:
                for num in elem.split(&#39; &#39;):
                    tmp.append(float(num))
                real_boxes.append(tmp)
        real_boxes = np.array(real_boxes)
        real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
        anchors = np.array(
            [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
        num_layers = 3
        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

        real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
        input_shape = np.array((height, width), dtype=&#39;int32&#39;)

        boxes_wh = real_boxes[..., 2:4] * input_shape

        cells = [13, 26, 52]
        y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
                  range(num_layers)]
        box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

        anchor_area = anchors[:, 0] * anchors[:, 1]
        for r in range(len(real_boxes)):
            correct_anchors = []
            for anchor in anchors:
                correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
            correct_anchors = np.array(correct_anchors)
            correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
            iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
            best_anchor = np.argmax(iou, axis=-1)

            for m in range(num_layers):
                if best_anchor in anchor_mask[m]:
                    h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                    j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                    k = anchor_mask[m].index(int(best_anchor))
                    c = real_boxes[r, 4].astype(&#39;int32&#39;)
                    y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                    y_true[m][j, h, k, 4] = 1
                    y_true[m][j, h, k, 5 + c] = 1
                    break

        return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            words: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_audio"><code class="name flex">
<span>def <span class="ident">create_audio</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_audio(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_classification"><code class="name flex">
<span>def <span class="ident">create_classification</span></span>(<span>self, index, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_classification(self, index, **options):

    if options[&#39;one_hot_encoding&#39;]:
        index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
    index = np.array(index)

    return index</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_dataframe"><code class="name flex">
<span>def <span class="ident">create_dataframe</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dataframe(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_images"><code class="name flex">
<span>def <span class="ident">create_images</span></span>(<span>self, image_path: str, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_images(self, image_path: str, **options):

    shape = (options[&#39;height&#39;], options[&#39;width&#39;])
    img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
    array = img_to_array(img, dtype=np.uint8)
    if options[&#39;net&#39;] == &#39;Linear&#39;:
        array = array.reshape(np.prod(np.array(array.shape)))

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_object_detection"><code class="name flex">
<span>def <span class="ident">create_object_detection</span></span>(<span>self, txt_path: str, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>txt_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
height: int
Высота изображения.
width: int
Ширина изображения.
num_classes: tuple
Количество классов.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массивы в трёх выходах.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_object_detection(self, txt_path: str, **options):

    &#34;&#34;&#34;

    Args:
        txt_path: str
            Путь к файлу
        **options: Параметры сегментации:
            height: int
                Высота изображения.
            width: int
                Ширина изображения.
            num_classes: tuple
                Количество классов.

    Returns:
        array: np.ndarray
            Массивы в трёх выходах.

    &#34;&#34;&#34;

    height: int = options[&#39;height&#39;]
    width: int = options[&#39;width&#39;]
    num_classes: int = options[&#39;num_classes&#39;]

    with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
        bb_file = txt.read()
    real_boxes = []
    for elem in bb_file.split(&#39;\n&#39;):
        tmp = []
        if elem:
            for num in elem.split(&#39; &#39;):
                tmp.append(float(num))
            real_boxes.append(tmp)
    real_boxes = np.array(real_boxes)
    real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
    anchors = np.array(
        [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
    num_layers = 3
    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

    real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
    input_shape = np.array((height, width), dtype=&#39;int32&#39;)

    boxes_wh = real_boxes[..., 2:4] * input_shape

    cells = [13, 26, 52]
    y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
              range(num_layers)]
    box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

    anchor_area = anchors[:, 0] * anchors[:, 1]
    for r in range(len(real_boxes)):
        correct_anchors = []
        for anchor in anchors:
            correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
        correct_anchors = np.array(correct_anchors)
        correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
        iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
        best_anchor = np.argmax(iou, axis=-1)

        for m in range(num_layers):
            if best_anchor in anchor_mask[m]:
                h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                k = anchor_mask[m].index(int(best_anchor))
                c = real_boxes[r, 4].astype(&#39;int32&#39;)
                y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                y_true[m][j, h, k, 4] = 1
                y_true[m][j, h, k, 5 + c] = 1
                break

    return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_regression"><code class="name flex">
<span>def <span class="ident">create_regression</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_regression(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_scaler"><code class="name flex">
<span>def <span class="ident">create_scaler</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_scaler(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_segmentation"><code class="name flex">
<span>def <span class="ident">create_segmentation</span></span>(<span>self, image_path: str, **options: dict) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>image_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
mask_range: int
Диапазон для каждого из RGB каналов.
num_classes: int
Общее количество классов.
shape: tuple
Размер картинки (высота, ширина).
classes_colors: list
Список цветов для каждого класса.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

    &#34;&#34;&#34;

    Args:
        image_path: str
            Путь к файлу
        **options: Параметры сегментации:
            mask_range: int
                Диапазон для каждого из RGB каналов.
            num_classes: int
                Общее количество классов.
            shape: tuple
                Размер картинки (высота, ширина).
            classes_colors: list
                Список цветов для каждого класса.

    Returns:
        array: np.ndarray
            Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

    &#34;&#34;&#34;

    def cluster_to_ohe(mask_image):

        mask_image = mask_image.reshape(-1, 3)
        km = KMeans(n_clusters=options[&#39;num_classes&#39;])
        km.fit(mask_image)
        labels = km.labels_
        cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
        cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
        cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

        mask_ohe = np.zeros(options[&#39;shape&#39;])
        for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
            mask = np.zeros(options[&#39;shape&#39;])

            for j, cl_rgb in enumerate(cl_cent):
                if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                        rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                        rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                    mask = cl_mask[:, :, j]

            if k == 0:
                mask_ohe = mask
            else:
                mask_ohe = np.dstack((mask_ohe, mask))

        return mask_ohe

    img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
    array = img_to_array(img, dtype=np.uint8)
    array = cluster_to_ohe(array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_text"><code class="name flex">
<span>def <span class="ident">create_text</span></span>(<span>self, sample: dict, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong></dt>
<dd>dict
- file: Название файла.
- slice: Индексы рассматриваемой части последовательности</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры обработки текста:
bag_of_words: Tokenizer object, bool
Перевод в формат bag_of_words.
word_to_vec: Word2Vec object, bool
Перевод в векторное представление Word2Vec.
put: str
Индекс входа или выхода.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив текстового вектора.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_text(self, sample: dict, **options):

    &#34;&#34;&#34;

    Args:
        sample: dict
            - file: Название файла.
            - slice: Индексы рассматриваемой части последовательности
        **options: Параметры обработки текста:
            bag_of_words: Tokenizer object, bool
                Перевод в формат bag_of_words.
            word_to_vec: Word2Vec object, bool
                Перевод в векторное представление Word2Vec.
            put: str
                Индекс входа или выхода.

    Returns:
        array: np.ndarray
            Массив текстового вектора.
    &#34;&#34;&#34;

    filepath: str = sample[&#39;file&#39;]
    slicing: list = sample[&#39;slice&#39;]
    array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

    for key, value in options.items():
        if value:
            if key == &#39;bag_of_words&#39;:
                array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
            elif key == &#39;word_to_vec&#39;:
                reverse_tok = {}
                words_list = []
                for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                    reverse_tok[index] = word
                for idx in array:
                    words_list.append(reverse_tok[idx])
                array = []
                for word in words_list:
                    array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
            break

    array = np.array(array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_text_segmentation"><code class="name flex">
<span>def <span class="ident">create_text_segmentation</span></span>(<span>self, sample: dict, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_text_segmentation(self, sample: dict, **options):

    array = []

    for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
        tags = [0 for _ in range(options[&#39;num_classes&#39;])]
        if elem:
            for idx in elem:
                tags[idx] = 1
        array.append(tags)
    array = np.array(array, dtype=&#39;uint8&#39;)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_timeseries"><code class="name flex">
<span>def <span class="ident">create_timeseries</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_timeseries(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_tokenizer"><code class="name flex">
<span>def <span class="ident">create_tokenizer</span></span>(<span>self, mode: str, iteration: int, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры токенайзера:
num_words: int
Количество слов для токенайзера.
filters: str
Символы, подлежащие удалению.
lower: bool
Перевод заглавных букв в строчные.
split: str
Символ разделения.
char_level: bool
Учёт каждого символа в качестве отдельного токена.
oov_token: str
В случае указания этот токен будет заменять все слова, не попавшие в
диапазон частотности слов 0 &lt; num_words.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Токенайзер.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tokenizer(self, mode: str, iteration: int, **options):

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        **options: Параметры токенайзера:
                   num_words: int
                       Количество слов для токенайзера.
                   filters: str
                       Символы, подлежащие удалению.
                   lower: bool
                       Перевод заглавных букв в строчные.
                   split: str
                       Символ разделения.
                   char_level: bool
                       Учёт каждого символа в качестве отдельного токена.
                   oov_token: str
                       В случае указания этот токен будет заменять все слова, не попавшие в
                       диапазон частотности слов 0 &lt; num_words.

    Returns:
        Объект Токенайзер.

    &#34;&#34;&#34;

    self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_video"><code class="name flex">
<span>def <span class="ident">create_video</span></span>(<span>self, video_path, **options) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>video_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
height: int
Высота кадра.
width: int
Ширина кадра.
max_frames: int
Максимальное количество кадров.
mode: str
Режим обработки кадра (Сохранить пропорции, Растянуть).
x_len: int
Длина окна выборки.
step: int
Шаг окна выборки.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив видео.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_video(self, video_path, **options) -&gt; np.ndarray:

    &#34;&#34;&#34;

    Args:
        video_path: str
            Путь к файлу
        **options: Параметры сегментации:
            height: int
                Высота кадра.
            width: int
                Ширина кадра.
            max_frames: int
                Максимальное количество кадров.
            mode: str
                Режим обработки кадра (Сохранить пропорции, Растянуть).
            x_len: int
                Длина окна выборки.
            step: int
                Шаг окна выборки.

    Returns:
        array: np.ndarray
            Массив видео.

    &#34;&#34;&#34;

    def resize_frame(one_frame, original_shape, target_shape, mode):

        resized = None

        if mode == &#39;Растянуть&#39;:
            resized = resize_layer(one_frame[None, ...])
            resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
        elif mode == &#39;Сохранить пропорции&#39;:
            # height
            resized = one_frame.copy()
            if original_shape[0] &gt; target_shape[0]:
                resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
            else:
                black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3), dtype=&#39;uint8&#39;)
                resized = np.concatenate((black_bar, resized))
                resized = np.concatenate((resized, black_bar))
            # width
            if original_shape[1] &gt; target_shape[1]:
                resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
            else:
                black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3), dtype=&#39;uint8&#39;)
                resized = np.concatenate((black_bar, resized), axis=1)
                resized = np.concatenate((resized, black_bar), axis=1)

        # resized = resized.numpy().squeeze()

        return resized

    array = []
    shape = (options[&#39;height&#39;], options[&#39;width&#39;])
    resize_layer = Resizing(*shape)

    cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
    height = int(cap.get(4))
    width = int(cap.get(3))
    # fps = int(cap.get(5))
    frame_count = int(cap.get(7))
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if shape != (height, width):
                frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
            frame = frame[:, :, [2, 1, 0]]
            array.append(frame)
            if len(array) == options[&#39;max_frames&#39;]:
                break
    finally:
        cap.release()

    array = np.array(array)
    if frame_count &lt; options[&#39;max_frames&#39;]:
        add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
        array = np.concatenate((array, add_frames), axis=0)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.create_word2vec"><code class="name flex">
<span>def <span class="ident">create_word2vec</span></span>(<span>self, mode: str, iteration: int, words: list, **options) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>words</code></strong></dt>
<dd>list
Список слов для обучения Word2Vec.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры Word2Vec:
size: int
Dimensionality of the word vectors.
window: int
Maximum distance between the current and predicted word within a sentence.
min_count: int
Ignores all words with total frequency lower than this.
workers: int
Use these many worker threads to train the model (=faster training with multicore machines).
iter: int
Number of iterations (epochs) over the corpus.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Word2Vec.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        words: list
            Список слов для обучения Word2Vec.
        **options: Параметры Word2Vec:
                   size: int
                       Dimensionality of the word vectors.
                   window: int
                       Maximum distance between the current and predicted word within a sentence.
                   min_count: int
                       Ignores all words with total frequency lower than this.
                   workers: int
                       Use these many worker threads to train the model (=faster training with multicore machines).
                   iter: int
                       Number of iterations (epochs) over the corpus.

    Returns:
        Объект Word2Vec.

    &#34;&#34;&#34;

    self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.datasets.arrays_create.CreateArray.inverse_data"><code class="name flex">
<span>def <span class="ident">inverse_data</span></span>(<span>self, put: str, array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>put</code></strong></dt>
<dd>str
Рассматриваемый вход или выход (input_2, output_1);</dd>
<dt><strong><code>array</code></strong></dt>
<dd>np.ndarray
NumPy массив, подлежащий возврату в исходное состояние.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Данные в исходном состоянии.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_data(self, put: str, array: np.ndarray):

    &#34;&#34;&#34;

    Args:
        put: str
            Рассматриваемый вход или выход (input_2, output_1);
        array: np.ndarray
            NumPy массив, подлежащий возврату в исходное состояние.

    Returns:
        Данные в исходном состоянии.

    &#34;&#34;&#34;

    inverted_data = None

    for attr in self.__dict__.keys():
        if self.__dict__[attr] and put in self.__dict__[attr].keys():
            if attr == &#39;tokenizer&#39;:
                if array.shape[0] == self.tokenizer[put].num_words:
                    idx = 0
                    arr = []
                    for num in array:
                        if num == 1:
                            arr.append(idx)
                        idx += 1
                    array = np.array(arr)
                inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

            elif attr == &#39;word2vec&#39;:
                text_list = []
                for i in range(len(array)):
                    text_list.append(
                        self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                inverted_data = &#39; &#39;.join(text_list)

            elif attr == &#39;scaler&#39;:
                original_shape = array.shape
                array = array.reshape(-1, 1)
                array = self.scaler[put].inverse_transform(array)
                inverted_data = array.reshape(original_shape)
        break

    return inverted_data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="terra_ai.datasets" href="index.html">terra_ai.datasets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="terra_ai.datasets.arrays_create.CreateArray" href="#terra_ai.datasets.arrays_create.CreateArray">CreateArray</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_audio" href="#terra_ai.datasets.arrays_create.CreateArray.create_audio">create_audio</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_classification" href="#terra_ai.datasets.arrays_create.CreateArray.create_classification">create_classification</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_dataframe" href="#terra_ai.datasets.arrays_create.CreateArray.create_dataframe">create_dataframe</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_images" href="#terra_ai.datasets.arrays_create.CreateArray.create_images">create_images</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_object_detection" href="#terra_ai.datasets.arrays_create.CreateArray.create_object_detection">create_object_detection</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_regression" href="#terra_ai.datasets.arrays_create.CreateArray.create_regression">create_regression</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_scaler" href="#terra_ai.datasets.arrays_create.CreateArray.create_scaler">create_scaler</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_segmentation" href="#terra_ai.datasets.arrays_create.CreateArray.create_segmentation">create_segmentation</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_text" href="#terra_ai.datasets.arrays_create.CreateArray.create_text">create_text</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_text_segmentation" href="#terra_ai.datasets.arrays_create.CreateArray.create_text_segmentation">create_text_segmentation</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_timeseries" href="#terra_ai.datasets.arrays_create.CreateArray.create_timeseries">create_timeseries</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_tokenizer" href="#terra_ai.datasets.arrays_create.CreateArray.create_tokenizer">create_tokenizer</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_video" href="#terra_ai.datasets.arrays_create.CreateArray.create_video">create_video</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.create_word2vec" href="#terra_ai.datasets.arrays_create.CreateArray.create_word2vec">create_word2vec</a></code></li>
<li><code><a title="terra_ai.datasets.arrays_create.CreateArray.inverse_data" href="#terra_ai.datasets.arrays_create.CreateArray.inverse_data">inverse_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>