<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>terra_ai.customLayers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>terra_ai.customLayers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy
from typing import Optional, Dict, Any, Union, Tuple
import tensorflow as tf
import tensorflow
from tensorflow.keras.layers import Layer, InputSpec
from tensorflow.keras import initializers, regularizers, constraints
from tensorflow.keras import backend as K
from tensorflow.keras import layers

__version__ = 0.02


class InstanceNormalization(Layer):
    &#34;&#34;&#34;Instance normalization layer.
    Normalize the activations of the previous layer at each step,
    i.e. applies a transformation that maintains the mean activation
    close to 0 and the activation standard deviation close to 1.
    # Arguments
        axis: Integer, the axis that should be normalized
            (typically the features axis).
            For instance, after a `Conv2D` layer with
            `data_format=&#34;channels_first&#34;`,
            set `axis=1` in `InstanceNormalization`.
            Setting `axis=None` will normalize all values in each
            instance of the batch.
            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.
        epsilon: Small float added to variance to avoid dividing by zero.
        center: If True, add offset of `beta` to normalized tensor.
            If False, `beta` is ignored.
        scale: If True, multiply by `gamma`.
            If False, `gamma` is not used.
            When the next layer is linear (also e.g. `nn.relu`),
            this can be disabled since the scaling
            will be done by the next layer.
        beta_initializer: Initializer for the beta weight.
        gamma_initializer: Initializer for the gamma weight.
        beta_regularizer: Optional regularizer for the beta weight.
        gamma_regularizer: Optional regularizer for the gamma weight.
        beta_constraint: Optional constraint for the beta weight.
        gamma_constraint: Optional constraint for the gamma weight.
    # Input shape
        Arbitrary. Use the keyword argument `input_shape`
        (tuple of integers, does not include the samples axis)
        when using this layer as the first layer in a Sequential model.
    # Output shape
        Same shape as input.
    # References
        - [Layer Normalization](https://arxiv.org/abs/1607.06450)
        - [Instance Normalization: The Missing Ingredient for Fast Stylization](
        https://arxiv.org/abs/1607.08022)
    &#34;&#34;&#34;

    def __init__(self,
                 axis=None,
                 epsilon=1e-3,
                 center=True,
                 scale=True,
                 beta_initializer=&#39;zeros&#39;,
                 gamma_initializer=&#39;ones&#39;,
                 beta_regularizer=None,
                 gamma_regularizer=None,
                 beta_constraint=None,
                 gamma_constraint=None,
                 **kwargs):
        super(InstanceNormalization, self).__init__(**kwargs)
        self.supports_masking = True
        self.axis = axis
        self.epsilon = epsilon
        self.center = center
        self.scale = scale
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)
        self.beta_regularizer = regularizers.get(beta_regularizer)
        self.gamma_regularizer = regularizers.get(gamma_regularizer)
        self.beta_constraint = constraints.get(beta_constraint)
        self.gamma_constraint = constraints.get(gamma_constraint)

    def build(self, input_shape):
        ndim = len(input_shape)
        if self.axis == 0:
            raise ValueError(&#39;Axis cannot be zero&#39;)

        if (self.axis is not None) and (ndim == 2):
            raise ValueError(&#39;Cannot specify axis for rank 1 tensor&#39;)

        self.input_spec = InputSpec(ndim=ndim)

        if self.axis is None:
            shape = (1,)
        else:
            shape = (input_shape[self.axis],)

        if self.scale:
            self.gamma = self.add_weight(shape=shape,
                                         name=&#39;gamma&#39;,
                                         initializer=self.gamma_initializer,
                                         regularizer=self.gamma_regularizer,
                                         constraint=self.gamma_constraint)
        else:
            self.gamma = None
        if self.center:
            self.beta = self.add_weight(shape=shape,
                                        name=&#39;beta&#39;,
                                        initializer=self.beta_initializer,
                                        regularizer=self.beta_regularizer,
                                        constraint=self.beta_constraint)
        else:
            self.beta = None
        self.built = True

    def call(self, inputs, training=None):
        input_shape = K.int_shape(inputs)
        reduction_axes = list(range(0, len(input_shape)))

        if self.axis is not None:
            del reduction_axes[self.axis]

        del reduction_axes[0]

        mean = K.mean(inputs, reduction_axes, keepdims=True)
        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon
        normed = (inputs - mean) / stddev

        broadcast_shape = [1] * len(input_shape)
        if self.axis is not None:
            broadcast_shape[self.axis] = input_shape[self.axis]

        if self.scale:
            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
            normed = normed * broadcast_gamma
        if self.center:
            broadcast_beta = K.reshape(self.beta, broadcast_shape)
            normed = normed + broadcast_beta
        return normed

    def get_config(self):
        config = {
            &#39;axis&#39;: self.axis,
            &#39;epsilon&#39;: self.epsilon,
            &#39;center&#39;: self.center,
            &#39;scale&#39;: self.scale,
            &#39;beta_initializer&#39;: initializers.serialize(self.beta_initializer),
            &#39;gamma_initializer&#39;: initializers.serialize(self.gamma_initializer),
            &#39;beta_regularizer&#39;: regularizers.serialize(self.beta_regularizer),
            &#39;gamma_regularizer&#39;: regularizers.serialize(self.gamma_regularizer),
            &#39;beta_constraint&#39;: constraints.serialize(self.beta_constraint),
            &#39;gamma_constraint&#39;: constraints.serialize(self.gamma_constraint)
        }
        base_config = super(InstanceNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)


class CustomUNETBlock(Layer):
    &#34;&#34;&#34;Unet block layer &#34;&#34;&#34;

    def __init__(self,
                 filters=32,
                 activation=&#39;relu&#39;,
                 **kwargs):
        super(CustomUNETBlock, self).__init__(**kwargs)
        self.filters = filters
        self.activation = activation
        self.x_1 = layers.Conv2D(filters=self.filters, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_3 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=&#39;same&#39;, data_format=&#39;channels_last&#39;)
        self.x_4 = layers.Conv2D(filters=self.filters * 2, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_5 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_6 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=&#39;same&#39;, data_format=&#39;channels_last&#39;)
        self.x_7 = layers.Conv2D(filters=self.filters * 4, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_8 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_9 = layers.Conv2D(filters=self.filters * 4, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_10 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_11 = layers.Conv2DTranspose(filters=self.filters * 2, kernel_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;,
                                           activation=self.activation, output_padding=None, data_format=&#39;channels_last&#39;,
                                           dilation_rate=(1, 1), use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;,
                                           bias_initializer=&#39;zeros&#39;, kernel_regularizer=None, bias_regularizer=None,
                                           activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
        self.x_12 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_13 = layers.Concatenate(axis=-1)
        self.x_14 = layers.Conv2D(filters=self.filters * 2, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                  activation=self.activation,
                                  data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                  kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                  kernel_regularizer=None,
                                  bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                  bias_constraint=None)
        self.x_15 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_16 = layers.Conv2DTranspose(filters=self.filters, kernel_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;,
                                           activation=self.activation,
                                           output_padding=None, data_format=&#39;channels_last&#39;, dilation_rate=(1, 1),
                                           use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                           kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,
                                           kernel_constraint=None, bias_constraint=None)
        self.x_17 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_18 = layers.Concatenate(axis=-1)
        self.x_19 = layers.Conv2D(filters=self.filters, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                  activation=self.activation,
                                  data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                  kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                  kernel_regularizer=None,
                                  bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                  bias_constraint=None)

    def call(self, inputs, training=True):
        x_1 = self.x_1(inputs)
        x_2 = self.x_2(x_1)
        x_3 = self.x_3(x_2)
        x_4 = self.x_4(x_3)
        x_5 = self.x_5(x_4)
        x_6 = self.x_6(x_5)
        x_7 = self.x_7(x_6)
        x_8 = self.x_8(x_7)
        x_9 = self.x_9(x_8)
        x_10 = self.x_10(x_9)
        x_11 = self.x_11(x_10)
        x_12 = self.x_12(x_11)
        x_13 = self.x_13([x_12, x_5])
        x_14 = self.x_14(x_13)
        x_15 = self.x_15(x_14)
        x_16 = self.x_16(x_15)
        x_17 = self.x_17(x_16)
        x_18 = self.x_18([x_17, x_2])
        x_19 = self.x_19(x_18)
        return x_19

    def get_config(self):
        config = {
            &#39;filters&#39;: self.filters,
            &#39;activation&#39;: self.activation,
        }
        base_config = super(CustomUNETBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)


class VAEBlock(Layer):
    &#39;&#39;&#39;
    Custom Layer VAEBlock
    Keras Layer to grab a random sample from a distribution (by multiplication)
    Computes &#34;(normal)*stddev + mean&#34; for the vae sampling operation
    (written for tf backend)
    Additionally,
        Applies regularization to the latent space representation.
        Can perform standard regularization or B-VAE regularization.
    call:
        pass in mean then stddev layers to sample from the distribution
        ex.
            sample = SampleLayer(&#39;bvae&#39;, 16)([mean, stddev])
    &#39;&#39;&#39;

    def __init__(self, latent_size=32, latent_regularizer=&#39;vae&#39;, beta=5.,
                 capacity=128., randomSample=True, roll_up=True, **kwargs):
        &#39;&#39;&#39;
        args:
        ------
        latent_regularizer : str
            Either &#39;bvae&#39;, &#39;vae&#39;, or None
            Determines whether regularization is applied
                to the latent space representation.
        beta : float
            beta &gt; 1, used for &#39;bvae&#39; latent_regularizer,
            (Unused if &#39;bvae&#39; not selected)
        capacity : float
            used for &#39;bvae&#39; to try to break input down to a set number
                of basis. (e.g. at 25, the network will try to use
                25 dimensions of the latent space)
            (unused if &#39;bvae&#39; not selected)
        randomSample : bool
            whether or not to use random sampling when selecting from
                distribution.
            if false, the latent vector equals the mean, essentially turning
                this into a standard autoencoder.
        latent_size : int
        roll_up: bool
        ------
        ex.
            sample = VAEBlock(latent_regularizer=&#39;bvae&#39;, beta=16,
                              latent_size=32)(x)
        &#39;&#39;&#39;
        super(VAEBlock, self).__init__(name=&#39;vaeblock&#39;, **kwargs)
        # sampling
        self.reg = latent_regularizer
        self.beta = beta
        self.capacity = capacity
        self.random = randomSample
        # variational encoder
        self.latent_size = latent_size
        self.roll_up = roll_up
        self.conv_mean = layers.Conv2D(filters=self.latent_size, kernel_size=(1, 1),
                                       padding=&#39;same&#39;)
        self.gla_mean = layers.GlobalAveragePooling2D()
        self.conv_stddev = layers.Conv2D(filters=self.latent_size, kernel_size=(1, 1),
                                         padding=&#39;same&#39;)
        self.gla_stddev = layers.GlobalAveragePooling2D()
        self.inter_dense = layers.Dense(8 * self.latent_size, activation=&#39;relu&#39;)
        self.dense_mean = layers.Dense(self.latent_size)
        self.dense_stddev = layers.Dense(self.latent_size)

    def call(self, inputs):
        # variational encoder output (distributions)
        if K.ndim(inputs) == 4 or K.ndim(inputs) == 4:
            mean = self.conv_mean(inputs)
            stddev = self.conv_stddev(inputs)
            if self.roll_up:
                mean = self.gla_mean(mean)
                stddev = self.gla_stddev(stddev)

        elif K.ndim(inputs) == 2 or K.ndim(inputs) == 2:
            inter = self.inter_dense(inputs)
            mean = self.dense_mean(inter)
            stddev = self.dense_stddev(inter)
        else:
            raise Exception(
                &#39;input shape VAEBlock is not a vector [batchSize, intermediate_dim] or [batchSize, width, heigth, ch]&#39;)
        if self.reg:
            # kl divergence:
            latent_loss = K.mean(-0.5 * K.sum(1 + stddev
                                              - K.square(mean)
                                              - K.exp(stddev), axis=-1))
            if self.reg == &#39;bvae&#39;:
                # use beta to force less usage of vector space:
                # also try to use &lt;capacity&gt; dimensions of the space:
                latent_loss = self.beta * K.abs(latent_loss - self.capacity / self.latent_size)
            self.add_loss(latent_loss)

        epsilon = K.random_normal(shape=K.shape(mean),
                                  mean=0., stddev=1.)

        if self.random:
            # &#39;reparameterization trick&#39;:
            return mean + K.exp(stddev / 2) * epsilon
        else:  # do not perform random sampling, simply grab the impulse value
            return mean + 0 * stddev  # Keras needs the *0 so the gradinent is not None

    # def compute_output_shape(self, input_shape):
    #     return tf.shape(input_shape)[0]

    def get_config(self):
        config = {
            &#39;latent_regularizer&#39;: self.reg,
            &#39;beta&#39;: self.beta,
            &#39;capacity&#39;: self.capacity,
            &#39;randomSample&#39;: self.random,
            &#39;latent_size&#39;: self.latent_size,
            &#39;roll_up&#39;: self.roll_up,
        }
        base_config = super(VAEBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)


class YOLOResBlock(Layer):
    def __init__(self,
                 mode=&#34;YOLOv3&#34;,
                 filters=32,
                 num_resblocks=1,
                 activation=&#39;LeakyReLU&#39;,
                 use_bias=False,
                 include_head=True,
                 all_narrow=False):
        super(YOLOResBlock, self).__init__()
        self.mode = mode
        self.all_narrow = all_narrow
        self.filters = filters
        self.num_resblocks = num_resblocks
        if activation == &#39;LeakyReLU&#39;:
            self.activation = tensorflow.keras.layers.LeakyReLU(alpha=0.1)
        if activation == &#39;Mish&#39;:
            self.activation = Mish()
        self.use_bias = use_bias
        self.include_head = include_head
        self.kwargs = {}
        if self.mode == &#34;YOLOv3&#34;:
            self.kwargs[&#34;kernel_regularizer&#34;] = tensorflow.keras.regularizers.l2(5e-4)
        if self.mode == &#34;YOLOv4&#34;:
            self.kwargs[&#34;kernel_initializer&#34;] = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)

        if self.include_head:
            self.zero2d = tensorflow.keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)))
            self.conv_start = tensorflow.keras.layers.Conv2D(filters=self.filters, kernel_size=(3, 3),
                                                             strides=(2, 2), use_bias=self.use_bias,
                                                             padding=&#39;valid&#39;, activation=&#39;linear&#39;,
                                                             **self.kwargs)
            self.bn_start = tensorflow.keras.layers.BatchNormalization()
            self.activation_start = copy.deepcopy(self.activation)

            if self.mode == &#34;YOLOv4&#34;:
                self.preconv_1 = tensorflow.keras.layers.Conv2D(
                    filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                    use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
                self.prebn_1 = tensorflow.keras.layers.BatchNormalization()
                self.preactivation_1 = copy.deepcopy(self.activation)
                self.preconv_2 = tensorflow.keras.layers.Conv2D(
                    filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                    use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
                self.prebn_2 = tensorflow.keras.layers.BatchNormalization()
                self.preactivation_2 = copy.deepcopy(self.activation)

        for i in range(self.num_resblocks):
            setattr(self, f&#34;conv_1_{i}&#34;,
                    tensorflow.keras.layers.Conv2D(filters=self.filters // 2, kernel_size=(1, 1),
                                                   activation=&#39;linear&#39;, use_bias=self.use_bias,
                                                   padding=&#39;same&#39;, **self.kwargs))
            setattr(self, f&#34;conv_2_{i}&#34;,
                    tensorflow.keras.layers.Conv2D(filters=self.filters // 2 if (
                            self.all_narrow and self.mode == &#34;YOLOv4&#34;) else self.filters,
                                                   kernel_size=(3, 3), activation=&#39;linear&#39;, use_bias=self.use_bias,
                                                   padding=&#39;same&#39;, **self.kwargs))
            setattr(self, f&#34;bn_1_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            setattr(self, f&#34;bn_2_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            setattr(self, f&#34;activ_1_{i}&#34;, copy.deepcopy(self.activation))
            setattr(self, f&#34;activ_2_{i}&#34;, copy.deepcopy(self.activation))
            setattr(self, f&#34;add_{i}&#34;, tensorflow.keras.layers.Add())

        if self.include_head and self.mode == &#34;YOLOv4&#34;:
            self.postconv_1 = tensorflow.keras.layers.Conv2D(
                filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
            self.postbn_1 = tensorflow.keras.layers.BatchNormalization()
            self.postactivation_1 = copy.deepcopy(self.activation)
            self.concatenate_1 = tensorflow.keras.layers.Concatenate()
            self.postconv_2 = tensorflow.keras.layers.Conv2D(
                filters=self.filters, kernel_size=(1, 1), use_bias=self.use_bias, padding=&#39;same&#39;,
                activation=&#39;linear&#39;, **self.kwargs)
            self.postbn_2 = tensorflow.keras.layers.BatchNormalization()
            self.postactivation_2 = copy.deepcopy(self.activation)

    def call(self, inputs, training=True, **kwargs):
        if self.include_head:
            x = self.zero2d(inputs)
            x = self.conv_start(x)
            x = self.bn_start(x)
            x = self.activation_start(x)
            if self.mode == &#34;YOLOv4&#34;:
                x_concat = self.preconv_1(x)
                x_concat = self.prebn_1(x_concat)
                x_concat = self.preactivation_1(x_concat)
                x = self.preconv_2(x)
                x = self.prebn_2(x)
                x = self.preactivation_2(x)
        else:
            x = inputs
        for i in range(self.num_resblocks):
            y = getattr(self, f&#34;conv_1_{i}&#34;)(x)
            y = getattr(self, f&#34;bn_1_{i}&#34;)(y)
            y = getattr(self, f&#34;activ_1_{i}&#34;)(y)
            y = getattr(self, f&#34;conv_2_{i}&#34;)(y)
            y = getattr(self, f&#34;bn_2_{i}&#34;)(y)
            y = getattr(self, f&#34;activ_2_{i}&#34;)(y)
            x = getattr(self, f&#34;add_{i}&#34;)([y, x])
        if self.include_head and self.mode == &#34;YOLOv4&#34;:
            x = self.postconv_1(x)
            x = self.postbn_1(x)
            x = self.postactivation_1(x)
            x = self.concatenate_1([x, x_concat])
            x = self.postconv_2(x)
            x = self.postbn_2(x)
            x = self.postactivation_2(x)
        return x

    def get_config(self):
        config = {
            &#39;mode&#39;: self.mode,
            &#39;filters&#39;: self.filters,
            &#39;num_resblocks&#39;: self.num_resblocks,
            &#39;activation&#39;: self.activation,
            &#39;use_bias&#39;: self.use_bias,
            &#39;include_head&#39;: self.include_head,
            &#39;all_narrow&#39;: self.all_narrow
        }
        base_config = super(YOLOResBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)


class YOLOConvBlock(Layer):
    &#34;&#34;&#34;Unet block layer &#34;&#34;&#34;

    def __init__(self,
                 mode=&#34;YOLOv3&#34;,
                 filters=32,
                 num_conv=1,
                 activation=&#39;LeakyReLU&#39;,
                 use_bias=False,
                 first_conv_kernel=(1, 1),
                 first_conv_strides=(1, 1),
                 first_conv_padding=&#39;same&#39;):
        super(YOLOConvBlock, self).__init__()
        self.mode = mode
        self.use_bias = use_bias
        self.strides = first_conv_strides
        self.kernel = first_conv_kernel
        self.padding = first_conv_padding
        self.kwargs = {&#39;kernel_size&#39;: self.kernel, &#39;strides&#39;: self.strides, &#39;activation&#39;: &#39;linear&#39;,
                       &#39;use_bias&#39;: self.use_bias, &#39;padding&#39;: self.padding}
        if self.mode == &#34;YOLOv3&#34;:
            self.kwargs[&#34;kernel_regularizer&#34;] = tensorflow.keras.regularizers.l2(5e-4)
        if self.mode == &#34;YOLOv4&#34;:
            self.kwargs[&#34;kernel_initializer&#34;] = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)

        self.filters = filters
        self.num_conv = num_conv
        self.mode = mode
        self.activation = activation
        self.use_bias = use_bias
        self.strides = first_conv_strides
        self.kernel = first_conv_kernel
        self.padding = first_conv_padding

        for i in range(self.num_conv):
            if i == 0:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=self.filters, **self.kwargs))
            elif i != 0 and i % 2 == 0:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=self.filters, **self.kwargs))
            else:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=2 * self.filters, **self.kwargs))
            setattr(self, f&#34;bn_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            if self.activation == &#34;LeakyReLU&#34;:
                setattr(self, f&#34;act_{i}&#34;, tensorflow.keras.layers.LeakyReLU(alpha=0.1))
            if self.activation == &#34;Mish&#34;:
                setattr(self, f&#34;act_{i}&#34;, Mish())

    def call(self, inputs, training=True, **kwargs):
        for i in range(self.num_conv):
            if i == 0:
                x = getattr(self, f&#34;conv_{i}&#34;)(inputs)
            else:
                x = getattr(self, f&#34;conv_{i}&#34;)(x)
            x = getattr(self, f&#34;bn_{i}&#34;)(x)
            x = getattr(self, f&#34;act_{i}&#34;)(x)
        return x

    def get_config(self):
        config = {
            &#39;mode&#39;: self.mode,
            &#39;filters&#39;: self.filters,
            &#39;num_conv&#39;: self.num_conv,
            &#39;activation&#39;: self.activation,
            &#39;use_bias&#39;: self.use_bias,
            &#39;first_conv_strides&#39;: self.strides,
            &#39;first_conv_kernel&#39;: self.kernel,
            &#39;first_conv_padding&#39;: self.padding
        }
        base_config = super(YOLOConvBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)


class Mish(Layer):
    &#34;&#34;&#34;
    Mish Activation Function.
    .. math::
        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))
    Shape:
        - Input: Arbitrary. Use the keyword argument `input_shape`
        (tuple of integers, does not include the samples axis)
        when using this layer as the first layer in a model.
        - Output: Same shape as the input.
    Examples:
        - X_input = Input(input_shape)
        - X = Mish()(X_input)
    &#34;&#34;&#34;

    def __init__(self, **kwargs):
        super(Mish, self).__init__(**kwargs)
        self.supports_masking = True

    def call(self, inputs, **kwargs):
        return inputs * K.tanh(K.softplus(inputs))

    def get_config(self):
        config = super(Mish, self).get_config()
        return config

    def compute_output_shape(self, input_shape):
        return input_shape


if __name__ == &#34;__main__&#34;:
    # input = tensorflow.keras.layers.Input(shape=(32, 32, 3))
    # x = YOLOResBlock(32, 2)(input)
    # print(x)
    block_type = &#39;YOLOResBlock&#39;
    # x = YOLOResBlock(**{&#39;mode&#39;: &#34;YOLOv4&#34;, &#39;filters&#39;: 32, &#34;num_resblocks&#34;: 5, &#34;activation&#34;: &#39;mish&#39;,
    #                         &#34;use_bias&#34;: False, &#34;include_head&#34;: True, &#34;all_narrow&#34;: False})
    # x = YOLOConvBlock(**{&#34;filters&#34;: 64, &#34;num_conv&#34;: 5, &#39;activation&#39;: &#39;mish&#39;})
    # print(x.compute_output_shape(input_shape=(None, 32, 32, 64))
    pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="terra_ai.customLayers.CustomUNETBlock"><code class="flex name class">
<span>class <span class="ident">CustomUNETBlock</span></span>
<span>(</span><span>filters=32, activation='relu', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Unet block layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomUNETBlock(Layer):
    &#34;&#34;&#34;Unet block layer &#34;&#34;&#34;

    def __init__(self,
                 filters=32,
                 activation=&#39;relu&#39;,
                 **kwargs):
        super(CustomUNETBlock, self).__init__(**kwargs)
        self.filters = filters
        self.activation = activation
        self.x_1 = layers.Conv2D(filters=self.filters, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_3 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=&#39;same&#39;, data_format=&#39;channels_last&#39;)
        self.x_4 = layers.Conv2D(filters=self.filters * 2, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_5 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_6 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=&#39;same&#39;, data_format=&#39;channels_last&#39;)
        self.x_7 = layers.Conv2D(filters=self.filters * 4, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_8 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                             beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                             moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                             gamma_constraint=None)
        self.x_9 = layers.Conv2D(filters=self.filters * 4, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                 activation=self.activation,
                                 data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                 kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None,
                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                 bias_constraint=None)
        self.x_10 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_11 = layers.Conv2DTranspose(filters=self.filters * 2, kernel_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;,
                                           activation=self.activation, output_padding=None, data_format=&#39;channels_last&#39;,
                                           dilation_rate=(1, 1), use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;,
                                           bias_initializer=&#39;zeros&#39;, kernel_regularizer=None, bias_regularizer=None,
                                           activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
        self.x_12 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_13 = layers.Concatenate(axis=-1)
        self.x_14 = layers.Conv2D(filters=self.filters * 2, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                  activation=self.activation,
                                  data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                  kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                  kernel_regularizer=None,
                                  bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                  bias_constraint=None)
        self.x_15 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_16 = layers.Conv2DTranspose(filters=self.filters, kernel_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;,
                                           activation=self.activation,
                                           output_padding=None, data_format=&#39;channels_last&#39;, dilation_rate=(1, 1),
                                           use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                           kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,
                                           kernel_constraint=None, bias_constraint=None)
        self.x_17 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                              beta_initializer=&#39;zeros&#39;, gamma_initializer=&#39;ones&#39;,
                                              moving_mean_initializer=&#39;zeros&#39;, moving_variance_initializer=&#39;ones&#39;,
                                              beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,
                                              gamma_constraint=None)
        self.x_18 = layers.Concatenate(axis=-1)
        self.x_19 = layers.Conv2D(filters=self.filters, kernel_size=(3, 3), strides=(1, 1), padding=&#39;same&#39;,
                                  activation=self.activation,
                                  data_format=&#39;channels_last&#39;, dilation_rate=(1, 1), groups=1, use_bias=True,
                                  kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;,
                                  kernel_regularizer=None,
                                  bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
                                  bias_constraint=None)

    def call(self, inputs, training=True):
        x_1 = self.x_1(inputs)
        x_2 = self.x_2(x_1)
        x_3 = self.x_3(x_2)
        x_4 = self.x_4(x_3)
        x_5 = self.x_5(x_4)
        x_6 = self.x_6(x_5)
        x_7 = self.x_7(x_6)
        x_8 = self.x_8(x_7)
        x_9 = self.x_9(x_8)
        x_10 = self.x_10(x_9)
        x_11 = self.x_11(x_10)
        x_12 = self.x_12(x_11)
        x_13 = self.x_13([x_12, x_5])
        x_14 = self.x_14(x_13)
        x_15 = self.x_15(x_14)
        x_16 = self.x_16(x_15)
        x_17 = self.x_17(x_16)
        x_18 = self.x_18([x_17, x_2])
        x_19 = self.x_19(x_18)
        return x_19

    def get_config(self):
        config = {
            &#39;filters&#39;: self.filters,
            &#39;activation&#39;: self.activation,
        }
        base_config = super(CustomUNETBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.customLayers.CustomUNETBlock.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.CustomUNETBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=True)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, training=True):
    x_1 = self.x_1(inputs)
    x_2 = self.x_2(x_1)
    x_3 = self.x_3(x_2)
    x_4 = self.x_4(x_3)
    x_5 = self.x_5(x_4)
    x_6 = self.x_6(x_5)
    x_7 = self.x_7(x_6)
    x_8 = self.x_8(x_7)
    x_9 = self.x_9(x_8)
    x_10 = self.x_10(x_9)
    x_11 = self.x_11(x_10)
    x_12 = self.x_12(x_11)
    x_13 = self.x_13([x_12, x_5])
    x_14 = self.x_14(x_13)
    x_15 = self.x_15(x_14)
    x_16 = self.x_16(x_15)
    x_17 = self.x_17(x_16)
    x_18 = self.x_18([x_17, x_2])
    x_19 = self.x_19(x_18)
    return x_19</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.CustomUNETBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;filters&#39;: self.filters,
        &#39;activation&#39;: self.activation,
    }
    base_config = super(CustomUNETBlock, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.customLayers.InstanceNormalization"><code class="flex name class">
<span>class <span class="ident">InstanceNormalization</span></span>
<span>(</span><span>axis=None, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Instance normalization layer.
Normalize the activations of the previous layer at each step,
i.e. applies a transformation that maintains the mean activation
close to 0 and the activation standard deviation close to 1.</p>
<h1 id="arguments">Arguments</h1>
<pre><code>axis: Integer, the axis that should be normalized
    (typically the features axis).
    For instance, after a &lt;code&gt;Conv2D&lt;/code&gt; layer with
    `data_format="channels_first"`,
    set `axis=1` in &lt;code&gt;&lt;a title="terra_ai.customLayers.InstanceNormalization" href="#terra_ai.customLayers.InstanceNormalization"&gt;InstanceNormalization&lt;/a&gt;&lt;/code&gt;.
    Setting `axis=None` will normalize all values in each
    instance of the batch.
    Axis 0 is the batch dimension. &lt;code&gt;axis&lt;/code&gt; cannot be set to 0 to avoid errors.
epsilon: Small float added to variance to avoid dividing by zero.
center: If True, add offset of &lt;code&gt;beta&lt;/code&gt; to normalized tensor.
    If False, &lt;code&gt;beta&lt;/code&gt; is ignored.
scale: If True, multiply by &lt;code&gt;gamma&lt;/code&gt;.
    If False, &lt;code&gt;gamma&lt;/code&gt; is not used.
    When the next layer is linear (also e.g. &lt;code&gt;nn.relu&lt;/code&gt;),
    this can be disabled since the scaling
    will be done by the next layer.
beta_initializer: Initializer for the beta weight.
gamma_initializer: Initializer for the gamma weight.
beta_regularizer: Optional regularizer for the beta weight.
gamma_regularizer: Optional regularizer for the gamma weight.
beta_constraint: Optional constraint for the beta weight.
gamma_constraint: Optional constraint for the gamma weight.
</code></pre>
<h1 id="input-shape">Input shape</h1>
<pre><code>Arbitrary. Use the keyword argument &lt;code&gt;input\_shape&lt;/code&gt;
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a Sequential model.
</code></pre>
<h1 id="output-shape">Output shape</h1>
<pre><code>Same shape as input.
</code></pre>
<h1 id="references">References</h1>
<pre><code>- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Instance Normalization: The Missing Ingredient for Fast Stylization](
&lt;https://arxiv.org/abs/1607.08022&gt;)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InstanceNormalization(Layer):
    &#34;&#34;&#34;Instance normalization layer.
    Normalize the activations of the previous layer at each step,
    i.e. applies a transformation that maintains the mean activation
    close to 0 and the activation standard deviation close to 1.
    # Arguments
        axis: Integer, the axis that should be normalized
            (typically the features axis).
            For instance, after a `Conv2D` layer with
            `data_format=&#34;channels_first&#34;`,
            set `axis=1` in `InstanceNormalization`.
            Setting `axis=None` will normalize all values in each
            instance of the batch.
            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.
        epsilon: Small float added to variance to avoid dividing by zero.
        center: If True, add offset of `beta` to normalized tensor.
            If False, `beta` is ignored.
        scale: If True, multiply by `gamma`.
            If False, `gamma` is not used.
            When the next layer is linear (also e.g. `nn.relu`),
            this can be disabled since the scaling
            will be done by the next layer.
        beta_initializer: Initializer for the beta weight.
        gamma_initializer: Initializer for the gamma weight.
        beta_regularizer: Optional regularizer for the beta weight.
        gamma_regularizer: Optional regularizer for the gamma weight.
        beta_constraint: Optional constraint for the beta weight.
        gamma_constraint: Optional constraint for the gamma weight.
    # Input shape
        Arbitrary. Use the keyword argument `input_shape`
        (tuple of integers, does not include the samples axis)
        when using this layer as the first layer in a Sequential model.
    # Output shape
        Same shape as input.
    # References
        - [Layer Normalization](https://arxiv.org/abs/1607.06450)
        - [Instance Normalization: The Missing Ingredient for Fast Stylization](
        https://arxiv.org/abs/1607.08022)
    &#34;&#34;&#34;

    def __init__(self,
                 axis=None,
                 epsilon=1e-3,
                 center=True,
                 scale=True,
                 beta_initializer=&#39;zeros&#39;,
                 gamma_initializer=&#39;ones&#39;,
                 beta_regularizer=None,
                 gamma_regularizer=None,
                 beta_constraint=None,
                 gamma_constraint=None,
                 **kwargs):
        super(InstanceNormalization, self).__init__(**kwargs)
        self.supports_masking = True
        self.axis = axis
        self.epsilon = epsilon
        self.center = center
        self.scale = scale
        self.beta_initializer = initializers.get(beta_initializer)
        self.gamma_initializer = initializers.get(gamma_initializer)
        self.beta_regularizer = regularizers.get(beta_regularizer)
        self.gamma_regularizer = regularizers.get(gamma_regularizer)
        self.beta_constraint = constraints.get(beta_constraint)
        self.gamma_constraint = constraints.get(gamma_constraint)

    def build(self, input_shape):
        ndim = len(input_shape)
        if self.axis == 0:
            raise ValueError(&#39;Axis cannot be zero&#39;)

        if (self.axis is not None) and (ndim == 2):
            raise ValueError(&#39;Cannot specify axis for rank 1 tensor&#39;)

        self.input_spec = InputSpec(ndim=ndim)

        if self.axis is None:
            shape = (1,)
        else:
            shape = (input_shape[self.axis],)

        if self.scale:
            self.gamma = self.add_weight(shape=shape,
                                         name=&#39;gamma&#39;,
                                         initializer=self.gamma_initializer,
                                         regularizer=self.gamma_regularizer,
                                         constraint=self.gamma_constraint)
        else:
            self.gamma = None
        if self.center:
            self.beta = self.add_weight(shape=shape,
                                        name=&#39;beta&#39;,
                                        initializer=self.beta_initializer,
                                        regularizer=self.beta_regularizer,
                                        constraint=self.beta_constraint)
        else:
            self.beta = None
        self.built = True

    def call(self, inputs, training=None):
        input_shape = K.int_shape(inputs)
        reduction_axes = list(range(0, len(input_shape)))

        if self.axis is not None:
            del reduction_axes[self.axis]

        del reduction_axes[0]

        mean = K.mean(inputs, reduction_axes, keepdims=True)
        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon
        normed = (inputs - mean) / stddev

        broadcast_shape = [1] * len(input_shape)
        if self.axis is not None:
            broadcast_shape[self.axis] = input_shape[self.axis]

        if self.scale:
            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
            normed = normed * broadcast_gamma
        if self.center:
            broadcast_beta = K.reshape(self.beta, broadcast_shape)
            normed = normed + broadcast_beta
        return normed

    def get_config(self):
        config = {
            &#39;axis&#39;: self.axis,
            &#39;epsilon&#39;: self.epsilon,
            &#39;center&#39;: self.center,
            &#39;scale&#39;: self.scale,
            &#39;beta_initializer&#39;: initializers.serialize(self.beta_initializer),
            &#39;gamma_initializer&#39;: initializers.serialize(self.gamma_initializer),
            &#39;beta_regularizer&#39;: regularizers.serialize(self.beta_regularizer),
            &#39;gamma_regularizer&#39;: regularizers.serialize(self.gamma_regularizer),
            &#39;beta_constraint&#39;: constraints.serialize(self.beta_constraint),
            &#39;gamma_constraint&#39;: constraints.serialize(self.gamma_constraint)
        }
        base_config = super(InstanceNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.customLayers.InstanceNormalization.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.InstanceNormalization.build"><code class="name flex">
<span>def <span class="ident">build</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <code>Layer</code> or <code>Model</code>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <code>Layer</code> subclasses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Instance of <code>TensorShape</code>, or list of instances of
<code>TensorShape</code> if the layer expects a list of inputs
(one instance per input).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build(self, input_shape):
    ndim = len(input_shape)
    if self.axis == 0:
        raise ValueError(&#39;Axis cannot be zero&#39;)

    if (self.axis is not None) and (ndim == 2):
        raise ValueError(&#39;Cannot specify axis for rank 1 tensor&#39;)

    self.input_spec = InputSpec(ndim=ndim)

    if self.axis is None:
        shape = (1,)
    else:
        shape = (input_shape[self.axis],)

    if self.scale:
        self.gamma = self.add_weight(shape=shape,
                                     name=&#39;gamma&#39;,
                                     initializer=self.gamma_initializer,
                                     regularizer=self.gamma_regularizer,
                                     constraint=self.gamma_constraint)
    else:
        self.gamma = None
    if self.center:
        self.beta = self.add_weight(shape=shape,
                                    name=&#39;beta&#39;,
                                    initializer=self.beta_initializer,
                                    regularizer=self.beta_regularizer,
                                    constraint=self.beta_constraint)
    else:
        self.beta = None
    self.built = True</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.InstanceNormalization.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, training=None):
    input_shape = K.int_shape(inputs)
    reduction_axes = list(range(0, len(input_shape)))

    if self.axis is not None:
        del reduction_axes[self.axis]

    del reduction_axes[0]

    mean = K.mean(inputs, reduction_axes, keepdims=True)
    stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon
    normed = (inputs - mean) / stddev

    broadcast_shape = [1] * len(input_shape)
    if self.axis is not None:
        broadcast_shape[self.axis] = input_shape[self.axis]

    if self.scale:
        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
        normed = normed * broadcast_gamma
    if self.center:
        broadcast_beta = K.reshape(self.beta, broadcast_shape)
        normed = normed + broadcast_beta
    return normed</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.InstanceNormalization.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;axis&#39;: self.axis,
        &#39;epsilon&#39;: self.epsilon,
        &#39;center&#39;: self.center,
        &#39;scale&#39;: self.scale,
        &#39;beta_initializer&#39;: initializers.serialize(self.beta_initializer),
        &#39;gamma_initializer&#39;: initializers.serialize(self.gamma_initializer),
        &#39;beta_regularizer&#39;: regularizers.serialize(self.beta_regularizer),
        &#39;gamma_regularizer&#39;: regularizers.serialize(self.gamma_regularizer),
        &#39;beta_constraint&#39;: constraints.serialize(self.beta_constraint),
        &#39;gamma_constraint&#39;: constraints.serialize(self.gamma_constraint)
    }
    base_config = super(InstanceNormalization, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.customLayers.Mish"><code class="flex name class">
<span>class <span class="ident">Mish</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Mish Activation Function.
[ mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x})) ]</p>
<h2 id="shape">Shape</h2>
<ul>
<li>Input: Arbitrary. Use the keyword argument <code>input_shape</code>
(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</li>
<li>Output: Same shape as the input.</li>
</ul>
<h2 id="examples">Examples</h2>
<ul>
<li>X_input = Input(input_shape)</li>
<li>X = Mish()(X_input)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mish(Layer):
    &#34;&#34;&#34;
    Mish Activation Function.
    .. math::
        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))
    Shape:
        - Input: Arbitrary. Use the keyword argument `input_shape`
        (tuple of integers, does not include the samples axis)
        when using this layer as the first layer in a model.
        - Output: Same shape as the input.
    Examples:
        - X_input = Input(input_shape)
        - X = Mish()(X_input)
    &#34;&#34;&#34;

    def __init__(self, **kwargs):
        super(Mish, self).__init__(**kwargs)
        self.supports_masking = True

    def call(self, inputs, **kwargs):
        return inputs * K.tanh(K.softplus(inputs))

    def get_config(self):
        config = super(Mish, self).get_config()
        return config

    def compute_output_shape(self, input_shape):
        return input_shape</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.Mish.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, **kwargs):
    return inputs * K.tanh(K.softplus(inputs))</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.Mish.compute_output_shape"><code class="name flex">
<span>def <span class="ident">compute_output_shape</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <code>build</code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_shape</code></strong></dt>
<dd>Shape tuple (tuple of integers)
or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>An input shape tuple.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_output_shape(self, input_shape):
    return input_shape</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.Mish.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = super(Mish, self).get_config()
    return config</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.customLayers.VAEBlock"><code class="flex name class">
<span>class <span class="ident">VAEBlock</span></span>
<span>(</span><span>latent_size=32, latent_regularizer='vae', beta=5.0, capacity=128.0, randomSample=True, roll_up=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom Layer VAEBlock
Keras Layer to grab a random sample from a distribution (by multiplication)
Computes "(normal)*stddev + mean" for the vae sampling operation
(written for tf backend)
Additionally,
Applies regularization to the latent space representation.
Can perform standard regularization or B-VAE regularization.
call:
pass in mean then stddev layers to sample from the distribution
ex.
sample = SampleLayer('bvae', 16)([mean, stddev])</p>
<h2 id="args">args:</h2>
<p>latent_regularizer : str
Either 'bvae', 'vae', or None
Determines whether regularization is applied
to the latent space representation.
beta : float
beta &gt; 1, used for 'bvae' latent_regularizer,
(Unused if 'bvae' not selected)
capacity : float
used for 'bvae' to try to break input down to a set number
of basis. (e.g. at 25, the network will try to use
25 dimensions of the latent space)
(unused if 'bvae' not selected)
randomSample : bool
whether or not to use random sampling when selecting from
distribution.
if false, the latent vector equals the mean, essentially turning
this into a standard autoencoder.
latent_size : int
roll_up: bool</p>
<hr>
<p>ex.
sample = VAEBlock(latent_regularizer='bvae', beta=16,
latent_size=32)(x)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VAEBlock(Layer):
    &#39;&#39;&#39;
    Custom Layer VAEBlock
    Keras Layer to grab a random sample from a distribution (by multiplication)
    Computes &#34;(normal)*stddev + mean&#34; for the vae sampling operation
    (written for tf backend)
    Additionally,
        Applies regularization to the latent space representation.
        Can perform standard regularization or B-VAE regularization.
    call:
        pass in mean then stddev layers to sample from the distribution
        ex.
            sample = SampleLayer(&#39;bvae&#39;, 16)([mean, stddev])
    &#39;&#39;&#39;

    def __init__(self, latent_size=32, latent_regularizer=&#39;vae&#39;, beta=5.,
                 capacity=128., randomSample=True, roll_up=True, **kwargs):
        &#39;&#39;&#39;
        args:
        ------
        latent_regularizer : str
            Either &#39;bvae&#39;, &#39;vae&#39;, or None
            Determines whether regularization is applied
                to the latent space representation.
        beta : float
            beta &gt; 1, used for &#39;bvae&#39; latent_regularizer,
            (Unused if &#39;bvae&#39; not selected)
        capacity : float
            used for &#39;bvae&#39; to try to break input down to a set number
                of basis. (e.g. at 25, the network will try to use
                25 dimensions of the latent space)
            (unused if &#39;bvae&#39; not selected)
        randomSample : bool
            whether or not to use random sampling when selecting from
                distribution.
            if false, the latent vector equals the mean, essentially turning
                this into a standard autoencoder.
        latent_size : int
        roll_up: bool
        ------
        ex.
            sample = VAEBlock(latent_regularizer=&#39;bvae&#39;, beta=16,
                              latent_size=32)(x)
        &#39;&#39;&#39;
        super(VAEBlock, self).__init__(name=&#39;vaeblock&#39;, **kwargs)
        # sampling
        self.reg = latent_regularizer
        self.beta = beta
        self.capacity = capacity
        self.random = randomSample
        # variational encoder
        self.latent_size = latent_size
        self.roll_up = roll_up
        self.conv_mean = layers.Conv2D(filters=self.latent_size, kernel_size=(1, 1),
                                       padding=&#39;same&#39;)
        self.gla_mean = layers.GlobalAveragePooling2D()
        self.conv_stddev = layers.Conv2D(filters=self.latent_size, kernel_size=(1, 1),
                                         padding=&#39;same&#39;)
        self.gla_stddev = layers.GlobalAveragePooling2D()
        self.inter_dense = layers.Dense(8 * self.latent_size, activation=&#39;relu&#39;)
        self.dense_mean = layers.Dense(self.latent_size)
        self.dense_stddev = layers.Dense(self.latent_size)

    def call(self, inputs):
        # variational encoder output (distributions)
        if K.ndim(inputs) == 4 or K.ndim(inputs) == 4:
            mean = self.conv_mean(inputs)
            stddev = self.conv_stddev(inputs)
            if self.roll_up:
                mean = self.gla_mean(mean)
                stddev = self.gla_stddev(stddev)

        elif K.ndim(inputs) == 2 or K.ndim(inputs) == 2:
            inter = self.inter_dense(inputs)
            mean = self.dense_mean(inter)
            stddev = self.dense_stddev(inter)
        else:
            raise Exception(
                &#39;input shape VAEBlock is not a vector [batchSize, intermediate_dim] or [batchSize, width, heigth, ch]&#39;)
        if self.reg:
            # kl divergence:
            latent_loss = K.mean(-0.5 * K.sum(1 + stddev
                                              - K.square(mean)
                                              - K.exp(stddev), axis=-1))
            if self.reg == &#39;bvae&#39;:
                # use beta to force less usage of vector space:
                # also try to use &lt;capacity&gt; dimensions of the space:
                latent_loss = self.beta * K.abs(latent_loss - self.capacity / self.latent_size)
            self.add_loss(latent_loss)

        epsilon = K.random_normal(shape=K.shape(mean),
                                  mean=0., stddev=1.)

        if self.random:
            # &#39;reparameterization trick&#39;:
            return mean + K.exp(stddev / 2) * epsilon
        else:  # do not perform random sampling, simply grab the impulse value
            return mean + 0 * stddev  # Keras needs the *0 so the gradinent is not None

    # def compute_output_shape(self, input_shape):
    #     return tf.shape(input_shape)[0]

    def get_config(self):
        config = {
            &#39;latent_regularizer&#39;: self.reg,
            &#39;beta&#39;: self.beta,
            &#39;capacity&#39;: self.capacity,
            &#39;randomSample&#39;: self.random,
            &#39;latent_size&#39;: self.latent_size,
            &#39;roll_up&#39;: self.roll_up,
        }
        base_config = super(VAEBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.customLayers.VAEBlock.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.VAEBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs):
    # variational encoder output (distributions)
    if K.ndim(inputs) == 4 or K.ndim(inputs) == 4:
        mean = self.conv_mean(inputs)
        stddev = self.conv_stddev(inputs)
        if self.roll_up:
            mean = self.gla_mean(mean)
            stddev = self.gla_stddev(stddev)

    elif K.ndim(inputs) == 2 or K.ndim(inputs) == 2:
        inter = self.inter_dense(inputs)
        mean = self.dense_mean(inter)
        stddev = self.dense_stddev(inter)
    else:
        raise Exception(
            &#39;input shape VAEBlock is not a vector [batchSize, intermediate_dim] or [batchSize, width, heigth, ch]&#39;)
    if self.reg:
        # kl divergence:
        latent_loss = K.mean(-0.5 * K.sum(1 + stddev
                                          - K.square(mean)
                                          - K.exp(stddev), axis=-1))
        if self.reg == &#39;bvae&#39;:
            # use beta to force less usage of vector space:
            # also try to use &lt;capacity&gt; dimensions of the space:
            latent_loss = self.beta * K.abs(latent_loss - self.capacity / self.latent_size)
        self.add_loss(latent_loss)

    epsilon = K.random_normal(shape=K.shape(mean),
                              mean=0., stddev=1.)

    if self.random:
        # &#39;reparameterization trick&#39;:
        return mean + K.exp(stddev / 2) * epsilon
    else:  # do not perform random sampling, simply grab the impulse value
        return mean + 0 * stddev  # Keras needs the *0 so the gradinent is not None</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.VAEBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;latent_regularizer&#39;: self.reg,
        &#39;beta&#39;: self.beta,
        &#39;capacity&#39;: self.capacity,
        &#39;randomSample&#39;: self.random,
        &#39;latent_size&#39;: self.latent_size,
        &#39;roll_up&#39;: self.roll_up,
    }
    base_config = super(VAEBlock, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.customLayers.YOLOConvBlock"><code class="flex name class">
<span>class <span class="ident">YOLOConvBlock</span></span>
<span>(</span><span>mode='YOLOv3', filters=32, num_conv=1, activation='LeakyReLU', use_bias=False, first_conv_kernel=(1, 1), first_conv_strides=(1, 1), first_conv_padding='same')</span>
</code></dt>
<dd>
<div class="desc"><p>Unet block layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class YOLOConvBlock(Layer):
    &#34;&#34;&#34;Unet block layer &#34;&#34;&#34;

    def __init__(self,
                 mode=&#34;YOLOv3&#34;,
                 filters=32,
                 num_conv=1,
                 activation=&#39;LeakyReLU&#39;,
                 use_bias=False,
                 first_conv_kernel=(1, 1),
                 first_conv_strides=(1, 1),
                 first_conv_padding=&#39;same&#39;):
        super(YOLOConvBlock, self).__init__()
        self.mode = mode
        self.use_bias = use_bias
        self.strides = first_conv_strides
        self.kernel = first_conv_kernel
        self.padding = first_conv_padding
        self.kwargs = {&#39;kernel_size&#39;: self.kernel, &#39;strides&#39;: self.strides, &#39;activation&#39;: &#39;linear&#39;,
                       &#39;use_bias&#39;: self.use_bias, &#39;padding&#39;: self.padding}
        if self.mode == &#34;YOLOv3&#34;:
            self.kwargs[&#34;kernel_regularizer&#34;] = tensorflow.keras.regularizers.l2(5e-4)
        if self.mode == &#34;YOLOv4&#34;:
            self.kwargs[&#34;kernel_initializer&#34;] = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)

        self.filters = filters
        self.num_conv = num_conv
        self.mode = mode
        self.activation = activation
        self.use_bias = use_bias
        self.strides = first_conv_strides
        self.kernel = first_conv_kernel
        self.padding = first_conv_padding

        for i in range(self.num_conv):
            if i == 0:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=self.filters, **self.kwargs))
            elif i != 0 and i % 2 == 0:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=self.filters, **self.kwargs))
            else:
                setattr(self, f&#34;conv_{i}&#34;, tensorflow.keras.layers.Conv2D(filters=2 * self.filters, **self.kwargs))
            setattr(self, f&#34;bn_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            if self.activation == &#34;LeakyReLU&#34;:
                setattr(self, f&#34;act_{i}&#34;, tensorflow.keras.layers.LeakyReLU(alpha=0.1))
            if self.activation == &#34;Mish&#34;:
                setattr(self, f&#34;act_{i}&#34;, Mish())

    def call(self, inputs, training=True, **kwargs):
        for i in range(self.num_conv):
            if i == 0:
                x = getattr(self, f&#34;conv_{i}&#34;)(inputs)
            else:
                x = getattr(self, f&#34;conv_{i}&#34;)(x)
            x = getattr(self, f&#34;bn_{i}&#34;)(x)
            x = getattr(self, f&#34;act_{i}&#34;)(x)
        return x

    def get_config(self):
        config = {
            &#39;mode&#39;: self.mode,
            &#39;filters&#39;: self.filters,
            &#39;num_conv&#39;: self.num_conv,
            &#39;activation&#39;: self.activation,
            &#39;use_bias&#39;: self.use_bias,
            &#39;first_conv_strides&#39;: self.strides,
            &#39;first_conv_kernel&#39;: self.kernel,
            &#39;first_conv_padding&#39;: self.padding
        }
        base_config = super(YOLOConvBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.customLayers.YOLOConvBlock.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.YOLOConvBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, training=True, **kwargs):
    for i in range(self.num_conv):
        if i == 0:
            x = getattr(self, f&#34;conv_{i}&#34;)(inputs)
        else:
            x = getattr(self, f&#34;conv_{i}&#34;)(x)
        x = getattr(self, f&#34;bn_{i}&#34;)(x)
        x = getattr(self, f&#34;act_{i}&#34;)(x)
    return x</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.YOLOConvBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;mode&#39;: self.mode,
        &#39;filters&#39;: self.filters,
        &#39;num_conv&#39;: self.num_conv,
        &#39;activation&#39;: self.activation,
        &#39;use_bias&#39;: self.use_bias,
        &#39;first_conv_strides&#39;: self.strides,
        &#39;first_conv_kernel&#39;: self.kernel,
        &#39;first_conv_padding&#39;: self.padding
    }
    base_config = super(YOLOConvBlock, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.customLayers.YOLOResBlock"><code class="flex name class">
<span>class <span class="ident">YOLOResBlock</span></span>
<span>(</span><span>mode='YOLOv3', filters=32, num_resblocks=1, activation='LeakyReLU', use_bias=False, include_head=True, all_narrow=False)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables), defined
either in the constructor <code>__init__()</code> or in the <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainable</code></strong></dt>
<dd>Boolean, whether the layer's variables should be trainable.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String name of the layer.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. Can also be a
<code>tf.keras.mixed_precision.Policy</code>, which allows the computation and weight
dtype to differ. Default of <code>None</code> means to use
<code>tf.keras.mixed_precision.global_policy()</code>, which is a float32 policy
unless set to different value.</dd>
<dt><strong><code>dynamic</code></strong></dt>
<dd>Set this to <code>True</code> if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If <code>False</code>, we assume that the layer can
safely be used to generate a static computation graph.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's weights.</dd>
<dt><strong><code>variable_dtype</code></strong></dt>
<dd>Alias of <code>dtype</code>.</dd>
<dt><strong><code>compute_dtype</code></strong></dt>
<dd>The dtype of the layer's computations. Layers automatically
cast inputs to this dtype which causes the computations and output to also
be in this dtype. When mixed precision is used with a
<code>tf.keras.mixed_precision.Policy</code>, this will be different than
<code>variable_dtype</code>.</dd>
<dt><strong><code>dtype_policy</code></strong></dt>
<dd>The layer's dtype policy. See the
<code>tf.keras.mixed_precision.Policy</code> documentation for details.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
<code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer state
variables that do not depend on input shapes, using <code>add_weight()</code>.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>. <code>__call__()</code>
will automatically build the layer (if it has not been built yet) by
calling <code>build()</code>.</li>
<li><code>call(self, inputs, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying the
layer to the input tensors (which should be passed in as argument).
Two reserved keyword arguments you can optionally use in <code>call()</code> are:<ul>
<li><code>training</code> (boolean, whether the call is in inference mode or training
mode). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method">the layer/model subclassing guide</a></li>
<li><code>mask</code> (boolean tensor encoding masked timesteps in the input, used
in RNN layers). See more details in <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method">the layer/model subclassing guide</a>
A typical signature for this method is <code>call(self, inputs)</code>, and user could
optionally add <code>training</code> and <code>mask</code> if the layer need them. <code>*args</code> and
<code>**kwargs</code> is only useful for future extension when more input parameters
are planned to be added.</li>
</ul>
</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in <code>__init__</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Note that the method <code>add_weight()</code> offers a shortcut to create weights:</p>
<pre><code class="language-python">class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre>
<p>For more information about creating layers, see the guide
<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">Making new Layers and Models via subclassing</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class YOLOResBlock(Layer):
    def __init__(self,
                 mode=&#34;YOLOv3&#34;,
                 filters=32,
                 num_resblocks=1,
                 activation=&#39;LeakyReLU&#39;,
                 use_bias=False,
                 include_head=True,
                 all_narrow=False):
        super(YOLOResBlock, self).__init__()
        self.mode = mode
        self.all_narrow = all_narrow
        self.filters = filters
        self.num_resblocks = num_resblocks
        if activation == &#39;LeakyReLU&#39;:
            self.activation = tensorflow.keras.layers.LeakyReLU(alpha=0.1)
        if activation == &#39;Mish&#39;:
            self.activation = Mish()
        self.use_bias = use_bias
        self.include_head = include_head
        self.kwargs = {}
        if self.mode == &#34;YOLOv3&#34;:
            self.kwargs[&#34;kernel_regularizer&#34;] = tensorflow.keras.regularizers.l2(5e-4)
        if self.mode == &#34;YOLOv4&#34;:
            self.kwargs[&#34;kernel_initializer&#34;] = tensorflow.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)

        if self.include_head:
            self.zero2d = tensorflow.keras.layers.ZeroPadding2D(padding=((1, 0), (1, 0)))
            self.conv_start = tensorflow.keras.layers.Conv2D(filters=self.filters, kernel_size=(3, 3),
                                                             strides=(2, 2), use_bias=self.use_bias,
                                                             padding=&#39;valid&#39;, activation=&#39;linear&#39;,
                                                             **self.kwargs)
            self.bn_start = tensorflow.keras.layers.BatchNormalization()
            self.activation_start = copy.deepcopy(self.activation)

            if self.mode == &#34;YOLOv4&#34;:
                self.preconv_1 = tensorflow.keras.layers.Conv2D(
                    filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                    use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
                self.prebn_1 = tensorflow.keras.layers.BatchNormalization()
                self.preactivation_1 = copy.deepcopy(self.activation)
                self.preconv_2 = tensorflow.keras.layers.Conv2D(
                    filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                    use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
                self.prebn_2 = tensorflow.keras.layers.BatchNormalization()
                self.preactivation_2 = copy.deepcopy(self.activation)

        for i in range(self.num_resblocks):
            setattr(self, f&#34;conv_1_{i}&#34;,
                    tensorflow.keras.layers.Conv2D(filters=self.filters // 2, kernel_size=(1, 1),
                                                   activation=&#39;linear&#39;, use_bias=self.use_bias,
                                                   padding=&#39;same&#39;, **self.kwargs))
            setattr(self, f&#34;conv_2_{i}&#34;,
                    tensorflow.keras.layers.Conv2D(filters=self.filters // 2 if (
                            self.all_narrow and self.mode == &#34;YOLOv4&#34;) else self.filters,
                                                   kernel_size=(3, 3), activation=&#39;linear&#39;, use_bias=self.use_bias,
                                                   padding=&#39;same&#39;, **self.kwargs))
            setattr(self, f&#34;bn_1_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            setattr(self, f&#34;bn_2_{i}&#34;, tensorflow.keras.layers.BatchNormalization())
            setattr(self, f&#34;activ_1_{i}&#34;, copy.deepcopy(self.activation))
            setattr(self, f&#34;activ_2_{i}&#34;, copy.deepcopy(self.activation))
            setattr(self, f&#34;add_{i}&#34;, tensorflow.keras.layers.Add())

        if self.include_head and self.mode == &#34;YOLOv4&#34;:
            self.postconv_1 = tensorflow.keras.layers.Conv2D(
                filters=self.filters // 2 if self.all_narrow else self.filters, kernel_size=(1, 1),
                use_bias=self.use_bias, padding=&#39;same&#39;, activation=&#39;linear&#39;, **self.kwargs)
            self.postbn_1 = tensorflow.keras.layers.BatchNormalization()
            self.postactivation_1 = copy.deepcopy(self.activation)
            self.concatenate_1 = tensorflow.keras.layers.Concatenate()
            self.postconv_2 = tensorflow.keras.layers.Conv2D(
                filters=self.filters, kernel_size=(1, 1), use_bias=self.use_bias, padding=&#39;same&#39;,
                activation=&#39;linear&#39;, **self.kwargs)
            self.postbn_2 = tensorflow.keras.layers.BatchNormalization()
            self.postactivation_2 = copy.deepcopy(self.activation)

    def call(self, inputs, training=True, **kwargs):
        if self.include_head:
            x = self.zero2d(inputs)
            x = self.conv_start(x)
            x = self.bn_start(x)
            x = self.activation_start(x)
            if self.mode == &#34;YOLOv4&#34;:
                x_concat = self.preconv_1(x)
                x_concat = self.prebn_1(x_concat)
                x_concat = self.preactivation_1(x_concat)
                x = self.preconv_2(x)
                x = self.prebn_2(x)
                x = self.preactivation_2(x)
        else:
            x = inputs
        for i in range(self.num_resblocks):
            y = getattr(self, f&#34;conv_1_{i}&#34;)(x)
            y = getattr(self, f&#34;bn_1_{i}&#34;)(y)
            y = getattr(self, f&#34;activ_1_{i}&#34;)(y)
            y = getattr(self, f&#34;conv_2_{i}&#34;)(y)
            y = getattr(self, f&#34;bn_2_{i}&#34;)(y)
            y = getattr(self, f&#34;activ_2_{i}&#34;)(y)
            x = getattr(self, f&#34;add_{i}&#34;)([y, x])
        if self.include_head and self.mode == &#34;YOLOv4&#34;:
            x = self.postconv_1(x)
            x = self.postbn_1(x)
            x = self.postactivation_1(x)
            x = self.concatenate_1([x, x_concat])
            x = self.postconv_2(x)
            x = self.postbn_2(x)
            x = self.postactivation_2(x)
        return x

    def get_config(self):
        config = {
            &#39;mode&#39;: self.mode,
            &#39;filters&#39;: self.filters,
            &#39;num_resblocks&#39;: self.num_resblocks,
            &#39;activation&#39;: self.activation,
            &#39;use_bias&#39;: self.use_bias,
            &#39;include_head&#39;: self.include_head,
            &#39;all_narrow&#39;: self.all_narrow
        }
        base_config = super(YOLOResBlock, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    @classmethod
    def from_config(cls, config):
        return cls(**config)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.customLayers.YOLOResBlock.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.customLayers.YOLOResBlock.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is where the layer's logic lives.</p>
<p>Note here that <code>call()</code> method in <code>tf.keras</code> is little bit different
from <code>keras</code> API. In <code>keras</code> API, you can pass support masking for
layers as additional arguments. Whereas <code>tf.keras</code> has <code>compute_mask()</code>
method to support masking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>Input tensor, or list/tuple of input tensors.</dd>
<dt><strong><code>*args</code></strong></dt>
<dd>Additional positional arguments. Currently unused.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments. Currently unused.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tensor or list/tuple of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, training=True, **kwargs):
    if self.include_head:
        x = self.zero2d(inputs)
        x = self.conv_start(x)
        x = self.bn_start(x)
        x = self.activation_start(x)
        if self.mode == &#34;YOLOv4&#34;:
            x_concat = self.preconv_1(x)
            x_concat = self.prebn_1(x_concat)
            x_concat = self.preactivation_1(x_concat)
            x = self.preconv_2(x)
            x = self.prebn_2(x)
            x = self.preactivation_2(x)
    else:
        x = inputs
    for i in range(self.num_resblocks):
        y = getattr(self, f&#34;conv_1_{i}&#34;)(x)
        y = getattr(self, f&#34;bn_1_{i}&#34;)(y)
        y = getattr(self, f&#34;activ_1_{i}&#34;)(y)
        y = getattr(self, f&#34;conv_2_{i}&#34;)(y)
        y = getattr(self, f&#34;bn_2_{i}&#34;)(y)
        y = getattr(self, f&#34;activ_2_{i}&#34;)(y)
        x = getattr(self, f&#34;add_{i}&#34;)([y, x])
    if self.include_head and self.mode == &#34;YOLOv4&#34;:
        x = self.postconv_1(x)
        x = self.postbn_1(x)
        x = self.postactivation_1(x)
        x = self.concatenate_1([x, x_concat])
        x = self.postconv_2(x)
        x = self.postbn_2(x)
        x = self.postactivation_2(x)
    return x</code></pre>
</details>
</dd>
<dt id="terra_ai.customLayers.YOLOResBlock.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<p>Note that <code>get_config()</code> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    config = {
        &#39;mode&#39;: self.mode,
        &#39;filters&#39;: self.filters,
        &#39;num_resblocks&#39;: self.num_resblocks,
        &#39;activation&#39;: self.activation,
        &#39;use_bias&#39;: self.use_bias,
        &#39;include_head&#39;: self.include_head,
        &#39;all_narrow&#39;: self.all_narrow
    }
    base_config = super(YOLOResBlock, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="terra_ai" href="index.html">terra_ai</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="terra_ai.customLayers.CustomUNETBlock" href="#terra_ai.customLayers.CustomUNETBlock">CustomUNETBlock</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.CustomUNETBlock.call" href="#terra_ai.customLayers.CustomUNETBlock.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.CustomUNETBlock.from_config" href="#terra_ai.customLayers.CustomUNETBlock.from_config">from_config</a></code></li>
<li><code><a title="terra_ai.customLayers.CustomUNETBlock.get_config" href="#terra_ai.customLayers.CustomUNETBlock.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.customLayers.InstanceNormalization" href="#terra_ai.customLayers.InstanceNormalization">InstanceNormalization</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.InstanceNormalization.build" href="#terra_ai.customLayers.InstanceNormalization.build">build</a></code></li>
<li><code><a title="terra_ai.customLayers.InstanceNormalization.call" href="#terra_ai.customLayers.InstanceNormalization.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.InstanceNormalization.from_config" href="#terra_ai.customLayers.InstanceNormalization.from_config">from_config</a></code></li>
<li><code><a title="terra_ai.customLayers.InstanceNormalization.get_config" href="#terra_ai.customLayers.InstanceNormalization.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.customLayers.Mish" href="#terra_ai.customLayers.Mish">Mish</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.Mish.call" href="#terra_ai.customLayers.Mish.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.Mish.compute_output_shape" href="#terra_ai.customLayers.Mish.compute_output_shape">compute_output_shape</a></code></li>
<li><code><a title="terra_ai.customLayers.Mish.get_config" href="#terra_ai.customLayers.Mish.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.customLayers.VAEBlock" href="#terra_ai.customLayers.VAEBlock">VAEBlock</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.VAEBlock.call" href="#terra_ai.customLayers.VAEBlock.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.VAEBlock.from_config" href="#terra_ai.customLayers.VAEBlock.from_config">from_config</a></code></li>
<li><code><a title="terra_ai.customLayers.VAEBlock.get_config" href="#terra_ai.customLayers.VAEBlock.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.customLayers.YOLOConvBlock" href="#terra_ai.customLayers.YOLOConvBlock">YOLOConvBlock</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.YOLOConvBlock.call" href="#terra_ai.customLayers.YOLOConvBlock.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.YOLOConvBlock.from_config" href="#terra_ai.customLayers.YOLOConvBlock.from_config">from_config</a></code></li>
<li><code><a title="terra_ai.customLayers.YOLOConvBlock.get_config" href="#terra_ai.customLayers.YOLOConvBlock.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.customLayers.YOLOResBlock" href="#terra_ai.customLayers.YOLOResBlock">YOLOResBlock</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.customLayers.YOLOResBlock.call" href="#terra_ai.customLayers.YOLOResBlock.call">call</a></code></li>
<li><code><a title="terra_ai.customLayers.YOLOResBlock.from_config" href="#terra_ai.customLayers.YOLOResBlock.from_config">from_config</a></code></li>
<li><code><a title="terra_ai.customLayers.YOLOResBlock.get_config" href="#terra_ai.customLayers.YOLOResBlock.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>