<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>terra_ai.trds API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>terra_ai.trds</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from terra_ai.data.datasets.extra import SourceModeChoice
from terra_ai.data.datasets.creation import SourceData

from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100, imdb, reuters, boston_housing
from tensorflow.keras.layers.experimental.preprocessing import Resizing
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras import utils
from tensorflow.python.data.ops.dataset_ops import DatasetV2 as Dataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from PIL import Image, ImageColor
from librosa import load as librosa_load
import librosa.feature as librosa_feature
import os
import random
import numpy as np
import pandas as pd
import pathlib
import re
import pymorphy2
import shutil
from gensim.models.word2vec import Word2Vec
from tqdm.notebook import tqdm
from io import open as io_open
from terra_ai.guiexchange import Exchange
import joblib
import requests
from tempfile import mkdtemp
from datetime import datetime
from pytz import timezone
import json
import imgaug.augmenters as iaa
from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage
import cv2

tr2dj_obj = Exchange()

__version__ = 1.01


class CreateDTS(object):

    def __init__(self, trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;,
                 exch_obj=tr2dj_obj):

        self.Exch = exch_obj
        self.django_flag: bool = False
        if self.Exch.property_of != &#39;TERRA&#39;:
            self.django_flag = True

        self.dataloader = Dataloader()
        self.createarray = None

        self.trds_path: str = trds_path
        self.file_folder: str = &#39;&#39;

        self.name: str = &#39;&#39;
        self.source: str = &#39;&#39;
        self.tags: dict = {}
        self.user_tags: list = []
        self.language: str = &#39;&#39;
        self.divide_ratio: list = [0.8, 0.1, 0.1]
        self.limit: int = 0
        self.input_datatype: dict = {}  # string
        self.input_dtype: dict = {}
        self.input_shape: dict = {}
        self.input_names: dict = {}
        self.output_datatype: dict = {}
        self.output_dtype: dict = {}
        self.output_shape: dict = {}
        self.output_names: dict = {}
        self.num_classes: dict = {}
        self.classes_names: dict = {}
        self.classes_colors: dict = {}
        self.one_hot_encoding: dict = {}
        self.task_type: dict = {}
        self.zip_params: dict = {}
        self.user_parameters: dict = {}
        self.use_generator: bool = False

        self.X: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.Y: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.df: dict = {}
        self.tsgenerator: dict = {}

        self.instructions: dict = {&#39;inputs&#39;: {}, &#39;outputs&#39;: {}}
        self.limit: int
        self.dataset: dict = {}

        self.y_cls: list = []
        self.sequence: list = []
        self.peg: list = []
        self.iter: int = 0
        self.mode: str = &#39;&#39;
        self.split_sequence: dict = {}
        self.temporary: dict = {}

        pass

    @staticmethod
    def _set_datatype(shape) -&gt; str:

        datatype = {0: &#39;DIM&#39;,
                    1: &#39;DIM&#39;,
                    2: &#39;1D&#39;,
                    3: &#39;2D&#39;,
                    4: &#39;3D&#39;,
                    5: &#39;4D&#39;
                    }

        return datatype[len(shape)]

    def load_data(self, strict_object):

        self.dataloader.load_data(strict_object=strict_object)

        self.zip_params = json.loads(strict_object.json())
        self.file_folder = self.dataloader.file_folder

        pass

    def create_dataset(self, dataset_dict: dict):

        self.createarray = CreateArray(file_folder=self.file_folder)

        self.name = dataset_dict[&#39;parameters&#39;][&#39;name&#39;]
        self.divide_ratio = (dataset_dict[&#39;parameters&#39;][&#39;train_part&#39;], dataset_dict[&#39;parameters&#39;][&#39;val_part&#39;],
                             dataset_dict[&#39;parameters&#39;][&#39;test_part&#39;])
        self.source = &#39;custom dataset&#39;
        self.user_tags = dataset_dict[&#39;parameters&#39;][&#39;user_tags&#39;]
        self.use_generator = dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]

        for key in dataset_dict[&#39;inputs&#39;].keys():
            self.tags[key] = dataset_dict[&#39;inputs&#39;][key][&#39;tag&#39;]
            self.input_names[key] = dataset_dict[&#39;inputs&#39;][key][&#39;name&#39;]
            self.user_parameters[key] = dataset_dict[&#39;inputs&#39;][key][&#39;parameters&#39;]
        for key in dataset_dict[&#39;outputs&#39;].keys():
            self.tags[key] = dataset_dict[&#39;outputs&#39;][key][&#39;tag&#39;]
            self.output_names[key] = dataset_dict[&#39;outputs&#39;][key][&#39;name&#39;]
            self.user_parameters[key] = dataset_dict[&#39;outputs&#39;][key][&#39;parameters&#39;]

        # Создаем входные инструкции
        self.iter = 0
        self.mode = &#39;input&#39;
        for inp in dataset_dict[&#39;inputs&#39;]:
            self.iter += 1
            self.instructions[&#39;inputs&#39;][inp] = getattr(self, f&#34;instructions_{self.tags[inp]}&#34;)(
                **dataset_dict[&#39;inputs&#39;][inp][&#39;parameters&#39;])
        # Создаем выходные инструкции
        self.iter = 0
        self.mode = &#39;output&#39;
        for out in dataset_dict[&#39;outputs&#39;]:
            self.iter += 1
            self.instructions[&#39;outputs&#39;][out] = getattr(self, f&#34;instructions_{self.tags[out]}&#34;)(
                **dataset_dict[&#39;outputs&#39;][out][&#39;parameters&#39;])

        # Получаем входные параметры
        for key in self.instructions[&#39;inputs&#39;].keys():
            array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            self.input_shape[key] = array.shape
            self.input_dtype[key] = str(array.dtype)
            self.input_datatype[key] = self._set_datatype(array.shape)
        # Получаем выходные параметры
        for key in self.instructions[&#39;outputs&#39;].keys():
            array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
                self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
            if isinstance(array, tuple):
                for i in range(len(array)):
                    self.output_shape[key.replace(key[-1], str(int(key[-1]) + i))] = array[i].shape
                    self.output_dtype[key.replace(key[-1], str(int(key[-1]) + i))] = str(array[i].dtype)
                    self.output_datatype[key.replace(key[-1], str(int(key[-1]) + i))] = self._set_datatype(
                        array[i].shape)
            else:
                self.output_shape[key] = array.shape
                self.output_dtype[key] = str(array.dtype)
                self.output_datatype[key] = self._set_datatype(array.shape)

        # Разделение на три выборки
        self.split_sequence[&#39;train&#39;] = []
        self.split_sequence[&#39;val&#39;] = []
        self.split_sequence[&#39;test&#39;] = []
        for i in range(len(self.peg) - 1):
            indices = np.arange(self.peg[i], self.peg[i + 1])
            train_len = int(self.divide_ratio[0] * len(indices))
            val_len = int(self.divide_ratio[1] * len(indices))
            indices = indices.tolist()
            self.split_sequence[&#39;train&#39;].extend(indices[:train_len])
            self.split_sequence[&#39;val&#39;].extend(indices[train_len:train_len + val_len])
            self.split_sequence[&#39;test&#39;].extend(indices[train_len + val_len:])
        if not dataset_dict[&#39;parameters&#39;][&#39;preserve_sequence&#39;]:
            random.shuffle(self.split_sequence[&#39;train&#39;])
            random.shuffle(self.split_sequence[&#39;val&#39;])
            random.shuffle(self.split_sequence[&#39;test&#39;])

        self.limit: int = len(self.instructions[&#39;inputs&#39;][&#39;input_1&#39;][&#39;instructions&#39;])

        data = {}
        if dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]:
            # Сохранение датасета для генератора
            data[&#39;zip_params&#39;] = self.zip_params
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;), exist_ok=True)
            for key in self.instructions.keys():
                os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key), exist_ok=True)
                for inp in self.instructions[key].keys():
                    with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key, f&#39;{inp}.json&#39;),
                              &#39;w&#39;) as instruction:
                        json.dump(self.instructions[key][inp], instruction)
            with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                      &#39;w&#39;) as seq:
                json.dump(self.split_sequence, seq)
            if &#39;text&#39; in self.tags.keys():  # if &#39;txt_list&#39; in self.createarray.__dict__.keys():
                with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                          &#39;w&#39;) as fp:
                    json.dump(self.createarray.txt_list, fp)
        else:
            # Сохранение датасета с NumPy
            for key in self.instructions[&#39;inputs&#39;].keys():
                x: list = []
                for i in range(self.limit):
                    x.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][i],
                        **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;]))
                self.X[&#39;train&#39;][key] = np.array(x)[self.split_sequence[&#39;train&#39;]]
                self.X[&#39;val&#39;][key] = np.array(x)[self.split_sequence[&#39;val&#39;]]
                self.X[&#39;test&#39;][key] = np.array(x)[self.split_sequence[&#39;test&#39;]]

            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    y_1: list = []
                    y_2: list = []
                    y_3: list = []
                    for i in range(self.limit):
                        arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                            self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                            **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                        y_1.append(arrays[0])
                        y_2.append(arrays[1])
                        y_3.append(arrays[2])

                    splits = [&#39;train&#39;, &#39;val&#39;, &#39;test&#39;]
                    for spl_seq in splits:
                        for i in range(len(splits)):
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1])))] = np.array(y_1)[
                                self.split_sequence[spl_seq]]
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 1))] = np.array(y_2)[
                                self.split_sequence[spl_seq]]
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 2))] = np.array(y_3)[
                                self.split_sequence[spl_seq]]
                else:
                    y: list = []
                    for i in range(self.limit):
                        y.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                            self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                            **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;]))
                    self.Y[&#39;train&#39;][key] = np.array(y)[self.split_sequence[&#39;train&#39;]]
                    self.Y[&#39;val&#39;][key] = np.array(y)[self.split_sequence[&#39;val&#39;]]
                    self.Y[&#39;test&#39;][key] = np.array(y)[self.split_sequence[&#39;test&#39;]]

            for sample in self.X.keys():
                os.makedirs(os.path.join(self.trds_path, &#39;arrays&#39;, sample), exist_ok=True)
                for inp in self.X[sample].keys():
                    os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                    joblib.dump(self.X[sample][inp],
                                os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

            for sample in self.Y.keys():
                for inp in self.Y[sample].keys():
                    os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                    joblib.dump(self.Y[sample][inp],
                                os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

        if self.createarray.scaler:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;), exist_ok=True)
        if self.createarray.tokenizer:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;), exist_ok=True)
        if self.createarray.word2vec:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;), exist_ok=True)
        if self.createarray.augmentation:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;), exist_ok=True)
        # if self.createarray.tsgenerator:
        #     os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;), exist_ok=True)

        for scaler in self.createarray.scaler.keys():
            if self.createarray.scaler[scaler]:
                joblib.dump(self.createarray.scaler[scaler],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;, f&#39;{scaler}.gz&#39;))
        for tok in self.createarray.tokenizer.keys():
            if self.createarray.tokenizer[tok]:
                joblib.dump(self.createarray.tokenizer[tok],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;, f&#39;{tok}.gz&#39;))
        for w2v in self.createarray.word2vec.keys():
            if self.createarray.word2vec[w2v]:
                joblib.dump(self.createarray.word2vec[w2v],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;, f&#39;{w2v}.gz&#39;))
        for aug in self.createarray.augmentation.keys():
            if self.createarray.augmentation[aug]:
                joblib.dump(self.createarray.augmentation[aug],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;, f&#39;{aug}.gz&#39;))
        # for tsg in self.createarray.tsgenerator.keys():
        #     if self.createarray.tsgenerator[tsg]:
        #         joblib.dump(self.createarray.tsgenerator[tsg],
        #                     os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;, f&#39;{tsg}.gz&#39;))

        attributes = [&#39;name&#39;, &#39;source&#39;, &#39;tags&#39;, &#39;user_tags&#39;, &#39;language&#39;,
                      &#39;input_datatype&#39;, &#39;input_dtype&#39;, &#39;input_shape&#39;, &#39;input_names&#39;,
                      &#39;output_datatype&#39;, &#39;output_dtype&#39;, &#39;output_shape&#39;, &#39;output_names&#39;,
                      &#39;num_classes&#39;, &#39;classes_names&#39;, &#39;classes_colors&#39;,
                      &#39;one_hot_encoding&#39;, &#39;task_type&#39;, &#39;limit&#39;, &#39;use_generator&#39;]

        for attr in attributes:
            data[attr] = self.__dict__[attr]
        data[&#39;date&#39;] = datetime.now().astimezone(timezone(&#39;Europe/Moscow&#39;)).isoformat()
        with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;config.json&#39;), &#39;w&#39;) as fp:
            json.dump(data, fp)

        pass

    def instructions_images(self, **options):

        instructions: dict = {}
        instr: list = []
        y_cls: list = []
        cls_idx = 0
        peg_idx = 0
        self.peg.append(0)
        options[&#39;put&#39;] = f&#39;{self.mode}_{self.iter}&#39;
        if &#39;object_detection&#39; in self.tags.values():
            options[&#39;object_detection&#39;] = True
        if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_folder&#39;:
            for folder_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                for directory, folder, file_name in sorted(os.walk(os.path.join(self.file_folder, folder_name))):
                    if file_name:
                        file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
                        for name in sorted(file_name):
                            if &#39;object_detection&#39; in self.tags.values():
                                if &#39;txt&#39; not in name:
                                    instr.append(os.path.join(file_folder, name))
                                    peg_idx += 1
                            else:
                                instr.append(os.path.join(file_folder, name))
                                peg_idx += 1
                            y_cls.append(cls_idx)
                        cls_idx += 1
                        self.peg.append(peg_idx)
            self.y_cls = y_cls
        elif options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
            for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                data = pd.read_csv(os.path.join(self.file_folder, file_name),
                                   usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
                instr = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
                prev_elem = instr[0].split(&#39;/&#39;)[-2]
                for elem in instr:
                    cur_elem = elem.split(&#39;/&#39;)[-2]
                    if cur_elem != prev_elem:
                        self.peg.append(peg_idx)
                    prev_elem = cur_elem
                    peg_idx += 1
                self.peg.append(len(instr))

        if &#39;augmentation&#39; in options.keys():
            aug_parameters = []
            for key, value in options[&#39;augmentation&#39;].items():
                aug_parameters.append(getattr(iaa, key)(**value))
            self.createarray.augmentation[f&#39;{self.mode}_{self.iter}&#39;] = iaa.Sequential(aug_parameters,
                                                                                       random_order=True)
        del options[&#39;augmentation&#39;]
        instructions[&#39;instructions&#39;] = instr
        instructions[&#39;parameters&#39;] = options

        return instructions

    def instructions_video(self, **options):

        instructions: dict = {}
        instr: list = []
        y_cls: list = []
        cls_idx = 0
        peg_idx = 0
        self.peg.append(0)

        path = self.file_folder
        if options[&#39;folder_name&#39;]:
            path = os.path.join(self.file_folder, options[&#39;folder_name&#39;])
        for directory, folder, file_name in sorted(os.walk(path)):
            if file_name:
                file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
                for name in sorted(file_name):
                    instr.append(os.path.join(file_folder, name))
                    peg_idx += 1
                    if options[&#39;class_mode&#39;] == &#39;По каждому кадру&#39;:
                        y_cls.append(np.full((options[&#39;max_frames&#39;], 1), cls_idx).tolist())
                    else:
                        y_cls.append(cls_idx)
                cls_idx += 1
                self.peg.append(peg_idx)
        instructions[&#39;instructions&#39;] = instr
        instructions[&#39;parameters&#39;] = options
        self.y_cls = y_cls

        return instructions

    def instructions_text(self, **options):

        def read_text(file_path):

            del_symbols = [&#39;\n&#39;, &#39;\t&#39;, &#39;\ufeff&#39;]
            if options[&#39;delete_symbols&#39;]:
                del_symbols += options[&#39;delete_symbols&#39;].split(&#39; &#39;)

            with io_open(file_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;) as f:
                text = f.read()
                for del_symbol in del_symbols:
                    text = text.replace(del_symbol, &#39; &#39;)
            for put, tag in self.tags.items():
                if tag == &#39;text_segmentation&#39;:
                    open_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][0]
                    close_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][-1]
                    text = re.sub(open_symbol, f&#34; {open_symbol}&#34;, text)
                    text = re.sub(close_symbol, f&#34;{close_symbol} &#34;, text)
                    break

            return text

        def apply_pymorphy(text, morphy) -&gt; list:

            words_list = text.split(&#39; &#39;)
            words_list = [morphy.parse(w)[0].normal_form for w in words_list]

            return words_list

        txt_list: dict = {}

        if options[&#39;folder_name&#39;]:
            for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
                txt_list[os.path.join(options[&#39;folder_name&#39;], file_name)] = read_text(
                    os.path.join(self.file_folder, options[&#39;folder_name&#39;], file_name))
        else:
            tree = os.walk(self.file_folder)
            for directory, folder, file_name in sorted(tree):
                if bool(file_name) is not False:
                    folder_name = directory.split(os.path.sep)[-1]
                    for name in sorted(file_name):
                        text_file = read_text(os.path.join(directory, name))
                        if text_file:
                            txt_list[os.path.join(folder_name, name)] = text_file
                else:
                    continue

        #################################################
        if options[&#39;pymorphy&#39;]:
            pymorphy = pymorphy2.MorphAnalyzer()
            for i in range(len(txt_list)):
                txt_list[i] = apply_pymorphy(txt_list[i], pymorphy)
        #################################################

        filters = &#39;–—!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^«»№_`{|}~\t\n\xa0–\ufeff&#39;
        for key, value in self.tags.items():
            if value == &#39;text_segmentation&#39;:
                open_tags = self.user_parameters[key][&#39;open_tags&#39;]
                close_tags = self.user_parameters[key][&#39;close_tags&#39;]
                tags = f&#39;{open_tags} {close_tags}&#39;
                for ch in filters:
                    if ch in set(tags):
                        filters = filters.replace(ch, &#39;&#39;)
                break

        self.createarray.create_tokenizer(self.mode, self.iter, **{&#39;num_words&#39;: options[&#39;max_words_count&#39;],
                                                                   &#39;filters&#39;: filters,
                                                                   &#39;lower&#39;: True,
                                                                   &#39;split&#39;: &#39; &#39;,
                                                                   &#39;char_level&#39;: False,
                                                                   &#39;oov_token&#39;: &#39;&lt;UNK&gt;&#39;})
        self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].fit_on_texts(list(txt_list.values()))

        self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}
        for key, value in txt_list.items():
            self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key] = \
                self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].texts_to_sequences([value])[0]

        if options[&#39;word_to_vec&#39;]:
            reverse_tok = {}
            for key, value in self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].word_index.items():
                reverse_tok[value] = key
            words = []
            for key in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys():
                for lst in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]:
                    tmp = []
                    for word in lst:
                        tmp.append(reverse_tok[word])
                    words.append(tmp)
            self.createarray.create_word2vec(mode=self.mode, iteration=self.iter, words=words,
                                             size=options[&#39;word_to_vec_size&#39;], window=10, min_count=1, workers=10,
                                             iter=10)

        instr = []
        if &#39;text_segmentation&#39; not in self.tags.values():
            y_cls = []
            cls_idx = 0
            length = options[&#39;x_len&#39;]
            stride = options[&#39;step&#39;]
            peg_idx = 0
            self.peg.append(0)
            for key in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
                index = 0
                while index + length &lt;= len(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]):
                    instr.append({&#39;file&#39;: key, &#39;slice&#39;: [index, index + length]})
                    peg_idx += 1
                    index += stride
                    y_cls.append(cls_idx)
                self.peg.append(peg_idx)
                cls_idx += 1
            self.y_cls = y_cls
        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;bag_of_words&#39;: options[&#39;bag_of_words&#39;],
                                       &#39;word_to_vec&#39;: options[&#39;word_to_vec&#39;],
                                       &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;
                                       }
                        }

        return instructions

    def instructions_audio(self):

        pass

    def instructions_dataframe(self, **options):
        &#34;&#34;&#34;
            Args:
                **options: Параметры датафрейма:
                    MinMaxScaler: строка номеров колонок для обработки
                    StandardScaler: строка номеров колонок для обработки
                    Categorical: строка номеров колонок для обработки c уже готовыми категориями
                    Categorical_ranges: dict для присваивания категории  в зависимости от диапазона данных
                        num_cols: число колонок
                        cols: номера колонок
                        col_(int): строка с диапазонами
                    one_hot_encoding: строка номеров колонок для перевода категорий в ОНЕ
                    file_name: имя файла.csv
                    y_col: столбец датафрейма для классификации
            Returns:
                instructions: dict      Словарь с инструкциями для create_dataframe.
        &#34;&#34;&#34;

        def str_to_list(str_numbers, df_cols):
            &#34;&#34;&#34;
            Получает строку из пользовательских номеров колонок,
            возвращает лист индексов данных колонок
            &#34;&#34;&#34;
            merged = []
            try:
                str_numbers = str_numbers.split(&#39; &#39;)
            except:
                print(&#39;Разделите номера колонок ТОЛЬКО пробелами&#39;)
            for i in range(len(str_numbers)):
                if &#39;-&#39; in str_numbers[i]:
                    idx = str_numbers[i].index(&#39;-&#39;)
                    fi = int(str_numbers[i][:idx]) - 1
                    si = int(str_numbers[i][idx + 1:])
                    tmp = list(range(fi, si))
                    merged.extend(tmp)
                elif re.findall(r&#39;\D&#39;, str_numbers[i]) != []:
                    merged.append(df_cols.to_list().index(str_numbers[i]))
                else:
                    merged.append(int(str_numbers[i]) - 1)

            return merged

        general_df = pd.read_csv(os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), nrows=1)
        self.createarray.df_with_y = pd.read_csv(
            os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), usecols=(str_to_list(
                options[&#39;file_info&#39;][&#39;cols_name&#39;][0], general_df.columns) + str_to_list(options[&#39;y_col&#39;],
                                                                                        general_df.columns)))
        self.createarray.df_with_y.sort_values(by=options[&#39;y_col&#39;], inplace=True, ignore_index=True)

        self.peg.append(0)
        for i in range(len(self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]]) - 1):
            if self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i] != \
                    self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i + 1]:
                self.peg.append(i + 1)
        self.peg.append(len(self.createarray.df_with_y))

        self.createarray.df = self.createarray.df_with_y.iloc[:, str_to_list(
            options[&#39;file_info&#39;][&#39;cols_name&#39;][0], self.createarray.df_with_y.columns)]

        instructions = {&#39;instructions&#39;: np.arange(0, len(self.createarray.df)).tolist(),
                        &#39;parameters&#39;: {&#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}}

        if &#39;MinMaxScaler&#39; or &#39;StandardScaler&#39; in options.keys():
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;] = {}
            if &#39;MinMaxScaler&#39; in options.keys():
                instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;] = str_to_list(str_numbers=options[&#39;MinMaxScaler&#39;],
                                                                         df_cols=self.createarray.df.columns)
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;] = MinMaxScaler()
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;].fit(
                    self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;]].to_numpy().reshape(-1, 1))

            if &#39;StandardScaler&#39; in options.keys():
                instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;] = str_to_list(options[&#39;StandardScaler&#39;],
                                                                           self.createarray.df.columns)
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;] = StandardScaler()
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;].fit(
                    self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;]].to_numpy().reshape(-1, 1))

        if &#39;Categorical&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;Categorical&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;Categorical&#39;],
                                                                                self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;]:
                instructions[&#39;parameters&#39;][&#39;Categorical&#39;][f&#39;col_{i}&#39;] = np.unique(
                    self.createarray.df.iloc[:, i]).tolist()

        if &#39;Categorical_ranges&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;] = str_to_list(
                options[&#39;Categorical_ranges&#39;][&#39;cols&#39;], self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;] = {}
                for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))):
                    instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;] = int(
                        options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;)[j])

        if &#39;one_hot_encoding&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;one_hot_encoding&#39;],
                                                                                     self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
                if i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                    instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                        options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))
                else:
                    instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                        np.unique(self.createarray.df.iloc[:, i]))

        return instructions

    def instructions_classification(self, **options):

        instructions: dict = {}
        self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;classification&#39;
        self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;one_hot_encoding&#39;]

        if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
            for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                data = pd.read_csv(os.path.join(self.file_folder, file_name), usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
                column = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
                classes_names = []
                for elem in column:
                    if elem not in classes_names:
                        classes_names.append(elem)
                self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = classes_names
                self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(classes_names)
                for elem in column:
                    self.y_cls.append(classes_names.index(elem))

        else:
            for key, value in self.tags.items():
                if value in [&#39;images&#39;, &#39;text&#39;, &#39;audio&#39;, &#39;video&#39;]:
                    self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = \
                        sorted(self.user_parameters[key][&#39;file_info&#39;][&#39;path&#39;])
                    self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(self.classes_names[f&#39;{self.mode}_{self.iter}&#39;])

        instructions[&#39;parameters&#39;] = {&#39;num_classes&#39;: len(np.unique(self.y_cls)),
                                      &#39;one_hot_encoding&#39;: options[&#39;one_hot_encoding&#39;]}
        instructions[&#39;instructions&#39;] = self.y_cls

        return instructions

    def instructions_regression(self):

        pass

    def instructions_segmentation(self, **options):

        instr: list = []

        self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_names&#39;]
        self.classes_colors[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_colors&#39;]
        self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(options[&#39;classes_names&#39;])
        self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = True
        self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;segmentation&#39;

        for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
            instr.append(os.path.join(options[&#39;folder_name&#39;], file_name))

        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;mask_range&#39;: options[&#39;mask_range&#39;],
                                       &#39;num_classes&#39;: len(options[&#39;classes_names&#39;]),
                                       &#39;shape&#39;: (self.user_parameters[&#39;input_1&#39;][&#39;height&#39;],
                                                 self.user_parameters[&#39;input_1&#39;][&#39;width&#39;]),
                                       &#39;classes_colors&#39;: options[&#39;classes_colors&#39;]
                                       }
                        }

        return instructions

    def instructions_object_detection(self, **options):

        data = {}
        instructions = {}
        parameters = {}
        class_names = []

        # obj.data
        with open(os.path.join(self.file_folder, &#39;obj.data&#39;), &#39;r&#39;) as dt:
            d = dt.read()
        for elem in d.split(&#39;\n&#39;):
            if elem:
                elem = elem.split(&#39; = &#39;)
                data[elem[0]] = elem[1]

        for key, value in self.tags.items():
            if value == &#39;images&#39;:
                parameters[&#39;height&#39;] = self.user_parameters[key][&#39;height&#39;]
                parameters[&#39;width&#39;] = self.user_parameters[key][&#39;width&#39;]
                parameters[&#39;num_classes&#39;] = int(data[&#39;classes&#39;])

        # obj.names
        with open(os.path.join(self.file_folder, data[&#34;names&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
            names = dt.read()
        for elem in names.split(&#39;\n&#39;):
            if elem:
                class_names.append(elem)

        for i in range(3):
            self.classes_names[f&#39;{self.mode}_{self.iter + i}&#39;] = class_names
            self.num_classes[f&#39;{self.mode}_{self.iter + i}&#39;] = int(data[&#39;classes&#39;])

        # list of txt
        txt_list = []
        with open(os.path.join(self.file_folder, data[&#34;train&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
            images = dt.read()
        for elem in sorted(images.split(&#39;\n&#39;)):
            if elem:
                idx = elem.rfind(&#39;.&#39;)
                elem = elem.replace(elem[idx:], &#39;.txt&#39;)
                txt_list.append(os.path.join(*elem.split(&#39;/&#39;)[1:]))
        instructions[&#39;instructions&#39;] = txt_list
        instructions[&#39;parameters&#39;] = parameters

        return instructions

    def instructions_text_segmentation(self, **options):

        &#34;&#34;&#34;

        Args:
            **options:
                open_tags: str
                    Открывающие теги.
                close_tags: str
                    Закрывающие теги.

        Returns:

        &#34;&#34;&#34;

        def get_ohe_samples(list_of_txt, tags_index):

            segment_array = []
            new_list_of_txt = []
            tag_place = [0 for _ in range(len(open_tags))]
            for ex in list_of_txt:
                if ex in tags_index:
                    place = np.argwhere(tags_index == ex)
                    if len(place) != 0:
                        if place[0][0] &lt; len(open_tags):
                            tag_place[place[0][0]] = 1
                        else:
                            tag_place[place[0][0] - len(open_tags)] = 0
                else:
                    new_list_of_txt.append(ex)
                    segment_array.append(np.where(np.array(tag_place) == 1)[0].tolist())

            return new_list_of_txt, segment_array

        instr: list = []
        open_tags: list = options[&#39;open_tags&#39;].split(&#39; &#39;)
        close_tags: list = options[&#39;close_tags&#39;].split(&#39; &#39;)
        tags: list = open_tags + close_tags
        self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}

        for key, value in self.tags.items():
            if value == &#39;text&#39;:
                tags_indexes = np.array([self.createarray.tokenizer[key].word_index[idx] for idx in tags])
                for txt_file in self.createarray.txt_list[key].keys():
                    text_instr, segment_instr = get_ohe_samples(self.createarray.txt_list[key][txt_file], tags_indexes)
                    self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][txt_file] = segment_instr
                    self.createarray.txt_list[key][txt_file] = text_instr

                length = self.user_parameters[key][&#39;x_len&#39;]
                stride = self.user_parameters[key][&#39;step&#39;]
                peg_idx = 0
                self.peg = []
                self.peg.append(0)
                for path in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
                    index = 0
                    while index + length &lt;= len(self.createarray.txt_list[key][path]):
                        instr.append({&#39;file&#39;: path, &#39;slice&#39;: [index, index + length]})
                        peg_idx += 1
                        index += stride
                    self.peg.append(peg_idx)
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;] = instr

        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;num_classes&#39;: len(open_tags),
                                       &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}
                        }

        return instructions


class Dataloader(object):

    def __init__(self, path=mkdtemp(), trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;):

        self.file_folder: str = &#39;&#39;
        self.save_path = path
        self.trds_path = trds_path
        self.django_flag = False

        pass

    def _get_zipfiles(self) -&gt; list:

        return os.listdir(os.path.join(self.trds_path, &#39;sources&#39;))

    @staticmethod
    def unzip(file_folder: str, zip_name: str):

        file_path = pathlib.Path(os.path.join(file_folder, &#39;tmp&#39;, zip_name))
        temp_folder = os.path.join(file_folder, &#39;tmp&#39;)
        os.makedirs(temp_folder, exist_ok=True)
        shutil.unpack_archive(file_path, file_folder)
        shutil.rmtree(temp_folder, ignore_errors=True)

        pass

    @staticmethod
    def download(link: str, file_folder: str, file_name: str):

        resp = requests.get(link, stream=True)
        total = int(resp.headers.get(&#39;content-length&#39;, 0))
        idx = 0
        with open(os.path.join(file_folder, &#39;tmp&#39;, file_name), &#39;wb&#39;) as out_file, tqdm(
                desc=f&#34;Загрузка архива {file_name}&#34;, total=total, unit=&#39;iB&#39;, unit_scale=True,
                unit_divisor=1024) as progress_bar:
            for data in resp.iter_content(chunk_size=1024):
                size = out_file.write(data)
                progress_bar.update(size)
                idx += size
                # if self.django_flag:
                #     if idx % 143360 == 0 or idx == progress_bar.total:
                #         progress_bar_status = (progress_bar.desc, str(round(idx / progress_bar.total, 2)),
                #                            f&#39;{str(round(progress_bar.last_print_t - progress_bar.start_t, 2))} сек.&#39;)
                #         if idx == progress_bar.total:
                #             self.Exch.print_progress_bar(progress_bar_status, stop_flag=True)
                #         else:
                #             self.Exch.print_progress_bar(progress_bar_status)

        pass

    def load_data(self, strict_object):

        if strict_object.mode == SourceModeChoice.Terra:
            self.load_from_terra(strict_object.value)
        elif strict_object.mode == SourceModeChoice.URL:
            self.load_from_url(strict_object.value)
        elif strict_object.mode == SourceModeChoice.GoogleDrive:
            self.load_from_google(strict_object.value)

        pass

    def load_from_terra(self, name: str):

        file_folder = None
        data = {
            &#39;трейдинг&#39;: [&#39;trading.zip&#39;],
            &#39;автомобили&#39;: [&#39;cars.zip&#39;],
            &#39;умный_дом&#39;: [&#39;smart_home.zip&#39;],
            &#39;квартиры&#39;: [&#39;flats.zip&#39;],
            # &#39;диалоги&#39;: [&#39;dialog.txt&#39;],
            &#39;автомобили_3&#39;: [&#39;cars_3.zip&#39;],
            &#39;заболевания&#39;: [&#39;diseases.zip&#39;],
            &#39;договоры&#39;: [&#39;docs.zip&#39;],
            &#39;самолеты&#39;: [&#39;planes.zip&#39;],
            # &#39;болезни&#39;: [&#39;origin.zip&#39;, &#39;segmentation.zip&#39;],
            &#39;губы&#39;: [&#39;lips.zip&#39;],
            # &#39;жанры_музыки&#39;: [&#39;genres.zip&#39;],
            &#39;sber&#39;: [&#39;sber.zip&#39;]
        }

        for file_name in data[name]:
            file_folder = pathlib.Path(self.save_path).joinpath(name)
            os.makedirs(file_folder, exist_ok=True)
            os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
            link = &#39;https://storage.googleapis.com/terra_ai/DataSets/Numpy/&#39; + file_name
            self.download(link, file_folder, file_name)
            if &#39;zip&#39; in file_name:
                self.unzip(file_folder, file_name)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass

    def load_from_url(self, link: str):

        file_name = link.split(&#39;/&#39;)[-1]
        file_folder = pathlib.Path(os.path.join(self.save_path, file_name))
        if &#39;.&#39; in file_name:
            name = file_name[:file_name.rfind(&#39;.&#39;)]
            file_folder = pathlib.Path(os.path.join(self.save_path, name))
        os.makedirs(file_folder, exist_ok=True)
        os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
        self.download(link, file_folder, file_name)
        if &#39;zip&#39; in file_name or &#39;zip&#39; in link:
            self.unzip(file_folder, file_name)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass

    def load_from_google(self, filepath: str):

        zip_name = str(filepath).split(&#39;/&#39;)[-1]
        name = zip_name[:zip_name.rfind(&#39;.&#39;)]
        file_folder = os.path.join(self.save_path, name)
        shutil.unpack_archive(filepath, file_folder)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass


class CreateArray(object):

    def __init__(self, **options):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.augmentation: dict = {}
        self.temporary: dict = {&#39;bounding_boxes&#39;: {}}
        self.df = None

        self.file_folder = None
        self.txt_list: dict = {}

        for key, value in options.items():
            self.__dict__[key] = value

    @staticmethod
    def yolo_to_imgaug(args, shape):

        height, width = shape

        class_num = int(args[0])
        x_pos = float(args[1])
        y_pos = float(args[2])
        x_size = float(args[3])
        y_size = float(args[4])

        x1 = x_pos * width - (x_size * width / 2)
        y1 = y_pos * height - (y_size * height / 2)
        x2 = x_size * width + x1
        y2 = y_size * height + y1

        return [class_num, x1, y1, x2, y2]

    @staticmethod
    def imgaug_to_yolo(args, shape=(416, 416)):

        height, width = shape

        class_num = int(args[0])
        x1 = float(args[1])
        y1 = float(args[2])
        x2 = float(args[3])
        y2 = float(args[4])

        x_pos = x1 / width + ((x2 - x1) / width / 2)
        y_pos = y1 / height + ((y2 - y1) / height / 2)
        x_size = (x2 - x1) / width
        y_size = (y2 - y1) / height

        return_args = [class_num, x_pos, y_pos, x_size, y_size]

        for r in return_args[1:]:
            if r &gt; 1:
                return ()
            if r &lt; 0:
                return ()

        return return_args

    def create_images(self, image_path: str, **options):

        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
        array = img_to_array(img, dtype=np.uint8)
        if options[&#39;net&#39;] == &#39;Linear&#39;:
            array = array.reshape(np.prod(np.array(array.shape)))
        if self.augmentation[options[&#39;put&#39;]]:
            if &#39;object_detection&#39; in options.keys():
                txt_path = image_path[:image_path.rfind(&#39;.&#39;)] + &#39;.txt&#39;
                with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as b_boxes:
                    bounding_boxes = b_boxes.read()

                current_boxes = []
                for elem in bounding_boxes.split(&#39;\n&#39;):
                    b_box = self.yolo_to_imgaug(elem.split(&#39; &#39;), shape=array.shape[:2])
                    current_boxes.append(
                        BoundingBox(
                            **{&#39;label&#39;: b_box[0], &#39;x1&#39;: b_box[1], &#39;y1&#39;: b_box[2], &#39;x2&#39;: b_box[3], &#39;y2&#39;: b_box[4]}))

                bbs = BoundingBoxesOnImage(current_boxes, shape=array.shape)
                array, bbs_aug = self.augmentation[options[&#39;put&#39;]](image=array, bounding_boxes=bbs)
                list_of_bounding_boxes = []
                for elem in bbs_aug.remove_out_of_image().clip_out_of_image().bounding_boxes:
                    bb = elem.__dict__
                    b_box_coord = self.imgaug_to_yolo([bb[&#39;label&#39;], bb[&#39;x1&#39;], bb[&#39;y1&#39;], bb[&#39;x2&#39;], bb[&#39;y2&#39;]],
                                                      shape=array.shape[:2])
                    if b_box_coord != ():
                        list_of_bounding_boxes.append(b_box_coord)

                self.temporary[&#39;bounding_boxes&#39;][txt_path] = list_of_bounding_boxes
            else:
                array = self.augmentation[options[&#39;put&#39;]](image=array)

        return array

    def create_video(self, video_path, **options) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            video_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота кадра.
                width: int
                    Ширина кадра.
                max_frames: int
                    Максимальное количество кадров.
                mode: str
                    Режим обработки кадра (Сохранить пропорции, Растянуть).
                x_len: int
                    Длина окна выборки.
                step: int
                    Шаг окна выборки.

        Returns:
            array: np.ndarray
                Массив видео.

        &#34;&#34;&#34;

        def resize_frame(one_frame, original_shape, target_shape, mode):

            resized = None

            if mode == &#39;Растянуть&#39;:
                resized = resize_layer(one_frame[None, ...])
                resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
            elif mode == &#39;Сохранить пропорции&#39;:
                # height
                resized = one_frame.copy()
                if original_shape[0] &gt; target_shape[0]:
                    resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(
                        original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
                else:
                    black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3),
                                         dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized))
                    resized = np.concatenate((resized, black_bar))
                # width
                if original_shape[1] &gt; target_shape[1]:
                    resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(
                        original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
                else:
                    black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3),
                                         dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized), axis=1)
                    resized = np.concatenate((resized, black_bar), axis=1)

            return resized

        array = []
        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        resize_layer = Resizing(*shape)

        cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
        height = int(cap.get(4))
        width = int(cap.get(3))
        # fps = int(cap.get(5))
        frame_count = int(cap.get(7))
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                if shape != (height, width):
                    frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
                frame = frame[:, :, [2, 1, 0]]
                array.append(frame)
                if len(array) == options[&#39;max_frames&#39;]:
                    break
        finally:
            cap.release()

        array = np.array(array)
        if frame_count &lt; options[&#39;max_frames&#39;]:
            add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
            array = np.concatenate((array, add_frames), axis=0)

        return array

    def create_text(self, sample: dict, **options):

        &#34;&#34;&#34;

        Args:
            sample: dict
                - file: Название файла.
                - slice: Индексы рассматриваемой части последовательности
            **options: Параметры обработки текста:
                bag_of_words: Tokenizer object, bool
                    Перевод в формат bag_of_words.
                word_to_vec: Word2Vec object, bool
                    Перевод в векторное представление Word2Vec.
                put: str
                    Индекс входа или выхода.

        Returns:
            array: np.ndarray
                Массив текстового вектора.
        &#34;&#34;&#34;

        filepath: str = sample[&#39;file&#39;]
        slicing: list = sample[&#39;slice&#39;]
        array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

        for key, value in options.items():
            if value:
                if key == &#39;bag_of_words&#39;:
                    array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
                elif key == &#39;word_to_vec&#39;:
                    reverse_tok = {}
                    words_list = []
                    for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                        reverse_tok[index] = word
                    for idx in array:
                        words_list.append(reverse_tok[idx])
                    array = []
                    for word in words_list:
                        array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
                break

        array = np.array(array)

        return array

    def create_audio(self):

        pass

    def create_dataframe(self, row_number: int, **options):
        &#34;&#34;&#34;
            Args:
                row_number: номер строки с сырыми данными датафрейма,
                **options: Параметры обработки колонок:
                    MinMaxScaler: лист индексов колонок для обработки
                    StandardScaler: лист индексов колонок для обработки
                    Categorical: лист индексов колонок для перевода по готовым категориям
                    Categorical_ranges: лист индексов колонок для перевода по категориям по диапазонам
                    one_hot_encoding: лист индексов колонок для перевода в ОНЕ
                    put: str  Индекс входа или выхода.
            Returns:
                array: np.ndarray
                    Массив вектора обработанных данных.
        &#34;&#34;&#34;
        row = self.df.loc[row_number].copy().tolist()

        if &#39;StandardScaler&#39; in options.keys():
            for i in options[&#39;StandardScaler&#39;]:
                row[i] = self.scaler[options[&#39;put&#39;]][&#39;StandardScaler&#39;].transform(
                    np.array([row[i]]).reshape(-1, 1)).tolist()

        if &#39;MinMaxScaler&#39; in options.keys():
            for i in options[&#39;MinMaxScaler&#39;]:
                row[i] = self.scaler[options[&#39;put&#39;]][&#39;MinMaxScaler&#39;].transform(
                    np.array(row[i]).reshape(-1, 1)).tolist()

        if &#39;Categorical&#39; in options.keys():
            for i in options[&#39;Categorical&#39;][&#39;lst_cols&#39;]:
                row[i] = list(options[&#39;Categorical&#39;][f&#39;col_{i}&#39;]).index(row[i])

        if &#39;Categorical_ranges&#39; in options.keys():
            for i in options[&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;])):
                    if row[i] &lt;= options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;]:
                        row[i] = j
                        break

        if &#39;one_hot_encoding&#39; in options.keys():
            for i in options[&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
                row[i] = utils.to_categorical(row[i], options[&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;], dtype=&#39;uint8&#39;).tolist()

        array = []
        for i in row:
            if type(i) == list:
                if type(i[0]) == list:
                    array.extend(i[0])
                else:
                    array.extend(i)
            else:
                array.append(i)

        array = np.array(array)

        return array

    def create_classification(self, index, **options):

        if options[&#39;one_hot_encoding&#39;]:
            index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
        index = np.array(index)

        return index

    def create_regression(self):

        pass

    def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            image_path: str
                Путь к файлу
            **options: Параметры сегментации:
                mask_range: int
                    Диапазон для каждого из RGB каналов.
                num_classes: int
                    Общее количество классов.
                shape: tuple
                    Размер картинки (высота, ширина).
                classes_colors: list
                    Список цветов для каждого класса.

        Returns:
            array: np.ndarray
                Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

        &#34;&#34;&#34;

        def cluster_to_ohe(mask_image):

            mask_image = mask_image.reshape(-1, 3)
            km = KMeans(n_clusters=options[&#39;num_classes&#39;])
            km.fit(mask_image)
            labels = km.labels_
            cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
            cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
            cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

            mask_ohe = np.zeros(options[&#39;shape&#39;])
            for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
                mask = np.zeros(options[&#39;shape&#39;])

                for j, cl_rgb in enumerate(cl_cent):
                    if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                            rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                            rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                        mask = cl_mask[:, :, j]

                if k == 0:
                    mask_ohe = mask
                else:
                    mask_ohe = np.dstack((mask_ohe, mask))

            return mask_ohe

        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
        array = img_to_array(img, dtype=np.uint8)
        array = cluster_to_ohe(array)

        return array

    def create_text_segmentation(self, sample: dict, **options):

        array = []

        for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
            tags = [0 for _ in range(options[&#39;num_classes&#39;])]
            if elem:
                for idx in elem:
                    tags[idx] = 1
            array.append(tags)
        array = np.array(array, dtype=&#39;uint8&#39;)

        return array

    def create_timeseries(self):

        pass

    def create_object_detection(self, txt_path: str, **options):

        &#34;&#34;&#34;

        Args:
            txt_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота изображения.
                width: int
                    Ширина изображения.
                num_classes: tuple
                    Количество классов.

        Returns:
            array: np.ndarray
                Массивы в трёх выходах.

        &#34;&#34;&#34;

        height: int = options[&#39;height&#39;]
        width: int = options[&#39;width&#39;]
        num_classes: int = options[&#39;num_classes&#39;]
        zero_boxes_flag: bool = False

        if self.temporary[&#39;bounding_boxes&#39;]:
            real_boxes = self.temporary[&#39;bounding_boxes&#39;][txt_path]
        else:
            with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
                bb_file = txt.read()
            real_boxes = []
            for elem in bb_file.split(&#39;\n&#39;):
                tmp = []
                if elem:
                    for num in elem.split(&#39; &#39;):
                        tmp.append(float(num))
                    real_boxes.append(tmp)

        if not real_boxes:
            zero_boxes_flag = True
            real_boxes = [[0, 0, 0, 0, 0]]
        real_boxes = np.array(real_boxes)
        real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
        anchors = np.array(
            [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
        num_layers = 3
        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

        real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
        input_shape = np.array((height, width), dtype=&#39;int32&#39;)

        boxes_wh = real_boxes[..., 2:4] * input_shape

        cells = [13, 26, 52]
        y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
                  range(num_layers)]
        box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

        anchor_area = anchors[:, 0] * anchors[:, 1]
        for r in range(len(real_boxes)):
            correct_anchors = []
            for anchor in anchors:
                correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
            correct_anchors = np.array(correct_anchors)
            correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
            iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
            best_anchor = np.argmax(iou, axis=-1)

            for m in range(num_layers):
                if best_anchor in anchor_mask[m]:
                    h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                    j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                    k = anchor_mask[m].index(int(best_anchor))
                    c = real_boxes[r, 4].astype(&#39;int32&#39;)
                    y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                    y_true[m][j, h, k, 4] = 0 if zero_boxes_flag else 1
                    y_true[m][j, h, k, 5 + c] = 0 if zero_boxes_flag else 1

        return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            words: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data


class Preprocessing(object):

    def __init__(self):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, x_word: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            x_word: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(x_word, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data


class PrepareDTS(object):

    def __init__(self, trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;):

        self.name: str = &#39;&#39;
        self.source: str = &#39;&#39;
        self.language = None
        self.trds_path: str = trds_path
        self.input_shape: dict = {}
        self.input_dtype: dict = {}
        self.input_datatype: str = &#39;&#39;
        self.input_names: dict = {}
        self.output_shape: dict = {}
        self.output_dtype: dict = {}
        self.output_datatype: dict = {}
        self.output_names: dict = {}
        self.split_sequence: dict = {}
        self.file_folder: str = &#39;&#39;
        self.use_generator: bool = False
        self.zip_params: dict = {}
        self.instructions: dict = {&#39;inputs&#39;: {}, &#39;outputs&#39;: {}}
        self.tags: dict = {}
        self.task_type: dict = {}
        self.one_hot_encoding: dict = {}
        self.num_classes: dict = {}
        self.classes_names: dict = {}
        self.classes_colors: dict = {}
        self.dts_prepared: bool = False

        self.dataloader = None
        self.createarray = CreateArray()

        self.X: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.Y: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.dataset: dict = {}

        pass

    @staticmethod
    def _set_datatype(shape) -&gt; str:

        datatype = {0: &#39;DIM&#39;,
                    1: &#39;DIM&#39;,
                    2: &#39;DIM&#39;,
                    3: &#39;1D&#39;,
                    4: &#39;2D&#39;,
                    5: &#39;3D&#39;
                    }

        return datatype[len(shape)]

    @staticmethod
    def _set_language(name: str):

        language = {&#39;imdb&#39;: &#39;English&#39;,
                    &#39;boston_housing&#39;: &#39;English&#39;,
                    &#39;reuters&#39;: &#39;English&#39;,
                    &#39;заболевания&#39;: &#39;Russian&#39;,
                    &#39;договоры&#39;: &#39;Russian&#39;,
                    &#39;умный_дом&#39;: &#39;Russian&#39;,
                    &#39;квартиры&#39;: &#39;Russian&#39;
                    }

        if name in language.keys():
            return language[name]
        else:
            return None

    def generator_train(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;train&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def generator_val(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;val&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def generator_test(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;test&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def keras_datasets(self, dataset: str, **options):

        self.name = dataset.lower()
        tags = {&#39;mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;fashion_mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;cifar10&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;cifar100&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;imdb&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;boston_housing&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;regression&#39;},
                &#39;reuters&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;}}
        self.tags = tags[self.name]
        self.source = &#39;tensorflow.keras&#39;
        data = {
            &#39;mnist&#39;: mnist,
            &#39;fashion_mnist&#39;: fashion_mnist,
            &#39;cifar10&#39;: cifar10,
            &#39;cifar100&#39;: cifar100,
            &#39;imdb&#39;: imdb,
            &#39;reuters&#39;: reuters,
            &#39;boston_housing&#39;: boston_housing
        }
        (x_train, y_train), (x_val, y_val) = data[self.name].load_data()

        self.language = self._set_language(self.name)
        if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
            self.num_classes[&#39;output_1&#39;] = len(np.unique(y_train, axis=0))
            if self.name == &#39;fashion_mnist&#39;:
                self.classes_names[&#39;output_1&#39;] = [&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,
                                                  &#39;Shirt&#39;,
                                                  &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]
            elif self.name == &#39;cifar10&#39;:
                self.classes_names[&#39;output_1&#39;] = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;,
                                                  &#39;horse&#39;, &#39;ship&#39;,
                                                  &#39;truck&#39;]
            else:
                self.classes_names[&#39;output_1&#39;] = [str(i) for i in range(len(np.unique(y_train, axis=0)))]
        else:
            self.num_classes[&#39;output_1&#39;] = 1

        if &#39;net&#39; in options.keys() and self.name in list(data.keys())[:4]:
            if options[&#39;net&#39;].lower() == &#39;linear&#39;:
                x_train = x_train.reshape((-1, np.prod(np.array(x_train.shape)[1:])))
                x_val = x_val.reshape((-1, np.prod(np.array(x_val.shape)[1:])))
            elif options[&#39;net&#39;].lower() == &#39;conv&#39;:
                if len(x_train.shape) == 3:
                    x_train = x_train[..., None]
                    x_val = x_val[..., None]

        if &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39; or \
                &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:

            shape_xt = x_train.shape
            shape_xv = x_val.shape
            x_train = x_train.reshape(-1, 1)
            x_val = x_val.reshape(-1, 1)

            if options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39;:
                self.createarray.scaler[&#39;input_1&#39;] = MinMaxScaler()
                if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                    self.createarray.scaler[&#39;output_1&#39;] = MinMaxScaler()

            elif options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:
                self.createarray.scaler[&#39;input_1&#39;] = StandardScaler()
                if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                    self.createarray.scaler[&#39;output_1&#39;] = StandardScaler()

            self.createarray.scaler[&#39;input_1&#39;].fit(x_train)
            x_train = self.createarray.scaler[&#39;input_1&#39;].transform(x_train)
            x_val = self.createarray.scaler[&#39;input_1&#39;].transform(x_val)
            x_train = x_train.reshape(shape_xt)
            x_val = x_val.reshape(shape_xv)

            if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                shape_yt = y_train.shape
                shape_yv = y_val.shape
                y_train = y_train.reshape(-1, 1)
                y_val = y_val.reshape(-1, 1)
                self.createarray.scaler[&#39;output_1&#39;].fit(y_train)
                y_train = self.createarray.scaler[&#39;output_1&#39;].transform(y_train)
                y_val = self.createarray.scaler[&#39;output_1&#39;].transform(y_val)
                y_train = y_train.reshape(shape_yt)
                y_val = y_val.reshape(shape_yv)
        else:
            self.createarray.scaler[&#39;output_1&#39;] = None

        self.one_hot_encoding[&#39;output_1&#39;] = False
        if &#39;one_hot_encoding&#39; in options.keys() and options[&#39;one_hot_encoding&#39;] is True:
            if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
                y_train = utils.to_categorical(y_train, len(np.unique(y_train, axis=0)))
                y_val = utils.to_categorical(y_val, len(np.unique(y_val, axis=0)))
                self.one_hot_encoding[&#39;output_1&#39;] = True

        self.input_shape[&#39;input_1&#39;] = x_train.shape if len(x_train.shape) &lt; 2 else x_train.shape[1:]
        self.input_datatype = self._set_datatype(shape=x_train.shape)
        self.input_names[&#39;input_1&#39;] = &#39;Вход&#39;
        self.output_shape[&#39;output_1&#39;] = y_train.shape[1:]
        self.output_datatype[&#39;output_1&#39;] = self._set_datatype(shape=y_train.shape)
        self.output_names[&#39;input_1&#39;] = &#39;Выход&#39;

        x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, shuffle=True)
        self.X[&#39;train&#39;][&#39;input_1&#39;] = x_train
        self.X[&#39;val&#39;][&#39;input_1&#39;] = x_val
        self.X[&#39;test&#39;][&#39;input_1&#39;] = x_test
        self.Y[&#39;train&#39;][&#39;output_1&#39;] = y_train
        self.Y[&#39;val&#39;][&#39;output_1&#39;] = y_val
        self.Y[&#39;test&#39;][&#39;output_1&#39;] = y_test

        self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
        self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
        self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

        return self

    def prepare_dataset(self, dataset_name: str, source: str):

        def load_arrays():

            for sample in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;)):
                for arr in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample)):
                    if &#39;input&#39; in arr:
                        self.X[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                            os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))
                    elif &#39;output&#39; in arr:
                        self.Y[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                            os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))

            pass

        def load_scalers():

            scalers = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;scalers&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    scalers.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in scalers:
                    self.createarray.scaler[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.scaler[put] = None

            pass

        def load_tokenizer():

            tokenizer = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;tokenizer&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    tokenizer.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in tokenizer:
                    self.createarray.tokenizer[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.tokenizer[put] = None

            pass

        def load_word2vec():

            word2v = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;word2vec&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    word2v.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in word2v:
                    self.createarray.word2vec[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.word2vec[put] = None

            pass

        def load_augmentation():

            augmentation = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;augmentation&#39;)
            if os.path.exists(folder_path):
                for aug in os.listdir(folder_path):
                    augmentation.append(aug[:-3])

            for put in list(self.tags.keys()):
                if put in augmentation:
                    self.createarray.augmentation[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.augmentation[put] = None

            pass

        if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;, &#39;imdb&#39;, &#39;boston_housing&#39;, &#39;reuters&#39;] and \
                source != &#39;custom_dataset&#39;:
            if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;]:
                self.keras_datasets(dataset_name, one_hot_encoding=True, scaler=&#39;MinMaxScaler&#39;, net=&#39;conv&#39;)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;imdb&#39;:
                self.keras_datasets(dataset_name, one_hot_encoding=True)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;reuters&#39;:
                self.keras_datasets(dataset_name)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;boston_housing&#39;:
                self.keras_datasets(dataset_name, scaler=&#39;StandardScaler&#39;)
                self.task_type[&#39;output_1&#39;] = &#39;regression&#39;
        elif source == &#39;custom_dataset&#39;:
            with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;config.json&#39;), &#39;r&#39;) as cfg:
                data = json.load(cfg)
            for key, value in data.items():
                self.__dict__[key] = value
            if self.use_generator:
                if &#39;text&#39; in self.tags.values():
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                              &#39;r&#39;) as txt:
                        self.createarray.txt_list = json.load(txt)

                self.dataloader = Dataloader()
                self.dataloader.load_data(strict_object=SourceData(**self.zip_params))

                with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                          &#39;r&#39;) as cfg:
                    self.split_sequence = json.load(cfg)
                for inp in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                                   &#39;inputs&#39;)):
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;inputs&#39;, inp),
                              &#39;r&#39;) as cfg:
                        data = json.load(cfg)
                    self.instructions[&#39;inputs&#39;][inp[:inp.rfind(&#39;.&#39;)]] = data
                for out in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                                   &#39;outputs&#39;)):
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;outputs&#39;, out),
                              &#39;r&#39;) as cfg:
                        data = json.load(cfg)
                    self.instructions[&#39;outputs&#39;][out[:out.rfind(&#39;.&#39;)]] = data
                self.createarray.file_folder = self.dataloader.file_folder

                self.dataset[&#39;train&#39;] = Dataset.from_generator(self.generator_train,
                                                               output_shapes=(self.input_shape, self.output_shape),
                                                               output_types=(self.input_dtype, self.output_dtype))
                self.dataset[&#39;val&#39;] = Dataset.from_generator(self.generator_val,
                                                             output_shapes=(self.input_shape, self.output_shape),
                                                             output_types=(self.input_dtype, self.output_dtype))
                self.dataset[&#39;test&#39;] = Dataset.from_generator(self.generator_test,
                                                              output_shapes=(self.input_shape, self.output_shape),
                                                              output_types=(self.input_dtype, self.output_dtype))
            else:
                load_arrays()

                self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
                self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
                self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

        load_scalers()
        load_tokenizer()
        load_word2vec()
        load_augmentation()

        self.dts_prepared = True

        pass</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="terra_ai.trds.CreateArray"><code class="flex name class">
<span>class <span class="ident">CreateArray</span></span>
<span>(</span><span>**options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CreateArray(object):

    def __init__(self, **options):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.augmentation: dict = {}
        self.temporary: dict = {&#39;bounding_boxes&#39;: {}}
        self.df = None

        self.file_folder = None
        self.txt_list: dict = {}

        for key, value in options.items():
            self.__dict__[key] = value

    @staticmethod
    def yolo_to_imgaug(args, shape):

        height, width = shape

        class_num = int(args[0])
        x_pos = float(args[1])
        y_pos = float(args[2])
        x_size = float(args[3])
        y_size = float(args[4])

        x1 = x_pos * width - (x_size * width / 2)
        y1 = y_pos * height - (y_size * height / 2)
        x2 = x_size * width + x1
        y2 = y_size * height + y1

        return [class_num, x1, y1, x2, y2]

    @staticmethod
    def imgaug_to_yolo(args, shape=(416, 416)):

        height, width = shape

        class_num = int(args[0])
        x1 = float(args[1])
        y1 = float(args[2])
        x2 = float(args[3])
        y2 = float(args[4])

        x_pos = x1 / width + ((x2 - x1) / width / 2)
        y_pos = y1 / height + ((y2 - y1) / height / 2)
        x_size = (x2 - x1) / width
        y_size = (y2 - y1) / height

        return_args = [class_num, x_pos, y_pos, x_size, y_size]

        for r in return_args[1:]:
            if r &gt; 1:
                return ()
            if r &lt; 0:
                return ()

        return return_args

    def create_images(self, image_path: str, **options):

        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
        array = img_to_array(img, dtype=np.uint8)
        if options[&#39;net&#39;] == &#39;Linear&#39;:
            array = array.reshape(np.prod(np.array(array.shape)))
        if self.augmentation[options[&#39;put&#39;]]:
            if &#39;object_detection&#39; in options.keys():
                txt_path = image_path[:image_path.rfind(&#39;.&#39;)] + &#39;.txt&#39;
                with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as b_boxes:
                    bounding_boxes = b_boxes.read()

                current_boxes = []
                for elem in bounding_boxes.split(&#39;\n&#39;):
                    b_box = self.yolo_to_imgaug(elem.split(&#39; &#39;), shape=array.shape[:2])
                    current_boxes.append(
                        BoundingBox(
                            **{&#39;label&#39;: b_box[0], &#39;x1&#39;: b_box[1], &#39;y1&#39;: b_box[2], &#39;x2&#39;: b_box[3], &#39;y2&#39;: b_box[4]}))

                bbs = BoundingBoxesOnImage(current_boxes, shape=array.shape)
                array, bbs_aug = self.augmentation[options[&#39;put&#39;]](image=array, bounding_boxes=bbs)
                list_of_bounding_boxes = []
                for elem in bbs_aug.remove_out_of_image().clip_out_of_image().bounding_boxes:
                    bb = elem.__dict__
                    b_box_coord = self.imgaug_to_yolo([bb[&#39;label&#39;], bb[&#39;x1&#39;], bb[&#39;y1&#39;], bb[&#39;x2&#39;], bb[&#39;y2&#39;]],
                                                      shape=array.shape[:2])
                    if b_box_coord != ():
                        list_of_bounding_boxes.append(b_box_coord)

                self.temporary[&#39;bounding_boxes&#39;][txt_path] = list_of_bounding_boxes
            else:
                array = self.augmentation[options[&#39;put&#39;]](image=array)

        return array

    def create_video(self, video_path, **options) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            video_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота кадра.
                width: int
                    Ширина кадра.
                max_frames: int
                    Максимальное количество кадров.
                mode: str
                    Режим обработки кадра (Сохранить пропорции, Растянуть).
                x_len: int
                    Длина окна выборки.
                step: int
                    Шаг окна выборки.

        Returns:
            array: np.ndarray
                Массив видео.

        &#34;&#34;&#34;

        def resize_frame(one_frame, original_shape, target_shape, mode):

            resized = None

            if mode == &#39;Растянуть&#39;:
                resized = resize_layer(one_frame[None, ...])
                resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
            elif mode == &#39;Сохранить пропорции&#39;:
                # height
                resized = one_frame.copy()
                if original_shape[0] &gt; target_shape[0]:
                    resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(
                        original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
                else:
                    black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3),
                                         dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized))
                    resized = np.concatenate((resized, black_bar))
                # width
                if original_shape[1] &gt; target_shape[1]:
                    resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(
                        original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
                else:
                    black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3),
                                         dtype=&#39;uint8&#39;)
                    resized = np.concatenate((black_bar, resized), axis=1)
                    resized = np.concatenate((resized, black_bar), axis=1)

            return resized

        array = []
        shape = (options[&#39;height&#39;], options[&#39;width&#39;])
        resize_layer = Resizing(*shape)

        cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
        height = int(cap.get(4))
        width = int(cap.get(3))
        # fps = int(cap.get(5))
        frame_count = int(cap.get(7))
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                if shape != (height, width):
                    frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
                frame = frame[:, :, [2, 1, 0]]
                array.append(frame)
                if len(array) == options[&#39;max_frames&#39;]:
                    break
        finally:
            cap.release()

        array = np.array(array)
        if frame_count &lt; options[&#39;max_frames&#39;]:
            add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
            array = np.concatenate((array, add_frames), axis=0)

        return array

    def create_text(self, sample: dict, **options):

        &#34;&#34;&#34;

        Args:
            sample: dict
                - file: Название файла.
                - slice: Индексы рассматриваемой части последовательности
            **options: Параметры обработки текста:
                bag_of_words: Tokenizer object, bool
                    Перевод в формат bag_of_words.
                word_to_vec: Word2Vec object, bool
                    Перевод в векторное представление Word2Vec.
                put: str
                    Индекс входа или выхода.

        Returns:
            array: np.ndarray
                Массив текстового вектора.
        &#34;&#34;&#34;

        filepath: str = sample[&#39;file&#39;]
        slicing: list = sample[&#39;slice&#39;]
        array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

        for key, value in options.items():
            if value:
                if key == &#39;bag_of_words&#39;:
                    array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
                elif key == &#39;word_to_vec&#39;:
                    reverse_tok = {}
                    words_list = []
                    for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                        reverse_tok[index] = word
                    for idx in array:
                        words_list.append(reverse_tok[idx])
                    array = []
                    for word in words_list:
                        array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
                break

        array = np.array(array)

        return array

    def create_audio(self):

        pass

    def create_dataframe(self, row_number: int, **options):
        &#34;&#34;&#34;
            Args:
                row_number: номер строки с сырыми данными датафрейма,
                **options: Параметры обработки колонок:
                    MinMaxScaler: лист индексов колонок для обработки
                    StandardScaler: лист индексов колонок для обработки
                    Categorical: лист индексов колонок для перевода по готовым категориям
                    Categorical_ranges: лист индексов колонок для перевода по категориям по диапазонам
                    one_hot_encoding: лист индексов колонок для перевода в ОНЕ
                    put: str  Индекс входа или выхода.
            Returns:
                array: np.ndarray
                    Массив вектора обработанных данных.
        &#34;&#34;&#34;
        row = self.df.loc[row_number].copy().tolist()

        if &#39;StandardScaler&#39; in options.keys():
            for i in options[&#39;StandardScaler&#39;]:
                row[i] = self.scaler[options[&#39;put&#39;]][&#39;StandardScaler&#39;].transform(
                    np.array([row[i]]).reshape(-1, 1)).tolist()

        if &#39;MinMaxScaler&#39; in options.keys():
            for i in options[&#39;MinMaxScaler&#39;]:
                row[i] = self.scaler[options[&#39;put&#39;]][&#39;MinMaxScaler&#39;].transform(
                    np.array(row[i]).reshape(-1, 1)).tolist()

        if &#39;Categorical&#39; in options.keys():
            for i in options[&#39;Categorical&#39;][&#39;lst_cols&#39;]:
                row[i] = list(options[&#39;Categorical&#39;][f&#39;col_{i}&#39;]).index(row[i])

        if &#39;Categorical_ranges&#39; in options.keys():
            for i in options[&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;])):
                    if row[i] &lt;= options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;]:
                        row[i] = j
                        break

        if &#39;one_hot_encoding&#39; in options.keys():
            for i in options[&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
                row[i] = utils.to_categorical(row[i], options[&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;], dtype=&#39;uint8&#39;).tolist()

        array = []
        for i in row:
            if type(i) == list:
                if type(i[0]) == list:
                    array.extend(i[0])
                else:
                    array.extend(i)
            else:
                array.append(i)

        array = np.array(array)

        return array

    def create_classification(self, index, **options):

        if options[&#39;one_hot_encoding&#39;]:
            index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
        index = np.array(index)

        return index

    def create_regression(self):

        pass

    def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

        &#34;&#34;&#34;

        Args:
            image_path: str
                Путь к файлу
            **options: Параметры сегментации:
                mask_range: int
                    Диапазон для каждого из RGB каналов.
                num_classes: int
                    Общее количество классов.
                shape: tuple
                    Размер картинки (высота, ширина).
                classes_colors: list
                    Список цветов для каждого класса.

        Returns:
            array: np.ndarray
                Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

        &#34;&#34;&#34;

        def cluster_to_ohe(mask_image):

            mask_image = mask_image.reshape(-1, 3)
            km = KMeans(n_clusters=options[&#39;num_classes&#39;])
            km.fit(mask_image)
            labels = km.labels_
            cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
            cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
            cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

            mask_ohe = np.zeros(options[&#39;shape&#39;])
            for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
                mask = np.zeros(options[&#39;shape&#39;])

                for j, cl_rgb in enumerate(cl_cent):
                    if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                            rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                            rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                        mask = cl_mask[:, :, j]

                if k == 0:
                    mask_ohe = mask
                else:
                    mask_ohe = np.dstack((mask_ohe, mask))

            return mask_ohe

        img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
        array = img_to_array(img, dtype=np.uint8)
        array = cluster_to_ohe(array)

        return array

    def create_text_segmentation(self, sample: dict, **options):

        array = []

        for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
            tags = [0 for _ in range(options[&#39;num_classes&#39;])]
            if elem:
                for idx in elem:
                    tags[idx] = 1
            array.append(tags)
        array = np.array(array, dtype=&#39;uint8&#39;)

        return array

    def create_timeseries(self):

        pass

    def create_object_detection(self, txt_path: str, **options):

        &#34;&#34;&#34;

        Args:
            txt_path: str
                Путь к файлу
            **options: Параметры сегментации:
                height: int
                    Высота изображения.
                width: int
                    Ширина изображения.
                num_classes: tuple
                    Количество классов.

        Returns:
            array: np.ndarray
                Массивы в трёх выходах.

        &#34;&#34;&#34;

        height: int = options[&#39;height&#39;]
        width: int = options[&#39;width&#39;]
        num_classes: int = options[&#39;num_classes&#39;]
        zero_boxes_flag: bool = False

        if self.temporary[&#39;bounding_boxes&#39;]:
            real_boxes = self.temporary[&#39;bounding_boxes&#39;][txt_path]
        else:
            with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
                bb_file = txt.read()
            real_boxes = []
            for elem in bb_file.split(&#39;\n&#39;):
                tmp = []
                if elem:
                    for num in elem.split(&#39; &#39;):
                        tmp.append(float(num))
                    real_boxes.append(tmp)

        if not real_boxes:
            zero_boxes_flag = True
            real_boxes = [[0, 0, 0, 0, 0]]
        real_boxes = np.array(real_boxes)
        real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
        anchors = np.array(
            [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
        num_layers = 3
        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

        real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
        input_shape = np.array((height, width), dtype=&#39;int32&#39;)

        boxes_wh = real_boxes[..., 2:4] * input_shape

        cells = [13, 26, 52]
        y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
                  range(num_layers)]
        box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

        anchor_area = anchors[:, 0] * anchors[:, 1]
        for r in range(len(real_boxes)):
            correct_anchors = []
            for anchor in anchors:
                correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
            correct_anchors = np.array(correct_anchors)
            correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
            iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
            best_anchor = np.argmax(iou, axis=-1)

            for m in range(num_layers):
                if best_anchor in anchor_mask[m]:
                    h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                    j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                    k = anchor_mask[m].index(int(best_anchor))
                    c = real_boxes[r, 4].astype(&#39;int32&#39;)
                    y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                    y_true[m][j, h, k, 4] = 0 if zero_boxes_flag else 1
                    y_true[m][j, h, k, 5 + c] = 0 if zero_boxes_flag else 1

        return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            words: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.trds.CreateArray.imgaug_to_yolo"><code class="name flex">
<span>def <span class="ident">imgaug_to_yolo</span></span>(<span>args, shape=(416, 416))</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def imgaug_to_yolo(args, shape=(416, 416)):

    height, width = shape

    class_num = int(args[0])
    x1 = float(args[1])
    y1 = float(args[2])
    x2 = float(args[3])
    y2 = float(args[4])

    x_pos = x1 / width + ((x2 - x1) / width / 2)
    y_pos = y1 / height + ((y2 - y1) / height / 2)
    x_size = (x2 - x1) / width
    y_size = (y2 - y1) / height

    return_args = [class_num, x_pos, y_pos, x_size, y_size]

    for r in return_args[1:]:
        if r &gt; 1:
            return ()
        if r &lt; 0:
            return ()

    return return_args</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.yolo_to_imgaug"><code class="name flex">
<span>def <span class="ident">yolo_to_imgaug</span></span>(<span>args, shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def yolo_to_imgaug(args, shape):

    height, width = shape

    class_num = int(args[0])
    x_pos = float(args[1])
    y_pos = float(args[2])
    x_size = float(args[3])
    y_size = float(args[4])

    x1 = x_pos * width - (x_size * width / 2)
    y1 = y_pos * height - (y_size * height / 2)
    x2 = x_size * width + x1
    y2 = y_size * height + y1

    return [class_num, x1, y1, x2, y2]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.trds.CreateArray.create_audio"><code class="name flex">
<span>def <span class="ident">create_audio</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_audio(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_classification"><code class="name flex">
<span>def <span class="ident">create_classification</span></span>(<span>self, index, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_classification(self, index, **options):

    if options[&#39;one_hot_encoding&#39;]:
        index = utils.to_categorical(index, num_classes=options[&#39;num_classes&#39;], dtype=&#39;uint8&#39;)
    index = np.array(index)

    return index</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_dataframe"><code class="name flex">
<span>def <span class="ident">create_dataframe</span></span>(<span>self, row_number: int, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>row_number</code></strong></dt>
<dd>номер строки с сырыми данными датафрейма,</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры обработки колонок:
MinMaxScaler: лист индексов колонок для обработки
StandardScaler: лист индексов колонок для обработки
Categorical: лист индексов колонок для перевода по готовым категориям
Categorical_ranges: лист индексов колонок для перевода по категориям по диапазонам
one_hot_encoding: лист индексов колонок для перевода в ОНЕ
put: str
Индекс входа или выхода.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив вектора обработанных данных.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dataframe(self, row_number: int, **options):
    &#34;&#34;&#34;
        Args:
            row_number: номер строки с сырыми данными датафрейма,
            **options: Параметры обработки колонок:
                MinMaxScaler: лист индексов колонок для обработки
                StandardScaler: лист индексов колонок для обработки
                Categorical: лист индексов колонок для перевода по готовым категориям
                Categorical_ranges: лист индексов колонок для перевода по категориям по диапазонам
                one_hot_encoding: лист индексов колонок для перевода в ОНЕ
                put: str  Индекс входа или выхода.
        Returns:
            array: np.ndarray
                Массив вектора обработанных данных.
    &#34;&#34;&#34;
    row = self.df.loc[row_number].copy().tolist()

    if &#39;StandardScaler&#39; in options.keys():
        for i in options[&#39;StandardScaler&#39;]:
            row[i] = self.scaler[options[&#39;put&#39;]][&#39;StandardScaler&#39;].transform(
                np.array([row[i]]).reshape(-1, 1)).tolist()

    if &#39;MinMaxScaler&#39; in options.keys():
        for i in options[&#39;MinMaxScaler&#39;]:
            row[i] = self.scaler[options[&#39;put&#39;]][&#39;MinMaxScaler&#39;].transform(
                np.array(row[i]).reshape(-1, 1)).tolist()

    if &#39;Categorical&#39; in options.keys():
        for i in options[&#39;Categorical&#39;][&#39;lst_cols&#39;]:
            row[i] = list(options[&#39;Categorical&#39;][f&#39;col_{i}&#39;]).index(row[i])

    if &#39;Categorical_ranges&#39; in options.keys():
        for i in options[&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
            for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;])):
                if row[i] &lt;= options[&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;]:
                    row[i] = j
                    break

    if &#39;one_hot_encoding&#39; in options.keys():
        for i in options[&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
            row[i] = utils.to_categorical(row[i], options[&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;], dtype=&#39;uint8&#39;).tolist()

    array = []
    for i in row:
        if type(i) == list:
            if type(i[0]) == list:
                array.extend(i[0])
            else:
                array.extend(i)
        else:
            array.append(i)

    array = np.array(array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_images"><code class="name flex">
<span>def <span class="ident">create_images</span></span>(<span>self, image_path: str, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_images(self, image_path: str, **options):

    shape = (options[&#39;height&#39;], options[&#39;width&#39;])
    img = load_img(path=os.path.join(self.file_folder, image_path), target_size=shape)
    array = img_to_array(img, dtype=np.uint8)
    if options[&#39;net&#39;] == &#39;Linear&#39;:
        array = array.reshape(np.prod(np.array(array.shape)))
    if self.augmentation[options[&#39;put&#39;]]:
        if &#39;object_detection&#39; in options.keys():
            txt_path = image_path[:image_path.rfind(&#39;.&#39;)] + &#39;.txt&#39;
            with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as b_boxes:
                bounding_boxes = b_boxes.read()

            current_boxes = []
            for elem in bounding_boxes.split(&#39;\n&#39;):
                b_box = self.yolo_to_imgaug(elem.split(&#39; &#39;), shape=array.shape[:2])
                current_boxes.append(
                    BoundingBox(
                        **{&#39;label&#39;: b_box[0], &#39;x1&#39;: b_box[1], &#39;y1&#39;: b_box[2], &#39;x2&#39;: b_box[3], &#39;y2&#39;: b_box[4]}))

            bbs = BoundingBoxesOnImage(current_boxes, shape=array.shape)
            array, bbs_aug = self.augmentation[options[&#39;put&#39;]](image=array, bounding_boxes=bbs)
            list_of_bounding_boxes = []
            for elem in bbs_aug.remove_out_of_image().clip_out_of_image().bounding_boxes:
                bb = elem.__dict__
                b_box_coord = self.imgaug_to_yolo([bb[&#39;label&#39;], bb[&#39;x1&#39;], bb[&#39;y1&#39;], bb[&#39;x2&#39;], bb[&#39;y2&#39;]],
                                                  shape=array.shape[:2])
                if b_box_coord != ():
                    list_of_bounding_boxes.append(b_box_coord)

            self.temporary[&#39;bounding_boxes&#39;][txt_path] = list_of_bounding_boxes
        else:
            array = self.augmentation[options[&#39;put&#39;]](image=array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_object_detection"><code class="name flex">
<span>def <span class="ident">create_object_detection</span></span>(<span>self, txt_path: str, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>txt_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
height: int
Высота изображения.
width: int
Ширина изображения.
num_classes: tuple
Количество классов.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массивы в трёх выходах.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_object_detection(self, txt_path: str, **options):

    &#34;&#34;&#34;

    Args:
        txt_path: str
            Путь к файлу
        **options: Параметры сегментации:
            height: int
                Высота изображения.
            width: int
                Ширина изображения.
            num_classes: tuple
                Количество классов.

    Returns:
        array: np.ndarray
            Массивы в трёх выходах.

    &#34;&#34;&#34;

    height: int = options[&#39;height&#39;]
    width: int = options[&#39;width&#39;]
    num_classes: int = options[&#39;num_classes&#39;]
    zero_boxes_flag: bool = False

    if self.temporary[&#39;bounding_boxes&#39;]:
        real_boxes = self.temporary[&#39;bounding_boxes&#39;][txt_path]
    else:
        with open(os.path.join(self.file_folder, txt_path), &#39;r&#39;) as txt:
            bb_file = txt.read()
        real_boxes = []
        for elem in bb_file.split(&#39;\n&#39;):
            tmp = []
            if elem:
                for num in elem.split(&#39; &#39;):
                    tmp.append(float(num))
                real_boxes.append(tmp)

    if not real_boxes:
        zero_boxes_flag = True
        real_boxes = [[0, 0, 0, 0, 0]]
    real_boxes = np.array(real_boxes)
    real_boxes = real_boxes[:, [1, 2, 3, 4, 0]]
    anchors = np.array(
        [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])
    num_layers = 3
    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]

    real_boxes = np.array(real_boxes, dtype=&#39;float32&#39;)
    input_shape = np.array((height, width), dtype=&#39;int32&#39;)

    boxes_wh = real_boxes[..., 2:4] * input_shape

    cells = [13, 26, 52]
    y_true = [np.zeros((cells[n], cells[n], len(anchor_mask[n]), 5 + num_classes), dtype=&#39;float32&#39;) for n in
              range(num_layers)]
    box_area = boxes_wh[:, 0] * boxes_wh[:, 1]

    anchor_area = anchors[:, 0] * anchors[:, 1]
    for r in range(len(real_boxes)):
        correct_anchors = []
        for anchor in anchors:
            correct_anchors.append([min(anchor[0], boxes_wh[r][0]), min(anchor[1], boxes_wh[r][1])])
        correct_anchors = np.array(correct_anchors)
        correct_anchors_area = correct_anchors[:, 0] * correct_anchors[:, 1]
        iou = correct_anchors_area / (box_area[r] + anchor_area - correct_anchors_area)
        best_anchor = np.argmax(iou, axis=-1)

        for m in range(num_layers):
            if best_anchor in anchor_mask[m]:
                h = np.floor(real_boxes[r, 0] * cells[m]).astype(&#39;int32&#39;)
                j = np.floor(real_boxes[r, 1] * cells[m]).astype(&#39;int32&#39;)
                k = anchor_mask[m].index(int(best_anchor))
                c = real_boxes[r, 4].astype(&#39;int32&#39;)
                y_true[m][j, h, k, 0:4] = real_boxes[r, 0:4]
                y_true[m][j, h, k, 4] = 0 if zero_boxes_flag else 1
                y_true[m][j, h, k, 5 + c] = 0 if zero_boxes_flag else 1

    return np.array(y_true[0]), np.array(y_true[1]), np.array(y_true[2])</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_regression"><code class="name flex">
<span>def <span class="ident">create_regression</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_regression(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_scaler"><code class="name flex">
<span>def <span class="ident">create_scaler</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_scaler(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_segmentation"><code class="name flex">
<span>def <span class="ident">create_segmentation</span></span>(<span>self, image_path: str, **options: dict) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>image_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
mask_range: int
Диапазон для каждого из RGB каналов.
num_classes: int
Общее количество классов.
shape: tuple
Размер картинки (высота, ширина).
classes_colors: list
Список цветов для каждого класса.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_segmentation(self, image_path: str, **options: dict) -&gt; np.ndarray:

    &#34;&#34;&#34;

    Args:
        image_path: str
            Путь к файлу
        **options: Параметры сегментации:
            mask_range: int
                Диапазон для каждого из RGB каналов.
            num_classes: int
                Общее количество классов.
            shape: tuple
                Размер картинки (высота, ширина).
            classes_colors: list
                Список цветов для каждого класса.

    Returns:
        array: np.ndarray
            Массив принадлежности каждого пикселя к определенному классу в формате One-Hot Encoding.

    &#34;&#34;&#34;

    def cluster_to_ohe(mask_image):

        mask_image = mask_image.reshape(-1, 3)
        km = KMeans(n_clusters=options[&#39;num_classes&#39;])
        km.fit(mask_image)
        labels = km.labels_
        cl_cent = km.cluster_centers_.astype(&#39;uint8&#39;)[:max(labels) + 1]
        cl_mask = utils.to_categorical(labels, max(labels) + 1, dtype=&#39;uint8&#39;)
        cl_mask = cl_mask.reshape(options[&#39;shape&#39;][0], options[&#39;shape&#39;][1], cl_mask.shape[-1])

        mask_ohe = np.zeros(options[&#39;shape&#39;])
        for k, rgb in enumerate(options[&#39;classes_colors&#39;]):
            mask = np.zeros(options[&#39;shape&#39;])

            for j, cl_rgb in enumerate(cl_cent):
                if rgb[0] in range(cl_rgb[0] - options[&#39;mask_range&#39;], cl_rgb[0] + options[&#39;mask_range&#39;]) and \
                        rgb[1] in range(cl_rgb[1] - options[&#39;mask_range&#39;], cl_rgb[1] + options[&#39;mask_range&#39;]) and \
                        rgb[2] in range(cl_rgb[2] - options[&#39;mask_range&#39;], cl_rgb[2] + options[&#39;mask_range&#39;]):
                    mask = cl_mask[:, :, j]

            if k == 0:
                mask_ohe = mask
            else:
                mask_ohe = np.dstack((mask_ohe, mask))

        return mask_ohe

    img = load_img(path=os.path.join(self.file_folder, image_path), target_size=options[&#39;shape&#39;])
    array = img_to_array(img, dtype=np.uint8)
    array = cluster_to_ohe(array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_text"><code class="name flex">
<span>def <span class="ident">create_text</span></span>(<span>self, sample: dict, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong></dt>
<dd>dict
- file: Название файла.
- slice: Индексы рассматриваемой части последовательности</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры обработки текста:
bag_of_words: Tokenizer object, bool
Перевод в формат bag_of_words.
word_to_vec: Word2Vec object, bool
Перевод в векторное представление Word2Vec.
put: str
Индекс входа или выхода.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив текстового вектора.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_text(self, sample: dict, **options):

    &#34;&#34;&#34;

    Args:
        sample: dict
            - file: Название файла.
            - slice: Индексы рассматриваемой части последовательности
        **options: Параметры обработки текста:
            bag_of_words: Tokenizer object, bool
                Перевод в формат bag_of_words.
            word_to_vec: Word2Vec object, bool
                Перевод в векторное представление Word2Vec.
            put: str
                Индекс входа или выхода.

    Returns:
        array: np.ndarray
            Массив текстового вектора.
    &#34;&#34;&#34;

    filepath: str = sample[&#39;file&#39;]
    slicing: list = sample[&#39;slice&#39;]
    array = self.txt_list[options[&#39;put&#39;]][filepath][slicing[0]:slicing[1]]

    for key, value in options.items():
        if value:
            if key == &#39;bag_of_words&#39;:
                array = self.tokenizer[options[&#39;put&#39;]].sequences_to_matrix([array]).astype(&#39;uint16&#39;)
            elif key == &#39;word_to_vec&#39;:
                reverse_tok = {}
                words_list = []
                for word, index in self.tokenizer[options[&#39;put&#39;]].word_index.items():
                    reverse_tok[index] = word
                for idx in array:
                    words_list.append(reverse_tok[idx])
                array = []
                for word in words_list:
                    array.append(self.word2vec[options[&#39;put&#39;]].wv[word])
            break

    array = np.array(array)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_text_segmentation"><code class="name flex">
<span>def <span class="ident">create_text_segmentation</span></span>(<span>self, sample: dict, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_text_segmentation(self, sample: dict, **options):

    array = []

    for elem in self.txt_list[options[&#39;put&#39;]][sample[&#39;file&#39;]][sample[&#39;slice&#39;][0]:sample[&#39;slice&#39;][1]]:
        tags = [0 for _ in range(options[&#39;num_classes&#39;])]
        if elem:
            for idx in elem:
                tags[idx] = 1
        array.append(tags)
    array = np.array(array, dtype=&#39;uint8&#39;)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_timeseries"><code class="name flex">
<span>def <span class="ident">create_timeseries</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_timeseries(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_tokenizer"><code class="name flex">
<span>def <span class="ident">create_tokenizer</span></span>(<span>self, mode: str, iteration: int, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры токенайзера:
num_words: int
Количество слов для токенайзера.
filters: str
Символы, подлежащие удалению.
lower: bool
Перевод заглавных букв в строчные.
split: str
Символ разделения.
char_level: bool
Учёт каждого символа в качестве отдельного токена.
oov_token: str
В случае указания этот токен будет заменять все слова, не попавшие в
диапазон частотности слов 0 &lt; num_words.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Токенайзер.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tokenizer(self, mode: str, iteration: int, **options):

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        **options: Параметры токенайзера:
                   num_words: int
                       Количество слов для токенайзера.
                   filters: str
                       Символы, подлежащие удалению.
                   lower: bool
                       Перевод заглавных букв в строчные.
                   split: str
                       Символ разделения.
                   char_level: bool
                       Учёт каждого символа в качестве отдельного токена.
                   oov_token: str
                       В случае указания этот токен будет заменять все слова, не попавшие в
                       диапазон частотности слов 0 &lt; num_words.

    Returns:
        Объект Токенайзер.

    &#34;&#34;&#34;

    self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_video"><code class="name flex">
<span>def <span class="ident">create_video</span></span>(<span>self, video_path, **options) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>video_path</code></strong></dt>
<dd>str
Путь к файлу</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры сегментации:
height: int
Высота кадра.
width: int
Ширина кадра.
max_frames: int
Максимальное количество кадров.
mode: str
Режим обработки кадра (Сохранить пропорции, Растянуть).
x_len: int
Длина окна выборки.
step: int
Шаг окна выборки.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>np.ndarray
Массив видео.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_video(self, video_path, **options) -&gt; np.ndarray:

    &#34;&#34;&#34;

    Args:
        video_path: str
            Путь к файлу
        **options: Параметры сегментации:
            height: int
                Высота кадра.
            width: int
                Ширина кадра.
            max_frames: int
                Максимальное количество кадров.
            mode: str
                Режим обработки кадра (Сохранить пропорции, Растянуть).
            x_len: int
                Длина окна выборки.
            step: int
                Шаг окна выборки.

    Returns:
        array: np.ndarray
            Массив видео.

    &#34;&#34;&#34;

    def resize_frame(one_frame, original_shape, target_shape, mode):

        resized = None

        if mode == &#39;Растянуть&#39;:
            resized = resize_layer(one_frame[None, ...])
            resized = resized.numpy().squeeze().astype(&#39;uint8&#39;)
        elif mode == &#39;Сохранить пропорции&#39;:
            # height
            resized = one_frame.copy()
            if original_shape[0] &gt; target_shape[0]:
                resized = resized[int(original_shape[0] / 2 - target_shape[0] / 2):int(
                    original_shape[0] / 2 - target_shape[0] / 2) + target_shape[0], :]
            else:
                black_bar = np.zeros((int((target_shape[0] - original_shape[0]) / 2), original_shape[1], 3),
                                     dtype=&#39;uint8&#39;)
                resized = np.concatenate((black_bar, resized))
                resized = np.concatenate((resized, black_bar))
            # width
            if original_shape[1] &gt; target_shape[1]:
                resized = resized[:, int(original_shape[1] / 2 - target_shape[1] / 2):int(
                    original_shape[1] / 2 - target_shape[1] / 2) + target_shape[1]]
            else:
                black_bar = np.zeros((target_shape[0], int((target_shape[1] - original_shape[1]) / 2), 3),
                                     dtype=&#39;uint8&#39;)
                resized = np.concatenate((black_bar, resized), axis=1)
                resized = np.concatenate((resized, black_bar), axis=1)

        return resized

    array = []
    shape = (options[&#39;height&#39;], options[&#39;width&#39;])
    resize_layer = Resizing(*shape)

    cap = cv2.VideoCapture(os.path.join(self.file_folder, video_path))
    height = int(cap.get(4))
    width = int(cap.get(3))
    # fps = int(cap.get(5))
    frame_count = int(cap.get(7))
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if shape != (height, width):
                frame = resize_frame(frame, (height, width), shape, options[&#39;mode&#39;])
            frame = frame[:, :, [2, 1, 0]]
            array.append(frame)
            if len(array) == options[&#39;max_frames&#39;]:
                break
    finally:
        cap.release()

    array = np.array(array)
    if frame_count &lt; options[&#39;max_frames&#39;]:
        add_frames = np.zeros((options[&#39;max_frames&#39;] - frame_count, *shape, 3), dtype=&#39;uint8&#39;)
        array = np.concatenate((array, add_frames), axis=0)

    return array</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.create_word2vec"><code class="name flex">
<span>def <span class="ident">create_word2vec</span></span>(<span>self, mode: str, iteration: int, words: list, **options) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>words</code></strong></dt>
<dd>list
Список слов для обучения Word2Vec.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры Word2Vec:
size: int
Dimensionality of the word vectors.
window: int
Maximum distance between the current and predicted word within a sentence.
min_count: int
Ignores all words with total frequency lower than this.
workers: int
Use these many worker threads to train the model (=faster training with multicore machines).
iter: int
Number of iterations (epochs) over the corpus.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Word2Vec.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_word2vec(self, mode: str, iteration: int, words: list, **options) -&gt; None:

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        words: list
            Список слов для обучения Word2Vec.
        **options: Параметры Word2Vec:
                   size: int
                       Dimensionality of the word vectors.
                   window: int
                       Maximum distance between the current and predicted word within a sentence.
                   min_count: int
                       Ignores all words with total frequency lower than this.
                   workers: int
                       Use these many worker threads to train the model (=faster training with multicore machines).
                   iter: int
                       Number of iterations (epochs) over the corpus.

    Returns:
        Объект Word2Vec.

    &#34;&#34;&#34;

    self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(words, **options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateArray.inverse_data"><code class="name flex">
<span>def <span class="ident">inverse_data</span></span>(<span>self, put: str, array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>put</code></strong></dt>
<dd>str
Рассматриваемый вход или выход (input_2, output_1);</dd>
<dt><strong><code>array</code></strong></dt>
<dd>np.ndarray
NumPy массив, подлежащий возврату в исходное состояние.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Данные в исходном состоянии.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_data(self, put: str, array: np.ndarray):

    &#34;&#34;&#34;

    Args:
        put: str
            Рассматриваемый вход или выход (input_2, output_1);
        array: np.ndarray
            NumPy массив, подлежащий возврату в исходное состояние.

    Returns:
        Данные в исходном состоянии.

    &#34;&#34;&#34;

    inverted_data = None

    for attr in self.__dict__.keys():
        if self.__dict__[attr] and put in self.__dict__[attr].keys():
            if attr == &#39;tokenizer&#39;:
                if array.shape[0] == self.tokenizer[put].num_words:
                    idx = 0
                    arr = []
                    for num in array:
                        if num == 1:
                            arr.append(idx)
                        idx += 1
                    array = np.array(arr)
                inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

            elif attr == &#39;word2vec&#39;:
                text_list = []
                for i in range(len(array)):
                    text_list.append(
                        self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                inverted_data = &#39; &#39;.join(text_list)

            elif attr == &#39;scaler&#39;:
                original_shape = array.shape
                array = array.reshape(-1, 1)
                array = self.scaler[put].inverse_transform(array)
                inverted_data = array.reshape(original_shape)
        break

    return inverted_data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.trds.CreateDTS"><code class="flex name class">
<span>class <span class="ident">CreateDTS</span></span>
<span>(</span><span>trds_path='/content/drive/MyDrive/TerraAI/datasets', exch_obj=&lt;terra_ai.guiexchange.Exchange object&gt;)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CreateDTS(object):

    def __init__(self, trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;,
                 exch_obj=tr2dj_obj):

        self.Exch = exch_obj
        self.django_flag: bool = False
        if self.Exch.property_of != &#39;TERRA&#39;:
            self.django_flag = True

        self.dataloader = Dataloader()
        self.createarray = None

        self.trds_path: str = trds_path
        self.file_folder: str = &#39;&#39;

        self.name: str = &#39;&#39;
        self.source: str = &#39;&#39;
        self.tags: dict = {}
        self.user_tags: list = []
        self.language: str = &#39;&#39;
        self.divide_ratio: list = [0.8, 0.1, 0.1]
        self.limit: int = 0
        self.input_datatype: dict = {}  # string
        self.input_dtype: dict = {}
        self.input_shape: dict = {}
        self.input_names: dict = {}
        self.output_datatype: dict = {}
        self.output_dtype: dict = {}
        self.output_shape: dict = {}
        self.output_names: dict = {}
        self.num_classes: dict = {}
        self.classes_names: dict = {}
        self.classes_colors: dict = {}
        self.one_hot_encoding: dict = {}
        self.task_type: dict = {}
        self.zip_params: dict = {}
        self.user_parameters: dict = {}
        self.use_generator: bool = False

        self.X: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.Y: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}
        self.df: dict = {}
        self.tsgenerator: dict = {}

        self.instructions: dict = {&#39;inputs&#39;: {}, &#39;outputs&#39;: {}}
        self.limit: int
        self.dataset: dict = {}

        self.y_cls: list = []
        self.sequence: list = []
        self.peg: list = []
        self.iter: int = 0
        self.mode: str = &#39;&#39;
        self.split_sequence: dict = {}
        self.temporary: dict = {}

        pass

    @staticmethod
    def _set_datatype(shape) -&gt; str:

        datatype = {0: &#39;DIM&#39;,
                    1: &#39;DIM&#39;,
                    2: &#39;1D&#39;,
                    3: &#39;2D&#39;,
                    4: &#39;3D&#39;,
                    5: &#39;4D&#39;
                    }

        return datatype[len(shape)]

    def load_data(self, strict_object):

        self.dataloader.load_data(strict_object=strict_object)

        self.zip_params = json.loads(strict_object.json())
        self.file_folder = self.dataloader.file_folder

        pass

    def create_dataset(self, dataset_dict: dict):

        self.createarray = CreateArray(file_folder=self.file_folder)

        self.name = dataset_dict[&#39;parameters&#39;][&#39;name&#39;]
        self.divide_ratio = (dataset_dict[&#39;parameters&#39;][&#39;train_part&#39;], dataset_dict[&#39;parameters&#39;][&#39;val_part&#39;],
                             dataset_dict[&#39;parameters&#39;][&#39;test_part&#39;])
        self.source = &#39;custom dataset&#39;
        self.user_tags = dataset_dict[&#39;parameters&#39;][&#39;user_tags&#39;]
        self.use_generator = dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]

        for key in dataset_dict[&#39;inputs&#39;].keys():
            self.tags[key] = dataset_dict[&#39;inputs&#39;][key][&#39;tag&#39;]
            self.input_names[key] = dataset_dict[&#39;inputs&#39;][key][&#39;name&#39;]
            self.user_parameters[key] = dataset_dict[&#39;inputs&#39;][key][&#39;parameters&#39;]
        for key in dataset_dict[&#39;outputs&#39;].keys():
            self.tags[key] = dataset_dict[&#39;outputs&#39;][key][&#39;tag&#39;]
            self.output_names[key] = dataset_dict[&#39;outputs&#39;][key][&#39;name&#39;]
            self.user_parameters[key] = dataset_dict[&#39;outputs&#39;][key][&#39;parameters&#39;]

        # Создаем входные инструкции
        self.iter = 0
        self.mode = &#39;input&#39;
        for inp in dataset_dict[&#39;inputs&#39;]:
            self.iter += 1
            self.instructions[&#39;inputs&#39;][inp] = getattr(self, f&#34;instructions_{self.tags[inp]}&#34;)(
                **dataset_dict[&#39;inputs&#39;][inp][&#39;parameters&#39;])
        # Создаем выходные инструкции
        self.iter = 0
        self.mode = &#39;output&#39;
        for out in dataset_dict[&#39;outputs&#39;]:
            self.iter += 1
            self.instructions[&#39;outputs&#39;][out] = getattr(self, f&#34;instructions_{self.tags[out]}&#34;)(
                **dataset_dict[&#39;outputs&#39;][out][&#39;parameters&#39;])

        # Получаем входные параметры
        for key in self.instructions[&#39;inputs&#39;].keys():
            array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            self.input_shape[key] = array.shape
            self.input_dtype[key] = str(array.dtype)
            self.input_datatype[key] = self._set_datatype(array.shape)
        # Получаем выходные параметры
        for key in self.instructions[&#39;outputs&#39;].keys():
            array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
                self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
            if isinstance(array, tuple):
                for i in range(len(array)):
                    self.output_shape[key.replace(key[-1], str(int(key[-1]) + i))] = array[i].shape
                    self.output_dtype[key.replace(key[-1], str(int(key[-1]) + i))] = str(array[i].dtype)
                    self.output_datatype[key.replace(key[-1], str(int(key[-1]) + i))] = self._set_datatype(
                        array[i].shape)
            else:
                self.output_shape[key] = array.shape
                self.output_dtype[key] = str(array.dtype)
                self.output_datatype[key] = self._set_datatype(array.shape)

        # Разделение на три выборки
        self.split_sequence[&#39;train&#39;] = []
        self.split_sequence[&#39;val&#39;] = []
        self.split_sequence[&#39;test&#39;] = []
        for i in range(len(self.peg) - 1):
            indices = np.arange(self.peg[i], self.peg[i + 1])
            train_len = int(self.divide_ratio[0] * len(indices))
            val_len = int(self.divide_ratio[1] * len(indices))
            indices = indices.tolist()
            self.split_sequence[&#39;train&#39;].extend(indices[:train_len])
            self.split_sequence[&#39;val&#39;].extend(indices[train_len:train_len + val_len])
            self.split_sequence[&#39;test&#39;].extend(indices[train_len + val_len:])
        if not dataset_dict[&#39;parameters&#39;][&#39;preserve_sequence&#39;]:
            random.shuffle(self.split_sequence[&#39;train&#39;])
            random.shuffle(self.split_sequence[&#39;val&#39;])
            random.shuffle(self.split_sequence[&#39;test&#39;])

        self.limit: int = len(self.instructions[&#39;inputs&#39;][&#39;input_1&#39;][&#39;instructions&#39;])

        data = {}
        if dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]:
            # Сохранение датасета для генератора
            data[&#39;zip_params&#39;] = self.zip_params
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;), exist_ok=True)
            for key in self.instructions.keys():
                os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key), exist_ok=True)
                for inp in self.instructions[key].keys():
                    with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key, f&#39;{inp}.json&#39;),
                              &#39;w&#39;) as instruction:
                        json.dump(self.instructions[key][inp], instruction)
            with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                      &#39;w&#39;) as seq:
                json.dump(self.split_sequence, seq)
            if &#39;text&#39; in self.tags.keys():  # if &#39;txt_list&#39; in self.createarray.__dict__.keys():
                with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                          &#39;w&#39;) as fp:
                    json.dump(self.createarray.txt_list, fp)
        else:
            # Сохранение датасета с NumPy
            for key in self.instructions[&#39;inputs&#39;].keys():
                x: list = []
                for i in range(self.limit):
                    x.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][i],
                        **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;]))
                self.X[&#39;train&#39;][key] = np.array(x)[self.split_sequence[&#39;train&#39;]]
                self.X[&#39;val&#39;][key] = np.array(x)[self.split_sequence[&#39;val&#39;]]
                self.X[&#39;test&#39;][key] = np.array(x)[self.split_sequence[&#39;test&#39;]]

            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    y_1: list = []
                    y_2: list = []
                    y_3: list = []
                    for i in range(self.limit):
                        arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                            self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                            **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                        y_1.append(arrays[0])
                        y_2.append(arrays[1])
                        y_3.append(arrays[2])

                    splits = [&#39;train&#39;, &#39;val&#39;, &#39;test&#39;]
                    for spl_seq in splits:
                        for i in range(len(splits)):
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1])))] = np.array(y_1)[
                                self.split_sequence[spl_seq]]
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 1))] = np.array(y_2)[
                                self.split_sequence[spl_seq]]
                            self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 2))] = np.array(y_3)[
                                self.split_sequence[spl_seq]]
                else:
                    y: list = []
                    for i in range(self.limit):
                        y.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                            self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                            **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;]))
                    self.Y[&#39;train&#39;][key] = np.array(y)[self.split_sequence[&#39;train&#39;]]
                    self.Y[&#39;val&#39;][key] = np.array(y)[self.split_sequence[&#39;val&#39;]]
                    self.Y[&#39;test&#39;][key] = np.array(y)[self.split_sequence[&#39;test&#39;]]

            for sample in self.X.keys():
                os.makedirs(os.path.join(self.trds_path, &#39;arrays&#39;, sample), exist_ok=True)
                for inp in self.X[sample].keys():
                    os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                    joblib.dump(self.X[sample][inp],
                                os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

            for sample in self.Y.keys():
                for inp in self.Y[sample].keys():
                    os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                    joblib.dump(self.Y[sample][inp],
                                os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

        if self.createarray.scaler:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;), exist_ok=True)
        if self.createarray.tokenizer:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;), exist_ok=True)
        if self.createarray.word2vec:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;), exist_ok=True)
        if self.createarray.augmentation:
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;), exist_ok=True)
        # if self.createarray.tsgenerator:
        #     os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;), exist_ok=True)

        for scaler in self.createarray.scaler.keys():
            if self.createarray.scaler[scaler]:
                joblib.dump(self.createarray.scaler[scaler],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;, f&#39;{scaler}.gz&#39;))
        for tok in self.createarray.tokenizer.keys():
            if self.createarray.tokenizer[tok]:
                joblib.dump(self.createarray.tokenizer[tok],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;, f&#39;{tok}.gz&#39;))
        for w2v in self.createarray.word2vec.keys():
            if self.createarray.word2vec[w2v]:
                joblib.dump(self.createarray.word2vec[w2v],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;, f&#39;{w2v}.gz&#39;))
        for aug in self.createarray.augmentation.keys():
            if self.createarray.augmentation[aug]:
                joblib.dump(self.createarray.augmentation[aug],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;, f&#39;{aug}.gz&#39;))
        # for tsg in self.createarray.tsgenerator.keys():
        #     if self.createarray.tsgenerator[tsg]:
        #         joblib.dump(self.createarray.tsgenerator[tsg],
        #                     os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;, f&#39;{tsg}.gz&#39;))

        attributes = [&#39;name&#39;, &#39;source&#39;, &#39;tags&#39;, &#39;user_tags&#39;, &#39;language&#39;,
                      &#39;input_datatype&#39;, &#39;input_dtype&#39;, &#39;input_shape&#39;, &#39;input_names&#39;,
                      &#39;output_datatype&#39;, &#39;output_dtype&#39;, &#39;output_shape&#39;, &#39;output_names&#39;,
                      &#39;num_classes&#39;, &#39;classes_names&#39;, &#39;classes_colors&#39;,
                      &#39;one_hot_encoding&#39;, &#39;task_type&#39;, &#39;limit&#39;, &#39;use_generator&#39;]

        for attr in attributes:
            data[attr] = self.__dict__[attr]
        data[&#39;date&#39;] = datetime.now().astimezone(timezone(&#39;Europe/Moscow&#39;)).isoformat()
        with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;config.json&#39;), &#39;w&#39;) as fp:
            json.dump(data, fp)

        pass

    def instructions_images(self, **options):

        instructions: dict = {}
        instr: list = []
        y_cls: list = []
        cls_idx = 0
        peg_idx = 0
        self.peg.append(0)
        options[&#39;put&#39;] = f&#39;{self.mode}_{self.iter}&#39;
        if &#39;object_detection&#39; in self.tags.values():
            options[&#39;object_detection&#39;] = True
        if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_folder&#39;:
            for folder_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                for directory, folder, file_name in sorted(os.walk(os.path.join(self.file_folder, folder_name))):
                    if file_name:
                        file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
                        for name in sorted(file_name):
                            if &#39;object_detection&#39; in self.tags.values():
                                if &#39;txt&#39; not in name:
                                    instr.append(os.path.join(file_folder, name))
                                    peg_idx += 1
                            else:
                                instr.append(os.path.join(file_folder, name))
                                peg_idx += 1
                            y_cls.append(cls_idx)
                        cls_idx += 1
                        self.peg.append(peg_idx)
            self.y_cls = y_cls
        elif options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
            for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                data = pd.read_csv(os.path.join(self.file_folder, file_name),
                                   usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
                instr = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
                prev_elem = instr[0].split(&#39;/&#39;)[-2]
                for elem in instr:
                    cur_elem = elem.split(&#39;/&#39;)[-2]
                    if cur_elem != prev_elem:
                        self.peg.append(peg_idx)
                    prev_elem = cur_elem
                    peg_idx += 1
                self.peg.append(len(instr))

        if &#39;augmentation&#39; in options.keys():
            aug_parameters = []
            for key, value in options[&#39;augmentation&#39;].items():
                aug_parameters.append(getattr(iaa, key)(**value))
            self.createarray.augmentation[f&#39;{self.mode}_{self.iter}&#39;] = iaa.Sequential(aug_parameters,
                                                                                       random_order=True)
        del options[&#39;augmentation&#39;]
        instructions[&#39;instructions&#39;] = instr
        instructions[&#39;parameters&#39;] = options

        return instructions

    def instructions_video(self, **options):

        instructions: dict = {}
        instr: list = []
        y_cls: list = []
        cls_idx = 0
        peg_idx = 0
        self.peg.append(0)

        path = self.file_folder
        if options[&#39;folder_name&#39;]:
            path = os.path.join(self.file_folder, options[&#39;folder_name&#39;])
        for directory, folder, file_name in sorted(os.walk(path)):
            if file_name:
                file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
                for name in sorted(file_name):
                    instr.append(os.path.join(file_folder, name))
                    peg_idx += 1
                    if options[&#39;class_mode&#39;] == &#39;По каждому кадру&#39;:
                        y_cls.append(np.full((options[&#39;max_frames&#39;], 1), cls_idx).tolist())
                    else:
                        y_cls.append(cls_idx)
                cls_idx += 1
                self.peg.append(peg_idx)
        instructions[&#39;instructions&#39;] = instr
        instructions[&#39;parameters&#39;] = options
        self.y_cls = y_cls

        return instructions

    def instructions_text(self, **options):

        def read_text(file_path):

            del_symbols = [&#39;\n&#39;, &#39;\t&#39;, &#39;\ufeff&#39;]
            if options[&#39;delete_symbols&#39;]:
                del_symbols += options[&#39;delete_symbols&#39;].split(&#39; &#39;)

            with io_open(file_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;) as f:
                text = f.read()
                for del_symbol in del_symbols:
                    text = text.replace(del_symbol, &#39; &#39;)
            for put, tag in self.tags.items():
                if tag == &#39;text_segmentation&#39;:
                    open_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][0]
                    close_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][-1]
                    text = re.sub(open_symbol, f&#34; {open_symbol}&#34;, text)
                    text = re.sub(close_symbol, f&#34;{close_symbol} &#34;, text)
                    break

            return text

        def apply_pymorphy(text, morphy) -&gt; list:

            words_list = text.split(&#39; &#39;)
            words_list = [morphy.parse(w)[0].normal_form for w in words_list]

            return words_list

        txt_list: dict = {}

        if options[&#39;folder_name&#39;]:
            for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
                txt_list[os.path.join(options[&#39;folder_name&#39;], file_name)] = read_text(
                    os.path.join(self.file_folder, options[&#39;folder_name&#39;], file_name))
        else:
            tree = os.walk(self.file_folder)
            for directory, folder, file_name in sorted(tree):
                if bool(file_name) is not False:
                    folder_name = directory.split(os.path.sep)[-1]
                    for name in sorted(file_name):
                        text_file = read_text(os.path.join(directory, name))
                        if text_file:
                            txt_list[os.path.join(folder_name, name)] = text_file
                else:
                    continue

        #################################################
        if options[&#39;pymorphy&#39;]:
            pymorphy = pymorphy2.MorphAnalyzer()
            for i in range(len(txt_list)):
                txt_list[i] = apply_pymorphy(txt_list[i], pymorphy)
        #################################################

        filters = &#39;–—!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^«»№_`{|}~\t\n\xa0–\ufeff&#39;
        for key, value in self.tags.items():
            if value == &#39;text_segmentation&#39;:
                open_tags = self.user_parameters[key][&#39;open_tags&#39;]
                close_tags = self.user_parameters[key][&#39;close_tags&#39;]
                tags = f&#39;{open_tags} {close_tags}&#39;
                for ch in filters:
                    if ch in set(tags):
                        filters = filters.replace(ch, &#39;&#39;)
                break

        self.createarray.create_tokenizer(self.mode, self.iter, **{&#39;num_words&#39;: options[&#39;max_words_count&#39;],
                                                                   &#39;filters&#39;: filters,
                                                                   &#39;lower&#39;: True,
                                                                   &#39;split&#39;: &#39; &#39;,
                                                                   &#39;char_level&#39;: False,
                                                                   &#39;oov_token&#39;: &#39;&lt;UNK&gt;&#39;})
        self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].fit_on_texts(list(txt_list.values()))

        self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}
        for key, value in txt_list.items():
            self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key] = \
                self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].texts_to_sequences([value])[0]

        if options[&#39;word_to_vec&#39;]:
            reverse_tok = {}
            for key, value in self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].word_index.items():
                reverse_tok[value] = key
            words = []
            for key in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys():
                for lst in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]:
                    tmp = []
                    for word in lst:
                        tmp.append(reverse_tok[word])
                    words.append(tmp)
            self.createarray.create_word2vec(mode=self.mode, iteration=self.iter, words=words,
                                             size=options[&#39;word_to_vec_size&#39;], window=10, min_count=1, workers=10,
                                             iter=10)

        instr = []
        if &#39;text_segmentation&#39; not in self.tags.values():
            y_cls = []
            cls_idx = 0
            length = options[&#39;x_len&#39;]
            stride = options[&#39;step&#39;]
            peg_idx = 0
            self.peg.append(0)
            for key in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
                index = 0
                while index + length &lt;= len(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]):
                    instr.append({&#39;file&#39;: key, &#39;slice&#39;: [index, index + length]})
                    peg_idx += 1
                    index += stride
                    y_cls.append(cls_idx)
                self.peg.append(peg_idx)
                cls_idx += 1
            self.y_cls = y_cls
        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;bag_of_words&#39;: options[&#39;bag_of_words&#39;],
                                       &#39;word_to_vec&#39;: options[&#39;word_to_vec&#39;],
                                       &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;
                                       }
                        }

        return instructions

    def instructions_audio(self):

        pass

    def instructions_dataframe(self, **options):
        &#34;&#34;&#34;
            Args:
                **options: Параметры датафрейма:
                    MinMaxScaler: строка номеров колонок для обработки
                    StandardScaler: строка номеров колонок для обработки
                    Categorical: строка номеров колонок для обработки c уже готовыми категориями
                    Categorical_ranges: dict для присваивания категории  в зависимости от диапазона данных
                        num_cols: число колонок
                        cols: номера колонок
                        col_(int): строка с диапазонами
                    one_hot_encoding: строка номеров колонок для перевода категорий в ОНЕ
                    file_name: имя файла.csv
                    y_col: столбец датафрейма для классификации
            Returns:
                instructions: dict      Словарь с инструкциями для create_dataframe.
        &#34;&#34;&#34;

        def str_to_list(str_numbers, df_cols):
            &#34;&#34;&#34;
            Получает строку из пользовательских номеров колонок,
            возвращает лист индексов данных колонок
            &#34;&#34;&#34;
            merged = []
            try:
                str_numbers = str_numbers.split(&#39; &#39;)
            except:
                print(&#39;Разделите номера колонок ТОЛЬКО пробелами&#39;)
            for i in range(len(str_numbers)):
                if &#39;-&#39; in str_numbers[i]:
                    idx = str_numbers[i].index(&#39;-&#39;)
                    fi = int(str_numbers[i][:idx]) - 1
                    si = int(str_numbers[i][idx + 1:])
                    tmp = list(range(fi, si))
                    merged.extend(tmp)
                elif re.findall(r&#39;\D&#39;, str_numbers[i]) != []:
                    merged.append(df_cols.to_list().index(str_numbers[i]))
                else:
                    merged.append(int(str_numbers[i]) - 1)

            return merged

        general_df = pd.read_csv(os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), nrows=1)
        self.createarray.df_with_y = pd.read_csv(
            os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), usecols=(str_to_list(
                options[&#39;file_info&#39;][&#39;cols_name&#39;][0], general_df.columns) + str_to_list(options[&#39;y_col&#39;],
                                                                                        general_df.columns)))
        self.createarray.df_with_y.sort_values(by=options[&#39;y_col&#39;], inplace=True, ignore_index=True)

        self.peg.append(0)
        for i in range(len(self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]]) - 1):
            if self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i] != \
                    self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i + 1]:
                self.peg.append(i + 1)
        self.peg.append(len(self.createarray.df_with_y))

        self.createarray.df = self.createarray.df_with_y.iloc[:, str_to_list(
            options[&#39;file_info&#39;][&#39;cols_name&#39;][0], self.createarray.df_with_y.columns)]

        instructions = {&#39;instructions&#39;: np.arange(0, len(self.createarray.df)).tolist(),
                        &#39;parameters&#39;: {&#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}}

        if &#39;MinMaxScaler&#39; or &#39;StandardScaler&#39; in options.keys():
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;] = {}
            if &#39;MinMaxScaler&#39; in options.keys():
                instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;] = str_to_list(str_numbers=options[&#39;MinMaxScaler&#39;],
                                                                         df_cols=self.createarray.df.columns)
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;] = MinMaxScaler()
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;].fit(
                    self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;]].to_numpy().reshape(-1, 1))

            if &#39;StandardScaler&#39; in options.keys():
                instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;] = str_to_list(options[&#39;StandardScaler&#39;],
                                                                           self.createarray.df.columns)
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;] = StandardScaler()
                self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;].fit(
                    self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;]].to_numpy().reshape(-1, 1))

        if &#39;Categorical&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;Categorical&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;Categorical&#39;],
                                                                                self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;]:
                instructions[&#39;parameters&#39;][&#39;Categorical&#39;][f&#39;col_{i}&#39;] = np.unique(
                    self.createarray.df.iloc[:, i]).tolist()

        if &#39;Categorical_ranges&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;] = str_to_list(
                options[&#39;Categorical_ranges&#39;][&#39;cols&#39;], self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;] = {}
                for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))):
                    instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;] = int(
                        options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;)[j])

        if &#39;one_hot_encoding&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;] = {}
            instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;one_hot_encoding&#39;],
                                                                                     self.createarray.df.columns)
            for i in instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
                if i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                    instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                        options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))
                else:
                    instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                        np.unique(self.createarray.df.iloc[:, i]))

        return instructions

    def instructions_classification(self, **options):

        instructions: dict = {}
        self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;classification&#39;
        self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;one_hot_encoding&#39;]

        if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
            for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
                data = pd.read_csv(os.path.join(self.file_folder, file_name), usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
                column = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
                classes_names = []
                for elem in column:
                    if elem not in classes_names:
                        classes_names.append(elem)
                self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = classes_names
                self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(classes_names)
                for elem in column:
                    self.y_cls.append(classes_names.index(elem))

        else:
            for key, value in self.tags.items():
                if value in [&#39;images&#39;, &#39;text&#39;, &#39;audio&#39;, &#39;video&#39;]:
                    self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = \
                        sorted(self.user_parameters[key][&#39;file_info&#39;][&#39;path&#39;])
                    self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(self.classes_names[f&#39;{self.mode}_{self.iter}&#39;])

        instructions[&#39;parameters&#39;] = {&#39;num_classes&#39;: len(np.unique(self.y_cls)),
                                      &#39;one_hot_encoding&#39;: options[&#39;one_hot_encoding&#39;]}
        instructions[&#39;instructions&#39;] = self.y_cls

        return instructions

    def instructions_regression(self):

        pass

    def instructions_segmentation(self, **options):

        instr: list = []

        self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_names&#39;]
        self.classes_colors[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_colors&#39;]
        self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(options[&#39;classes_names&#39;])
        self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = True
        self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;segmentation&#39;

        for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
            instr.append(os.path.join(options[&#39;folder_name&#39;], file_name))

        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;mask_range&#39;: options[&#39;mask_range&#39;],
                                       &#39;num_classes&#39;: len(options[&#39;classes_names&#39;]),
                                       &#39;shape&#39;: (self.user_parameters[&#39;input_1&#39;][&#39;height&#39;],
                                                 self.user_parameters[&#39;input_1&#39;][&#39;width&#39;]),
                                       &#39;classes_colors&#39;: options[&#39;classes_colors&#39;]
                                       }
                        }

        return instructions

    def instructions_object_detection(self, **options):

        data = {}
        instructions = {}
        parameters = {}
        class_names = []

        # obj.data
        with open(os.path.join(self.file_folder, &#39;obj.data&#39;), &#39;r&#39;) as dt:
            d = dt.read()
        for elem in d.split(&#39;\n&#39;):
            if elem:
                elem = elem.split(&#39; = &#39;)
                data[elem[0]] = elem[1]

        for key, value in self.tags.items():
            if value == &#39;images&#39;:
                parameters[&#39;height&#39;] = self.user_parameters[key][&#39;height&#39;]
                parameters[&#39;width&#39;] = self.user_parameters[key][&#39;width&#39;]
                parameters[&#39;num_classes&#39;] = int(data[&#39;classes&#39;])

        # obj.names
        with open(os.path.join(self.file_folder, data[&#34;names&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
            names = dt.read()
        for elem in names.split(&#39;\n&#39;):
            if elem:
                class_names.append(elem)

        for i in range(3):
            self.classes_names[f&#39;{self.mode}_{self.iter + i}&#39;] = class_names
            self.num_classes[f&#39;{self.mode}_{self.iter + i}&#39;] = int(data[&#39;classes&#39;])

        # list of txt
        txt_list = []
        with open(os.path.join(self.file_folder, data[&#34;train&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
            images = dt.read()
        for elem in sorted(images.split(&#39;\n&#39;)):
            if elem:
                idx = elem.rfind(&#39;.&#39;)
                elem = elem.replace(elem[idx:], &#39;.txt&#39;)
                txt_list.append(os.path.join(*elem.split(&#39;/&#39;)[1:]))
        instructions[&#39;instructions&#39;] = txt_list
        instructions[&#39;parameters&#39;] = parameters

        return instructions

    def instructions_text_segmentation(self, **options):

        &#34;&#34;&#34;

        Args:
            **options:
                open_tags: str
                    Открывающие теги.
                close_tags: str
                    Закрывающие теги.

        Returns:

        &#34;&#34;&#34;

        def get_ohe_samples(list_of_txt, tags_index):

            segment_array = []
            new_list_of_txt = []
            tag_place = [0 for _ in range(len(open_tags))]
            for ex in list_of_txt:
                if ex in tags_index:
                    place = np.argwhere(tags_index == ex)
                    if len(place) != 0:
                        if place[0][0] &lt; len(open_tags):
                            tag_place[place[0][0]] = 1
                        else:
                            tag_place[place[0][0] - len(open_tags)] = 0
                else:
                    new_list_of_txt.append(ex)
                    segment_array.append(np.where(np.array(tag_place) == 1)[0].tolist())

            return new_list_of_txt, segment_array

        instr: list = []
        open_tags: list = options[&#39;open_tags&#39;].split(&#39; &#39;)
        close_tags: list = options[&#39;close_tags&#39;].split(&#39; &#39;)
        tags: list = open_tags + close_tags
        self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}

        for key, value in self.tags.items():
            if value == &#39;text&#39;:
                tags_indexes = np.array([self.createarray.tokenizer[key].word_index[idx] for idx in tags])
                for txt_file in self.createarray.txt_list[key].keys():
                    text_instr, segment_instr = get_ohe_samples(self.createarray.txt_list[key][txt_file], tags_indexes)
                    self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][txt_file] = segment_instr
                    self.createarray.txt_list[key][txt_file] = text_instr

                length = self.user_parameters[key][&#39;x_len&#39;]
                stride = self.user_parameters[key][&#39;step&#39;]
                peg_idx = 0
                self.peg = []
                self.peg.append(0)
                for path in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
                    index = 0
                    while index + length &lt;= len(self.createarray.txt_list[key][path]):
                        instr.append({&#39;file&#39;: path, &#39;slice&#39;: [index, index + length]})
                        peg_idx += 1
                        index += stride
                    self.peg.append(peg_idx)
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;] = instr

        instructions = {&#39;instructions&#39;: instr,
                        &#39;parameters&#39;: {&#39;num_classes&#39;: len(open_tags),
                                       &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}
                        }

        return instructions</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.trds.CreateDTS.create_dataset"><code class="name flex">
<span>def <span class="ident">create_dataset</span></span>(<span>self, dataset_dict: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dataset(self, dataset_dict: dict):

    self.createarray = CreateArray(file_folder=self.file_folder)

    self.name = dataset_dict[&#39;parameters&#39;][&#39;name&#39;]
    self.divide_ratio = (dataset_dict[&#39;parameters&#39;][&#39;train_part&#39;], dataset_dict[&#39;parameters&#39;][&#39;val_part&#39;],
                         dataset_dict[&#39;parameters&#39;][&#39;test_part&#39;])
    self.source = &#39;custom dataset&#39;
    self.user_tags = dataset_dict[&#39;parameters&#39;][&#39;user_tags&#39;]
    self.use_generator = dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]

    for key in dataset_dict[&#39;inputs&#39;].keys():
        self.tags[key] = dataset_dict[&#39;inputs&#39;][key][&#39;tag&#39;]
        self.input_names[key] = dataset_dict[&#39;inputs&#39;][key][&#39;name&#39;]
        self.user_parameters[key] = dataset_dict[&#39;inputs&#39;][key][&#39;parameters&#39;]
    for key in dataset_dict[&#39;outputs&#39;].keys():
        self.tags[key] = dataset_dict[&#39;outputs&#39;][key][&#39;tag&#39;]
        self.output_names[key] = dataset_dict[&#39;outputs&#39;][key][&#39;name&#39;]
        self.user_parameters[key] = dataset_dict[&#39;outputs&#39;][key][&#39;parameters&#39;]

    # Создаем входные инструкции
    self.iter = 0
    self.mode = &#39;input&#39;
    for inp in dataset_dict[&#39;inputs&#39;]:
        self.iter += 1
        self.instructions[&#39;inputs&#39;][inp] = getattr(self, f&#34;instructions_{self.tags[inp]}&#34;)(
            **dataset_dict[&#39;inputs&#39;][inp][&#39;parameters&#39;])
    # Создаем выходные инструкции
    self.iter = 0
    self.mode = &#39;output&#39;
    for out in dataset_dict[&#39;outputs&#39;]:
        self.iter += 1
        self.instructions[&#39;outputs&#39;][out] = getattr(self, f&#34;instructions_{self.tags[out]}&#34;)(
            **dataset_dict[&#39;outputs&#39;][out][&#39;parameters&#39;])

    # Получаем входные параметры
    for key in self.instructions[&#39;inputs&#39;].keys():
        array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
            self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
        self.input_shape[key] = array.shape
        self.input_dtype[key] = str(array.dtype)
        self.input_datatype[key] = self._set_datatype(array.shape)
    # Получаем выходные параметры
    for key in self.instructions[&#39;outputs&#39;].keys():
        array = getattr(self.createarray, f&#39;create_{self.tags[key]}&#39;)(
            self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][0], **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
        if isinstance(array, tuple):
            for i in range(len(array)):
                self.output_shape[key.replace(key[-1], str(int(key[-1]) + i))] = array[i].shape
                self.output_dtype[key.replace(key[-1], str(int(key[-1]) + i))] = str(array[i].dtype)
                self.output_datatype[key.replace(key[-1], str(int(key[-1]) + i))] = self._set_datatype(
                    array[i].shape)
        else:
            self.output_shape[key] = array.shape
            self.output_dtype[key] = str(array.dtype)
            self.output_datatype[key] = self._set_datatype(array.shape)

    # Разделение на три выборки
    self.split_sequence[&#39;train&#39;] = []
    self.split_sequence[&#39;val&#39;] = []
    self.split_sequence[&#39;test&#39;] = []
    for i in range(len(self.peg) - 1):
        indices = np.arange(self.peg[i], self.peg[i + 1])
        train_len = int(self.divide_ratio[0] * len(indices))
        val_len = int(self.divide_ratio[1] * len(indices))
        indices = indices.tolist()
        self.split_sequence[&#39;train&#39;].extend(indices[:train_len])
        self.split_sequence[&#39;val&#39;].extend(indices[train_len:train_len + val_len])
        self.split_sequence[&#39;test&#39;].extend(indices[train_len + val_len:])
    if not dataset_dict[&#39;parameters&#39;][&#39;preserve_sequence&#39;]:
        random.shuffle(self.split_sequence[&#39;train&#39;])
        random.shuffle(self.split_sequence[&#39;val&#39;])
        random.shuffle(self.split_sequence[&#39;test&#39;])

    self.limit: int = len(self.instructions[&#39;inputs&#39;][&#39;input_1&#39;][&#39;instructions&#39;])

    data = {}
    if dataset_dict[&#39;parameters&#39;][&#39;use_generator&#39;]:
        # Сохранение датасета для генератора
        data[&#39;zip_params&#39;] = self.zip_params
        os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;), exist_ok=True)
        for key in self.instructions.keys():
            os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key), exist_ok=True)
            for inp in self.instructions[key].keys():
                with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, key, f&#39;{inp}.json&#39;),
                          &#39;w&#39;) as instruction:
                    json.dump(self.instructions[key][inp], instruction)
        with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                  &#39;w&#39;) as seq:
            json.dump(self.split_sequence, seq)
        if &#39;text&#39; in self.tags.keys():  # if &#39;txt_list&#39; in self.createarray.__dict__.keys():
            with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                      &#39;w&#39;) as fp:
                json.dump(self.createarray.txt_list, fp)
    else:
        # Сохранение датасета с NumPy
        for key in self.instructions[&#39;inputs&#39;].keys():
            x: list = []
            for i in range(self.limit):
                x.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][i],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;]))
            self.X[&#39;train&#39;][key] = np.array(x)[self.split_sequence[&#39;train&#39;]]
            self.X[&#39;val&#39;][key] = np.array(x)[self.split_sequence[&#39;val&#39;]]
            self.X[&#39;test&#39;][key] = np.array(x)[self.split_sequence[&#39;test&#39;]]

        for key in self.instructions[&#39;outputs&#39;].keys():
            if &#39;object_detection&#39; in self.tags.values():
                y_1: list = []
                y_2: list = []
                y_3: list = []
                for i in range(self.limit):
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    y_1.append(arrays[0])
                    y_2.append(arrays[1])
                    y_3.append(arrays[2])

                splits = [&#39;train&#39;, &#39;val&#39;, &#39;test&#39;]
                for spl_seq in splits:
                    for i in range(len(splits)):
                        self.Y[spl_seq][key.replace(key[-1], str(int(key[-1])))] = np.array(y_1)[
                            self.split_sequence[spl_seq]]
                        self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 1))] = np.array(y_2)[
                            self.split_sequence[spl_seq]]
                        self.Y[spl_seq][key.replace(key[-1], str(int(key[-1]) + 2))] = np.array(y_3)[
                            self.split_sequence[spl_seq]]
            else:
                y: list = []
                for i in range(self.limit):
                    y.append(getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][i],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;]))
                self.Y[&#39;train&#39;][key] = np.array(y)[self.split_sequence[&#39;train&#39;]]
                self.Y[&#39;val&#39;][key] = np.array(y)[self.split_sequence[&#39;val&#39;]]
                self.Y[&#39;test&#39;][key] = np.array(y)[self.split_sequence[&#39;test&#39;]]

        for sample in self.X.keys():
            os.makedirs(os.path.join(self.trds_path, &#39;arrays&#39;, sample), exist_ok=True)
            for inp in self.X[sample].keys():
                os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                joblib.dump(self.X[sample][inp],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

        for sample in self.Y.keys():
            for inp in self.Y[sample].keys():
                os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample), exist_ok=True)
                joblib.dump(self.Y[sample][inp],
                            os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;arrays&#39;, sample, f&#39;{inp}.gz&#39;))

    if self.createarray.scaler:
        os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;), exist_ok=True)
    if self.createarray.tokenizer:
        os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;), exist_ok=True)
    if self.createarray.word2vec:
        os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;), exist_ok=True)
    if self.createarray.augmentation:
        os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;), exist_ok=True)
    # if self.createarray.tsgenerator:
    #     os.makedirs(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;), exist_ok=True)

    for scaler in self.createarray.scaler.keys():
        if self.createarray.scaler[scaler]:
            joblib.dump(self.createarray.scaler[scaler],
                        os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;scalers&#39;, f&#39;{scaler}.gz&#39;))
    for tok in self.createarray.tokenizer.keys():
        if self.createarray.tokenizer[tok]:
            joblib.dump(self.createarray.tokenizer[tok],
                        os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tokenizer&#39;, f&#39;{tok}.gz&#39;))
    for w2v in self.createarray.word2vec.keys():
        if self.createarray.word2vec[w2v]:
            joblib.dump(self.createarray.word2vec[w2v],
                        os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;word2vec&#39;, f&#39;{w2v}.gz&#39;))
    for aug in self.createarray.augmentation.keys():
        if self.createarray.augmentation[aug]:
            joblib.dump(self.createarray.augmentation[aug],
                        os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;augmentation&#39;, f&#39;{aug}.gz&#39;))
    # for tsg in self.createarray.tsgenerator.keys():
    #     if self.createarray.tsgenerator[tsg]:
    #         joblib.dump(self.createarray.tsgenerator[tsg],
    #                     os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;tsgenerator&#39;, f&#39;{tsg}.gz&#39;))

    attributes = [&#39;name&#39;, &#39;source&#39;, &#39;tags&#39;, &#39;user_tags&#39;, &#39;language&#39;,
                  &#39;input_datatype&#39;, &#39;input_dtype&#39;, &#39;input_shape&#39;, &#39;input_names&#39;,
                  &#39;output_datatype&#39;, &#39;output_dtype&#39;, &#39;output_shape&#39;, &#39;output_names&#39;,
                  &#39;num_classes&#39;, &#39;classes_names&#39;, &#39;classes_colors&#39;,
                  &#39;one_hot_encoding&#39;, &#39;task_type&#39;, &#39;limit&#39;, &#39;use_generator&#39;]

    for attr in attributes:
        data[attr] = self.__dict__[attr]
    data[&#39;date&#39;] = datetime.now().astimezone(timezone(&#39;Europe/Moscow&#39;)).isoformat()
    with open(os.path.join(self.trds_path, f&#39;dataset {self.name}&#39;, &#39;config.json&#39;), &#39;w&#39;) as fp:
        json.dump(data, fp)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_audio"><code class="name flex">
<span>def <span class="ident">instructions_audio</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_audio(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_classification"><code class="name flex">
<span>def <span class="ident">instructions_classification</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_classification(self, **options):

    instructions: dict = {}
    self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;classification&#39;
    self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;one_hot_encoding&#39;]

    if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
        for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
            data = pd.read_csv(os.path.join(self.file_folder, file_name), usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
            column = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
            classes_names = []
            for elem in column:
                if elem not in classes_names:
                    classes_names.append(elem)
            self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = classes_names
            self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(classes_names)
            for elem in column:
                self.y_cls.append(classes_names.index(elem))

    else:
        for key, value in self.tags.items():
            if value in [&#39;images&#39;, &#39;text&#39;, &#39;audio&#39;, &#39;video&#39;]:
                self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = \
                    sorted(self.user_parameters[key][&#39;file_info&#39;][&#39;path&#39;])
                self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(self.classes_names[f&#39;{self.mode}_{self.iter}&#39;])

    instructions[&#39;parameters&#39;] = {&#39;num_classes&#39;: len(np.unique(self.y_cls)),
                                  &#39;one_hot_encoding&#39;: options[&#39;one_hot_encoding&#39;]}
    instructions[&#39;instructions&#39;] = self.y_cls

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_dataframe"><code class="name flex">
<span>def <span class="ident">instructions_dataframe</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры датафрейма:
MinMaxScaler: строка номеров колонок для обработки
StandardScaler: строка номеров колонок для обработки
Categorical: строка номеров колонок для обработки c уже готовыми категориями
Categorical_ranges: dict для присваивания категории
в зависимости от диапазона данных
num_cols: число колонок
cols: номера колонок
col_(int): строка с диапазонами
one_hot_encoding: строка номеров колонок для перевода категорий в ОНЕ
file_name: имя файла.csv
y_col: столбец датафрейма для классификации</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>instructions</code></dt>
<dd>dict
Словарь с инструкциями для create_dataframe.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_dataframe(self, **options):
    &#34;&#34;&#34;
        Args:
            **options: Параметры датафрейма:
                MinMaxScaler: строка номеров колонок для обработки
                StandardScaler: строка номеров колонок для обработки
                Categorical: строка номеров колонок для обработки c уже готовыми категориями
                Categorical_ranges: dict для присваивания категории  в зависимости от диапазона данных
                    num_cols: число колонок
                    cols: номера колонок
                    col_(int): строка с диапазонами
                one_hot_encoding: строка номеров колонок для перевода категорий в ОНЕ
                file_name: имя файла.csv
                y_col: столбец датафрейма для классификации
        Returns:
            instructions: dict      Словарь с инструкциями для create_dataframe.
    &#34;&#34;&#34;

    def str_to_list(str_numbers, df_cols):
        &#34;&#34;&#34;
        Получает строку из пользовательских номеров колонок,
        возвращает лист индексов данных колонок
        &#34;&#34;&#34;
        merged = []
        try:
            str_numbers = str_numbers.split(&#39; &#39;)
        except:
            print(&#39;Разделите номера колонок ТОЛЬКО пробелами&#39;)
        for i in range(len(str_numbers)):
            if &#39;-&#39; in str_numbers[i]:
                idx = str_numbers[i].index(&#39;-&#39;)
                fi = int(str_numbers[i][:idx]) - 1
                si = int(str_numbers[i][idx + 1:])
                tmp = list(range(fi, si))
                merged.extend(tmp)
            elif re.findall(r&#39;\D&#39;, str_numbers[i]) != []:
                merged.append(df_cols.to_list().index(str_numbers[i]))
            else:
                merged.append(int(str_numbers[i]) - 1)

        return merged

    general_df = pd.read_csv(os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), nrows=1)
    self.createarray.df_with_y = pd.read_csv(
        os.path.join(self.file_folder, options[&#39;file_info&#39;][&#39;path&#39;][0]), usecols=(str_to_list(
            options[&#39;file_info&#39;][&#39;cols_name&#39;][0], general_df.columns) + str_to_list(options[&#39;y_col&#39;],
                                                                                    general_df.columns)))
    self.createarray.df_with_y.sort_values(by=options[&#39;y_col&#39;], inplace=True, ignore_index=True)

    self.peg.append(0)
    for i in range(len(self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]]) - 1):
        if self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i] != \
                self.createarray.df_with_y.loc[:, options[&#39;y_col&#39;]][i + 1]:
            self.peg.append(i + 1)
    self.peg.append(len(self.createarray.df_with_y))

    self.createarray.df = self.createarray.df_with_y.iloc[:, str_to_list(
        options[&#39;file_info&#39;][&#39;cols_name&#39;][0], self.createarray.df_with_y.columns)]

    instructions = {&#39;instructions&#39;: np.arange(0, len(self.createarray.df)).tolist(),
                    &#39;parameters&#39;: {&#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}}

    if &#39;MinMaxScaler&#39; or &#39;StandardScaler&#39; in options.keys():
        self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;] = {}
        if &#39;MinMaxScaler&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;] = str_to_list(str_numbers=options[&#39;MinMaxScaler&#39;],
                                                                     df_cols=self.createarray.df.columns)
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;] = MinMaxScaler()
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;MinMaxScaler&#39;].fit(
                self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;MinMaxScaler&#39;]].to_numpy().reshape(-1, 1))

        if &#39;StandardScaler&#39; in options.keys():
            instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;] = str_to_list(options[&#39;StandardScaler&#39;],
                                                                       self.createarray.df.columns)
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;] = StandardScaler()
            self.createarray.scaler[f&#39;{self.mode}_{self.iter}&#39;][&#39;StandardScaler&#39;].fit(
                self.createarray.df.iloc[:, instructions[&#39;parameters&#39;][&#39;StandardScaler&#39;]].to_numpy().reshape(-1, 1))

    if &#39;Categorical&#39; in options.keys():
        instructions[&#39;parameters&#39;][&#39;Categorical&#39;] = {}
        instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;Categorical&#39;],
                                                                            self.createarray.df.columns)
        for i in instructions[&#39;parameters&#39;][&#39;Categorical&#39;][&#39;lst_cols&#39;]:
            instructions[&#39;parameters&#39;][&#39;Categorical&#39;][f&#39;col_{i}&#39;] = np.unique(
                self.createarray.df.iloc[:, i]).tolist()

    if &#39;Categorical_ranges&#39; in options.keys():
        instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;] = {}
        instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;] = str_to_list(
            options[&#39;Categorical_ranges&#39;][&#39;cols&#39;], self.createarray.df.columns)
        for i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
            instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;] = {}
            for j in range(len(options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))):
                instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][f&#39;col_{i}&#39;][f&#39;range_{j}&#39;] = int(
                    options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;)[j])

    if &#39;one_hot_encoding&#39; in options.keys():
        instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;] = {}
        instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;] = str_to_list(options[&#39;one_hot_encoding&#39;],
                                                                                 self.createarray.df.columns)
        for i in instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][&#39;lst_cols&#39;]:
            if i in instructions[&#39;parameters&#39;][&#39;Categorical_ranges&#39;][&#39;lst_cols&#39;]:
                instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                    options[&#39;Categorical_ranges&#39;][f&#39;col_{i + 1}&#39;].split(&#39; &#39;))
            else:
                instructions[&#39;parameters&#39;][&#39;one_hot_encoding&#39;][f&#39;col_{i}&#39;] = len(
                    np.unique(self.createarray.df.iloc[:, i]))

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_images"><code class="name flex">
<span>def <span class="ident">instructions_images</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_images(self, **options):

    instructions: dict = {}
    instr: list = []
    y_cls: list = []
    cls_idx = 0
    peg_idx = 0
    self.peg.append(0)
    options[&#39;put&#39;] = f&#39;{self.mode}_{self.iter}&#39;
    if &#39;object_detection&#39; in self.tags.values():
        options[&#39;object_detection&#39;] = True
    if options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_folder&#39;:
        for folder_name in options[&#39;file_info&#39;][&#39;path&#39;]:
            for directory, folder, file_name in sorted(os.walk(os.path.join(self.file_folder, folder_name))):
                if file_name:
                    file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
                    for name in sorted(file_name):
                        if &#39;object_detection&#39; in self.tags.values():
                            if &#39;txt&#39; not in name:
                                instr.append(os.path.join(file_folder, name))
                                peg_idx += 1
                        else:
                            instr.append(os.path.join(file_folder, name))
                            peg_idx += 1
                        y_cls.append(cls_idx)
                    cls_idx += 1
                    self.peg.append(peg_idx)
        self.y_cls = y_cls
    elif options[&#39;file_info&#39;][&#39;path_type&#39;] == &#39;path_file&#39;:
        for file_name in options[&#39;file_info&#39;][&#39;path&#39;]:
            data = pd.read_csv(os.path.join(self.file_folder, file_name),
                               usecols=options[&#39;file_info&#39;][&#39;cols_name&#39;])
            instr = data[options[&#39;file_info&#39;][&#39;cols_name&#39;][0]].to_list()
            prev_elem = instr[0].split(&#39;/&#39;)[-2]
            for elem in instr:
                cur_elem = elem.split(&#39;/&#39;)[-2]
                if cur_elem != prev_elem:
                    self.peg.append(peg_idx)
                prev_elem = cur_elem
                peg_idx += 1
            self.peg.append(len(instr))

    if &#39;augmentation&#39; in options.keys():
        aug_parameters = []
        for key, value in options[&#39;augmentation&#39;].items():
            aug_parameters.append(getattr(iaa, key)(**value))
        self.createarray.augmentation[f&#39;{self.mode}_{self.iter}&#39;] = iaa.Sequential(aug_parameters,
                                                                                   random_order=True)
    del options[&#39;augmentation&#39;]
    instructions[&#39;instructions&#39;] = instr
    instructions[&#39;parameters&#39;] = options

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_object_detection"><code class="name flex">
<span>def <span class="ident">instructions_object_detection</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_object_detection(self, **options):

    data = {}
    instructions = {}
    parameters = {}
    class_names = []

    # obj.data
    with open(os.path.join(self.file_folder, &#39;obj.data&#39;), &#39;r&#39;) as dt:
        d = dt.read()
    for elem in d.split(&#39;\n&#39;):
        if elem:
            elem = elem.split(&#39; = &#39;)
            data[elem[0]] = elem[1]

    for key, value in self.tags.items():
        if value == &#39;images&#39;:
            parameters[&#39;height&#39;] = self.user_parameters[key][&#39;height&#39;]
            parameters[&#39;width&#39;] = self.user_parameters[key][&#39;width&#39;]
            parameters[&#39;num_classes&#39;] = int(data[&#39;classes&#39;])

    # obj.names
    with open(os.path.join(self.file_folder, data[&#34;names&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
        names = dt.read()
    for elem in names.split(&#39;\n&#39;):
        if elem:
            class_names.append(elem)

    for i in range(3):
        self.classes_names[f&#39;{self.mode}_{self.iter + i}&#39;] = class_names
        self.num_classes[f&#39;{self.mode}_{self.iter + i}&#39;] = int(data[&#39;classes&#39;])

    # list of txt
    txt_list = []
    with open(os.path.join(self.file_folder, data[&#34;train&#34;].split(&#34;/&#34;)[-1]), &#39;r&#39;) as dt:
        images = dt.read()
    for elem in sorted(images.split(&#39;\n&#39;)):
        if elem:
            idx = elem.rfind(&#39;.&#39;)
            elem = elem.replace(elem[idx:], &#39;.txt&#39;)
            txt_list.append(os.path.join(*elem.split(&#39;/&#39;)[1:]))
    instructions[&#39;instructions&#39;] = txt_list
    instructions[&#39;parameters&#39;] = parameters

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_regression"><code class="name flex">
<span>def <span class="ident">instructions_regression</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_regression(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_segmentation"><code class="name flex">
<span>def <span class="ident">instructions_segmentation</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_segmentation(self, **options):

    instr: list = []

    self.classes_names[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_names&#39;]
    self.classes_colors[f&#39;{self.mode}_{self.iter}&#39;] = options[&#39;classes_colors&#39;]
    self.num_classes[f&#39;{self.mode}_{self.iter}&#39;] = len(options[&#39;classes_names&#39;])
    self.one_hot_encoding[f&#39;{self.mode}_{self.iter}&#39;] = True
    self.task_type[f&#39;{self.mode}_{self.iter}&#39;] = &#39;segmentation&#39;

    for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
        instr.append(os.path.join(options[&#39;folder_name&#39;], file_name))

    instructions = {&#39;instructions&#39;: instr,
                    &#39;parameters&#39;: {&#39;mask_range&#39;: options[&#39;mask_range&#39;],
                                   &#39;num_classes&#39;: len(options[&#39;classes_names&#39;]),
                                   &#39;shape&#39;: (self.user_parameters[&#39;input_1&#39;][&#39;height&#39;],
                                             self.user_parameters[&#39;input_1&#39;][&#39;width&#39;]),
                                   &#39;classes_colors&#39;: options[&#39;classes_colors&#39;]
                                   }
                    }

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_text"><code class="name flex">
<span>def <span class="ident">instructions_text</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_text(self, **options):

    def read_text(file_path):

        del_symbols = [&#39;\n&#39;, &#39;\t&#39;, &#39;\ufeff&#39;]
        if options[&#39;delete_symbols&#39;]:
            del_symbols += options[&#39;delete_symbols&#39;].split(&#39; &#39;)

        with io_open(file_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;) as f:
            text = f.read()
            for del_symbol in del_symbols:
                text = text.replace(del_symbol, &#39; &#39;)
        for put, tag in self.tags.items():
            if tag == &#39;text_segmentation&#39;:
                open_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][0]
                close_symbol = self.user_parameters[put][&#39;open_tags&#39;].split(&#39; &#39;)[0][-1]
                text = re.sub(open_symbol, f&#34; {open_symbol}&#34;, text)
                text = re.sub(close_symbol, f&#34;{close_symbol} &#34;, text)
                break

        return text

    def apply_pymorphy(text, morphy) -&gt; list:

        words_list = text.split(&#39; &#39;)
        words_list = [morphy.parse(w)[0].normal_form for w in words_list]

        return words_list

    txt_list: dict = {}

    if options[&#39;folder_name&#39;]:
        for file_name in sorted(os.listdir(os.path.join(self.file_folder, options[&#39;folder_name&#39;]))):
            txt_list[os.path.join(options[&#39;folder_name&#39;], file_name)] = read_text(
                os.path.join(self.file_folder, options[&#39;folder_name&#39;], file_name))
    else:
        tree = os.walk(self.file_folder)
        for directory, folder, file_name in sorted(tree):
            if bool(file_name) is not False:
                folder_name = directory.split(os.path.sep)[-1]
                for name in sorted(file_name):
                    text_file = read_text(os.path.join(directory, name))
                    if text_file:
                        txt_list[os.path.join(folder_name, name)] = text_file
            else:
                continue

    #################################################
    if options[&#39;pymorphy&#39;]:
        pymorphy = pymorphy2.MorphAnalyzer()
        for i in range(len(txt_list)):
            txt_list[i] = apply_pymorphy(txt_list[i], pymorphy)
    #################################################

    filters = &#39;–—!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^«»№_`{|}~\t\n\xa0–\ufeff&#39;
    for key, value in self.tags.items():
        if value == &#39;text_segmentation&#39;:
            open_tags = self.user_parameters[key][&#39;open_tags&#39;]
            close_tags = self.user_parameters[key][&#39;close_tags&#39;]
            tags = f&#39;{open_tags} {close_tags}&#39;
            for ch in filters:
                if ch in set(tags):
                    filters = filters.replace(ch, &#39;&#39;)
            break

    self.createarray.create_tokenizer(self.mode, self.iter, **{&#39;num_words&#39;: options[&#39;max_words_count&#39;],
                                                               &#39;filters&#39;: filters,
                                                               &#39;lower&#39;: True,
                                                               &#39;split&#39;: &#39; &#39;,
                                                               &#39;char_level&#39;: False,
                                                               &#39;oov_token&#39;: &#39;&lt;UNK&gt;&#39;})
    self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].fit_on_texts(list(txt_list.values()))

    self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}
    for key, value in txt_list.items():
        self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key] = \
            self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].texts_to_sequences([value])[0]

    if options[&#39;word_to_vec&#39;]:
        reverse_tok = {}
        for key, value in self.createarray.tokenizer[f&#39;{self.mode}_{self.iter}&#39;].word_index.items():
            reverse_tok[value] = key
        words = []
        for key in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys():
            for lst in self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]:
                tmp = []
                for word in lst:
                    tmp.append(reverse_tok[word])
                words.append(tmp)
        self.createarray.create_word2vec(mode=self.mode, iteration=self.iter, words=words,
                                         size=options[&#39;word_to_vec_size&#39;], window=10, min_count=1, workers=10,
                                         iter=10)

    instr = []
    if &#39;text_segmentation&#39; not in self.tags.values():
        y_cls = []
        cls_idx = 0
        length = options[&#39;x_len&#39;]
        stride = options[&#39;step&#39;]
        peg_idx = 0
        self.peg.append(0)
        for key in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
            index = 0
            while index + length &lt;= len(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][key]):
                instr.append({&#39;file&#39;: key, &#39;slice&#39;: [index, index + length]})
                peg_idx += 1
                index += stride
                y_cls.append(cls_idx)
            self.peg.append(peg_idx)
            cls_idx += 1
        self.y_cls = y_cls
    instructions = {&#39;instructions&#39;: instr,
                    &#39;parameters&#39;: {&#39;bag_of_words&#39;: options[&#39;bag_of_words&#39;],
                                   &#39;word_to_vec&#39;: options[&#39;word_to_vec&#39;],
                                   &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;
                                   }
                    }

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_text_segmentation"><code class="name flex">
<span>def <span class="ident">instructions_text_segmentation</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>**options:
open_tags: str
Открывающие теги.
close_tags: str
Закрывающие теги.
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_text_segmentation(self, **options):

    &#34;&#34;&#34;

    Args:
        **options:
            open_tags: str
                Открывающие теги.
            close_tags: str
                Закрывающие теги.

    Returns:

    &#34;&#34;&#34;

    def get_ohe_samples(list_of_txt, tags_index):

        segment_array = []
        new_list_of_txt = []
        tag_place = [0 for _ in range(len(open_tags))]
        for ex in list_of_txt:
            if ex in tags_index:
                place = np.argwhere(tags_index == ex)
                if len(place) != 0:
                    if place[0][0] &lt; len(open_tags):
                        tag_place[place[0][0]] = 1
                    else:
                        tag_place[place[0][0] - len(open_tags)] = 0
            else:
                new_list_of_txt.append(ex)
                segment_array.append(np.where(np.array(tag_place) == 1)[0].tolist())

        return new_list_of_txt, segment_array

    instr: list = []
    open_tags: list = options[&#39;open_tags&#39;].split(&#39; &#39;)
    close_tags: list = options[&#39;close_tags&#39;].split(&#39; &#39;)
    tags: list = open_tags + close_tags
    self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;] = {}

    for key, value in self.tags.items():
        if value == &#39;text&#39;:
            tags_indexes = np.array([self.createarray.tokenizer[key].word_index[idx] for idx in tags])
            for txt_file in self.createarray.txt_list[key].keys():
                text_instr, segment_instr = get_ohe_samples(self.createarray.txt_list[key][txt_file], tags_indexes)
                self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;][txt_file] = segment_instr
                self.createarray.txt_list[key][txt_file] = text_instr

            length = self.user_parameters[key][&#39;x_len&#39;]
            stride = self.user_parameters[key][&#39;step&#39;]
            peg_idx = 0
            self.peg = []
            self.peg.append(0)
            for path in sorted(self.createarray.txt_list[f&#39;{self.mode}_{self.iter}&#39;].keys()):
                index = 0
                while index + length &lt;= len(self.createarray.txt_list[key][path]):
                    instr.append({&#39;file&#39;: path, &#39;slice&#39;: [index, index + length]})
                    peg_idx += 1
                    index += stride
                self.peg.append(peg_idx)
            self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;] = instr

    instructions = {&#39;instructions&#39;: instr,
                    &#39;parameters&#39;: {&#39;num_classes&#39;: len(open_tags),
                                   &#39;put&#39;: f&#39;{self.mode}_{self.iter}&#39;}
                    }

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.instructions_video"><code class="name flex">
<span>def <span class="ident">instructions_video</span></span>(<span>self, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instructions_video(self, **options):

    instructions: dict = {}
    instr: list = []
    y_cls: list = []
    cls_idx = 0
    peg_idx = 0
    self.peg.append(0)

    path = self.file_folder
    if options[&#39;folder_name&#39;]:
        path = os.path.join(self.file_folder, options[&#39;folder_name&#39;])
    for directory, folder, file_name in sorted(os.walk(path)):
        if file_name:
            file_folder = directory.replace(self.file_folder, &#39;&#39;)[1:]
            for name in sorted(file_name):
                instr.append(os.path.join(file_folder, name))
                peg_idx += 1
                if options[&#39;class_mode&#39;] == &#39;По каждому кадру&#39;:
                    y_cls.append(np.full((options[&#39;max_frames&#39;], 1), cls_idx).tolist())
                else:
                    y_cls.append(cls_idx)
            cls_idx += 1
            self.peg.append(peg_idx)
    instructions[&#39;instructions&#39;] = instr
    instructions[&#39;parameters&#39;] = options
    self.y_cls = y_cls

    return instructions</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.CreateDTS.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, strict_object)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self, strict_object):

    self.dataloader.load_data(strict_object=strict_object)

    self.zip_params = json.loads(strict_object.json())
    self.file_folder = self.dataloader.file_folder

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.trds.Dataloader"><code class="flex name class">
<span>class <span class="ident">Dataloader</span></span>
<span>(</span><span>path='/tmp/tmpzehv5syy', trds_path='/content/drive/MyDrive/TerraAI/datasets')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataloader(object):

    def __init__(self, path=mkdtemp(), trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;):

        self.file_folder: str = &#39;&#39;
        self.save_path = path
        self.trds_path = trds_path
        self.django_flag = False

        pass

    def _get_zipfiles(self) -&gt; list:

        return os.listdir(os.path.join(self.trds_path, &#39;sources&#39;))

    @staticmethod
    def unzip(file_folder: str, zip_name: str):

        file_path = pathlib.Path(os.path.join(file_folder, &#39;tmp&#39;, zip_name))
        temp_folder = os.path.join(file_folder, &#39;tmp&#39;)
        os.makedirs(temp_folder, exist_ok=True)
        shutil.unpack_archive(file_path, file_folder)
        shutil.rmtree(temp_folder, ignore_errors=True)

        pass

    @staticmethod
    def download(link: str, file_folder: str, file_name: str):

        resp = requests.get(link, stream=True)
        total = int(resp.headers.get(&#39;content-length&#39;, 0))
        idx = 0
        with open(os.path.join(file_folder, &#39;tmp&#39;, file_name), &#39;wb&#39;) as out_file, tqdm(
                desc=f&#34;Загрузка архива {file_name}&#34;, total=total, unit=&#39;iB&#39;, unit_scale=True,
                unit_divisor=1024) as progress_bar:
            for data in resp.iter_content(chunk_size=1024):
                size = out_file.write(data)
                progress_bar.update(size)
                idx += size
                # if self.django_flag:
                #     if idx % 143360 == 0 or idx == progress_bar.total:
                #         progress_bar_status = (progress_bar.desc, str(round(idx / progress_bar.total, 2)),
                #                            f&#39;{str(round(progress_bar.last_print_t - progress_bar.start_t, 2))} сек.&#39;)
                #         if idx == progress_bar.total:
                #             self.Exch.print_progress_bar(progress_bar_status, stop_flag=True)
                #         else:
                #             self.Exch.print_progress_bar(progress_bar_status)

        pass

    def load_data(self, strict_object):

        if strict_object.mode == SourceModeChoice.Terra:
            self.load_from_terra(strict_object.value)
        elif strict_object.mode == SourceModeChoice.URL:
            self.load_from_url(strict_object.value)
        elif strict_object.mode == SourceModeChoice.GoogleDrive:
            self.load_from_google(strict_object.value)

        pass

    def load_from_terra(self, name: str):

        file_folder = None
        data = {
            &#39;трейдинг&#39;: [&#39;trading.zip&#39;],
            &#39;автомобили&#39;: [&#39;cars.zip&#39;],
            &#39;умный_дом&#39;: [&#39;smart_home.zip&#39;],
            &#39;квартиры&#39;: [&#39;flats.zip&#39;],
            # &#39;диалоги&#39;: [&#39;dialog.txt&#39;],
            &#39;автомобили_3&#39;: [&#39;cars_3.zip&#39;],
            &#39;заболевания&#39;: [&#39;diseases.zip&#39;],
            &#39;договоры&#39;: [&#39;docs.zip&#39;],
            &#39;самолеты&#39;: [&#39;planes.zip&#39;],
            # &#39;болезни&#39;: [&#39;origin.zip&#39;, &#39;segmentation.zip&#39;],
            &#39;губы&#39;: [&#39;lips.zip&#39;],
            # &#39;жанры_музыки&#39;: [&#39;genres.zip&#39;],
            &#39;sber&#39;: [&#39;sber.zip&#39;]
        }

        for file_name in data[name]:
            file_folder = pathlib.Path(self.save_path).joinpath(name)
            os.makedirs(file_folder, exist_ok=True)
            os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
            link = &#39;https://storage.googleapis.com/terra_ai/DataSets/Numpy/&#39; + file_name
            self.download(link, file_folder, file_name)
            if &#39;zip&#39; in file_name:
                self.unzip(file_folder, file_name)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass

    def load_from_url(self, link: str):

        file_name = link.split(&#39;/&#39;)[-1]
        file_folder = pathlib.Path(os.path.join(self.save_path, file_name))
        if &#39;.&#39; in file_name:
            name = file_name[:file_name.rfind(&#39;.&#39;)]
            file_folder = pathlib.Path(os.path.join(self.save_path, name))
        os.makedirs(file_folder, exist_ok=True)
        os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
        self.download(link, file_folder, file_name)
        if &#39;zip&#39; in file_name or &#39;zip&#39; in link:
            self.unzip(file_folder, file_name)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass

    def load_from_google(self, filepath: str):

        zip_name = str(filepath).split(&#39;/&#39;)[-1]
        name = zip_name[:zip_name.rfind(&#39;.&#39;)]
        file_folder = os.path.join(self.save_path, name)
        shutil.unpack_archive(filepath, file_folder)
        self.file_folder = str(file_folder)
        if not self.django_flag:
            print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

        pass</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="terra_ai.trds.Dataloader.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>link: str, file_folder: str, file_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def download(link: str, file_folder: str, file_name: str):

    resp = requests.get(link, stream=True)
    total = int(resp.headers.get(&#39;content-length&#39;, 0))
    idx = 0
    with open(os.path.join(file_folder, &#39;tmp&#39;, file_name), &#39;wb&#39;) as out_file, tqdm(
            desc=f&#34;Загрузка архива {file_name}&#34;, total=total, unit=&#39;iB&#39;, unit_scale=True,
            unit_divisor=1024) as progress_bar:
        for data in resp.iter_content(chunk_size=1024):
            size = out_file.write(data)
            progress_bar.update(size)
            idx += size
            # if self.django_flag:
            #     if idx % 143360 == 0 or idx == progress_bar.total:
            #         progress_bar_status = (progress_bar.desc, str(round(idx / progress_bar.total, 2)),
            #                            f&#39;{str(round(progress_bar.last_print_t - progress_bar.start_t, 2))} сек.&#39;)
            #         if idx == progress_bar.total:
            #             self.Exch.print_progress_bar(progress_bar_status, stop_flag=True)
            #         else:
            #             self.Exch.print_progress_bar(progress_bar_status)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Dataloader.unzip"><code class="name flex">
<span>def <span class="ident">unzip</span></span>(<span>file_folder: str, zip_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def unzip(file_folder: str, zip_name: str):

    file_path = pathlib.Path(os.path.join(file_folder, &#39;tmp&#39;, zip_name))
    temp_folder = os.path.join(file_folder, &#39;tmp&#39;)
    os.makedirs(temp_folder, exist_ok=True)
    shutil.unpack_archive(file_path, file_folder)
    shutil.rmtree(temp_folder, ignore_errors=True)

    pass</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.trds.Dataloader.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, strict_object)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self, strict_object):

    if strict_object.mode == SourceModeChoice.Terra:
        self.load_from_terra(strict_object.value)
    elif strict_object.mode == SourceModeChoice.URL:
        self.load_from_url(strict_object.value)
    elif strict_object.mode == SourceModeChoice.GoogleDrive:
        self.load_from_google(strict_object.value)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Dataloader.load_from_google"><code class="name flex">
<span>def <span class="ident">load_from_google</span></span>(<span>self, filepath: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_google(self, filepath: str):

    zip_name = str(filepath).split(&#39;/&#39;)[-1]
    name = zip_name[:zip_name.rfind(&#39;.&#39;)]
    file_folder = os.path.join(self.save_path, name)
    shutil.unpack_archive(filepath, file_folder)
    self.file_folder = str(file_folder)
    if not self.django_flag:
        print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Dataloader.load_from_terra"><code class="name flex">
<span>def <span class="ident">load_from_terra</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_terra(self, name: str):

    file_folder = None
    data = {
        &#39;трейдинг&#39;: [&#39;trading.zip&#39;],
        &#39;автомобили&#39;: [&#39;cars.zip&#39;],
        &#39;умный_дом&#39;: [&#39;smart_home.zip&#39;],
        &#39;квартиры&#39;: [&#39;flats.zip&#39;],
        # &#39;диалоги&#39;: [&#39;dialog.txt&#39;],
        &#39;автомобили_3&#39;: [&#39;cars_3.zip&#39;],
        &#39;заболевания&#39;: [&#39;diseases.zip&#39;],
        &#39;договоры&#39;: [&#39;docs.zip&#39;],
        &#39;самолеты&#39;: [&#39;planes.zip&#39;],
        # &#39;болезни&#39;: [&#39;origin.zip&#39;, &#39;segmentation.zip&#39;],
        &#39;губы&#39;: [&#39;lips.zip&#39;],
        # &#39;жанры_музыки&#39;: [&#39;genres.zip&#39;],
        &#39;sber&#39;: [&#39;sber.zip&#39;]
    }

    for file_name in data[name]:
        file_folder = pathlib.Path(self.save_path).joinpath(name)
        os.makedirs(file_folder, exist_ok=True)
        os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
        link = &#39;https://storage.googleapis.com/terra_ai/DataSets/Numpy/&#39; + file_name
        self.download(link, file_folder, file_name)
        if &#39;zip&#39; in file_name:
            self.unzip(file_folder, file_name)
    self.file_folder = str(file_folder)
    if not self.django_flag:
        print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Dataloader.load_from_url"><code class="name flex">
<span>def <span class="ident">load_from_url</span></span>(<span>self, link: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_url(self, link: str):

    file_name = link.split(&#39;/&#39;)[-1]
    file_folder = pathlib.Path(os.path.join(self.save_path, file_name))
    if &#39;.&#39; in file_name:
        name = file_name[:file_name.rfind(&#39;.&#39;)]
        file_folder = pathlib.Path(os.path.join(self.save_path, name))
    os.makedirs(file_folder, exist_ok=True)
    os.makedirs(os.path.join(file_folder, &#39;tmp&#39;), exist_ok=True)
    self.download(link, file_folder, file_name)
    if &#39;zip&#39; in file_name or &#39;zip&#39; in link:
        self.unzip(file_folder, file_name)
    self.file_folder = str(file_folder)
    if not self.django_flag:
        print(f&#39;Файлы скачаны в директорию {self.file_folder}&#39;)

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.trds.PrepareDTS"><code class="flex name class">
<span>class <span class="ident">PrepareDTS</span></span>
<span>(</span><span>trds_path='/content/drive/MyDrive/TerraAI/datasets')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PrepareDTS(object):

    def __init__(self, trds_path=&#39;/content/drive/MyDrive/TerraAI/datasets&#39;):

        self.name: str = &#39;&#39;
        self.source: str = &#39;&#39;
        self.language = None
        self.trds_path: str = trds_path
        self.input_shape: dict = {}
        self.input_dtype: dict = {}
        self.input_datatype: str = &#39;&#39;
        self.input_names: dict = {}
        self.output_shape: dict = {}
        self.output_dtype: dict = {}
        self.output_datatype: dict = {}
        self.output_names: dict = {}
        self.split_sequence: dict = {}
        self.file_folder: str = &#39;&#39;
        self.use_generator: bool = False
        self.zip_params: dict = {}
        self.instructions: dict = {&#39;inputs&#39;: {}, &#39;outputs&#39;: {}}
        self.tags: dict = {}
        self.task_type: dict = {}
        self.one_hot_encoding: dict = {}
        self.num_classes: dict = {}
        self.classes_names: dict = {}
        self.classes_colors: dict = {}
        self.dts_prepared: bool = False

        self.dataloader = None
        self.createarray = CreateArray()

        self.X: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.Y: dict = {&#39;train&#39;: {}, &#39;val&#39;: {}, &#39;test&#39;: {}}
        self.dataset: dict = {}

        pass

    @staticmethod
    def _set_datatype(shape) -&gt; str:

        datatype = {0: &#39;DIM&#39;,
                    1: &#39;DIM&#39;,
                    2: &#39;DIM&#39;,
                    3: &#39;1D&#39;,
                    4: &#39;2D&#39;,
                    5: &#39;3D&#39;
                    }

        return datatype[len(shape)]

    @staticmethod
    def _set_language(name: str):

        language = {&#39;imdb&#39;: &#39;English&#39;,
                    &#39;boston_housing&#39;: &#39;English&#39;,
                    &#39;reuters&#39;: &#39;English&#39;,
                    &#39;заболевания&#39;: &#39;Russian&#39;,
                    &#39;договоры&#39;: &#39;Russian&#39;,
                    &#39;умный_дом&#39;: &#39;Russian&#39;,
                    &#39;квартиры&#39;: &#39;Russian&#39;
                    }

        if name in language.keys():
            return language[name]
        else:
            return None

    def generator_train(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;train&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def generator_val(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;val&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def generator_test(self):

        inputs = {}
        outputs = {}
        for idx in self.split_sequence[&#39;test&#39;]:
            for key in self.instructions[&#39;inputs&#39;].keys():
                inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
            for key in self.instructions[&#39;outputs&#39;].keys():
                if &#39;object_detection&#39; in self.tags.values():
                    arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                    for i in range(3):
                        outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
                else:
                    outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                        self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                        **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

            yield inputs, outputs

    def keras_datasets(self, dataset: str, **options):

        self.name = dataset.lower()
        tags = {&#39;mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;fashion_mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;cifar10&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;cifar100&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;imdb&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;},
                &#39;boston_housing&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;regression&#39;},
                &#39;reuters&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;}}
        self.tags = tags[self.name]
        self.source = &#39;tensorflow.keras&#39;
        data = {
            &#39;mnist&#39;: mnist,
            &#39;fashion_mnist&#39;: fashion_mnist,
            &#39;cifar10&#39;: cifar10,
            &#39;cifar100&#39;: cifar100,
            &#39;imdb&#39;: imdb,
            &#39;reuters&#39;: reuters,
            &#39;boston_housing&#39;: boston_housing
        }
        (x_train, y_train), (x_val, y_val) = data[self.name].load_data()

        self.language = self._set_language(self.name)
        if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
            self.num_classes[&#39;output_1&#39;] = len(np.unique(y_train, axis=0))
            if self.name == &#39;fashion_mnist&#39;:
                self.classes_names[&#39;output_1&#39;] = [&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,
                                                  &#39;Shirt&#39;,
                                                  &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]
            elif self.name == &#39;cifar10&#39;:
                self.classes_names[&#39;output_1&#39;] = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;,
                                                  &#39;horse&#39;, &#39;ship&#39;,
                                                  &#39;truck&#39;]
            else:
                self.classes_names[&#39;output_1&#39;] = [str(i) for i in range(len(np.unique(y_train, axis=0)))]
        else:
            self.num_classes[&#39;output_1&#39;] = 1

        if &#39;net&#39; in options.keys() and self.name in list(data.keys())[:4]:
            if options[&#39;net&#39;].lower() == &#39;linear&#39;:
                x_train = x_train.reshape((-1, np.prod(np.array(x_train.shape)[1:])))
                x_val = x_val.reshape((-1, np.prod(np.array(x_val.shape)[1:])))
            elif options[&#39;net&#39;].lower() == &#39;conv&#39;:
                if len(x_train.shape) == 3:
                    x_train = x_train[..., None]
                    x_val = x_val[..., None]

        if &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39; or \
                &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:

            shape_xt = x_train.shape
            shape_xv = x_val.shape
            x_train = x_train.reshape(-1, 1)
            x_val = x_val.reshape(-1, 1)

            if options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39;:
                self.createarray.scaler[&#39;input_1&#39;] = MinMaxScaler()
                if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                    self.createarray.scaler[&#39;output_1&#39;] = MinMaxScaler()

            elif options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:
                self.createarray.scaler[&#39;input_1&#39;] = StandardScaler()
                if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                    self.createarray.scaler[&#39;output_1&#39;] = StandardScaler()

            self.createarray.scaler[&#39;input_1&#39;].fit(x_train)
            x_train = self.createarray.scaler[&#39;input_1&#39;].transform(x_train)
            x_val = self.createarray.scaler[&#39;input_1&#39;].transform(x_val)
            x_train = x_train.reshape(shape_xt)
            x_val = x_val.reshape(shape_xv)

            if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                shape_yt = y_train.shape
                shape_yv = y_val.shape
                y_train = y_train.reshape(-1, 1)
                y_val = y_val.reshape(-1, 1)
                self.createarray.scaler[&#39;output_1&#39;].fit(y_train)
                y_train = self.createarray.scaler[&#39;output_1&#39;].transform(y_train)
                y_val = self.createarray.scaler[&#39;output_1&#39;].transform(y_val)
                y_train = y_train.reshape(shape_yt)
                y_val = y_val.reshape(shape_yv)
        else:
            self.createarray.scaler[&#39;output_1&#39;] = None

        self.one_hot_encoding[&#39;output_1&#39;] = False
        if &#39;one_hot_encoding&#39; in options.keys() and options[&#39;one_hot_encoding&#39;] is True:
            if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
                y_train = utils.to_categorical(y_train, len(np.unique(y_train, axis=0)))
                y_val = utils.to_categorical(y_val, len(np.unique(y_val, axis=0)))
                self.one_hot_encoding[&#39;output_1&#39;] = True

        self.input_shape[&#39;input_1&#39;] = x_train.shape if len(x_train.shape) &lt; 2 else x_train.shape[1:]
        self.input_datatype = self._set_datatype(shape=x_train.shape)
        self.input_names[&#39;input_1&#39;] = &#39;Вход&#39;
        self.output_shape[&#39;output_1&#39;] = y_train.shape[1:]
        self.output_datatype[&#39;output_1&#39;] = self._set_datatype(shape=y_train.shape)
        self.output_names[&#39;input_1&#39;] = &#39;Выход&#39;

        x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, shuffle=True)
        self.X[&#39;train&#39;][&#39;input_1&#39;] = x_train
        self.X[&#39;val&#39;][&#39;input_1&#39;] = x_val
        self.X[&#39;test&#39;][&#39;input_1&#39;] = x_test
        self.Y[&#39;train&#39;][&#39;output_1&#39;] = y_train
        self.Y[&#39;val&#39;][&#39;output_1&#39;] = y_val
        self.Y[&#39;test&#39;][&#39;output_1&#39;] = y_test

        self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
        self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
        self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

        return self

    def prepare_dataset(self, dataset_name: str, source: str):

        def load_arrays():

            for sample in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;)):
                for arr in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample)):
                    if &#39;input&#39; in arr:
                        self.X[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                            os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))
                    elif &#39;output&#39; in arr:
                        self.Y[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                            os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))

            pass

        def load_scalers():

            scalers = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;scalers&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    scalers.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in scalers:
                    self.createarray.scaler[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.scaler[put] = None

            pass

        def load_tokenizer():

            tokenizer = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;tokenizer&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    tokenizer.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in tokenizer:
                    self.createarray.tokenizer[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.tokenizer[put] = None

            pass

        def load_word2vec():

            word2v = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;word2vec&#39;)
            if os.path.exists(folder_path):
                for arr in os.listdir(folder_path):
                    word2v.append(arr[:-3])

            for put in list(self.tags.keys()):
                if put in word2v:
                    self.createarray.word2vec[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.word2vec[put] = None

            pass

        def load_augmentation():

            augmentation = []
            folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;augmentation&#39;)
            if os.path.exists(folder_path):
                for aug in os.listdir(folder_path):
                    augmentation.append(aug[:-3])

            for put in list(self.tags.keys()):
                if put in augmentation:
                    self.createarray.augmentation[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
                else:
                    self.createarray.augmentation[put] = None

            pass

        if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;, &#39;imdb&#39;, &#39;boston_housing&#39;, &#39;reuters&#39;] and \
                source != &#39;custom_dataset&#39;:
            if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;]:
                self.keras_datasets(dataset_name, one_hot_encoding=True, scaler=&#39;MinMaxScaler&#39;, net=&#39;conv&#39;)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;imdb&#39;:
                self.keras_datasets(dataset_name, one_hot_encoding=True)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;reuters&#39;:
                self.keras_datasets(dataset_name)
                self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
            elif dataset_name == &#39;boston_housing&#39;:
                self.keras_datasets(dataset_name, scaler=&#39;StandardScaler&#39;)
                self.task_type[&#39;output_1&#39;] = &#39;regression&#39;
        elif source == &#39;custom_dataset&#39;:
            with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;config.json&#39;), &#39;r&#39;) as cfg:
                data = json.load(cfg)
            for key, value in data.items():
                self.__dict__[key] = value
            if self.use_generator:
                if &#39;text&#39; in self.tags.values():
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                              &#39;r&#39;) as txt:
                        self.createarray.txt_list = json.load(txt)

                self.dataloader = Dataloader()
                self.dataloader.load_data(strict_object=SourceData(**self.zip_params))

                with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                          &#39;r&#39;) as cfg:
                    self.split_sequence = json.load(cfg)
                for inp in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                                   &#39;inputs&#39;)):
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;inputs&#39;, inp),
                              &#39;r&#39;) as cfg:
                        data = json.load(cfg)
                    self.instructions[&#39;inputs&#39;][inp[:inp.rfind(&#39;.&#39;)]] = data
                for out in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                                   &#39;outputs&#39;)):
                    with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;outputs&#39;, out),
                              &#39;r&#39;) as cfg:
                        data = json.load(cfg)
                    self.instructions[&#39;outputs&#39;][out[:out.rfind(&#39;.&#39;)]] = data
                self.createarray.file_folder = self.dataloader.file_folder

                self.dataset[&#39;train&#39;] = Dataset.from_generator(self.generator_train,
                                                               output_shapes=(self.input_shape, self.output_shape),
                                                               output_types=(self.input_dtype, self.output_dtype))
                self.dataset[&#39;val&#39;] = Dataset.from_generator(self.generator_val,
                                                             output_shapes=(self.input_shape, self.output_shape),
                                                             output_types=(self.input_dtype, self.output_dtype))
                self.dataset[&#39;test&#39;] = Dataset.from_generator(self.generator_test,
                                                              output_shapes=(self.input_shape, self.output_shape),
                                                              output_types=(self.input_dtype, self.output_dtype))
            else:
                load_arrays()

                self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
                self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
                self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

        load_scalers()
        load_tokenizer()
        load_word2vec()
        load_augmentation()

        self.dts_prepared = True

        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.trds.PrepareDTS.generator_test"><code class="name flex">
<span>def <span class="ident">generator_test</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generator_test(self):

    inputs = {}
    outputs = {}
    for idx in self.split_sequence[&#39;test&#39;]:
        for key in self.instructions[&#39;inputs&#39;].keys():
            inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
        for key in self.instructions[&#39;outputs&#39;].keys():
            if &#39;object_detection&#39; in self.tags.values():
                arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                for i in range(3):
                    outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
            else:
                outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

        yield inputs, outputs</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.PrepareDTS.generator_train"><code class="name flex">
<span>def <span class="ident">generator_train</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generator_train(self):

    inputs = {}
    outputs = {}
    for idx in self.split_sequence[&#39;train&#39;]:
        for key in self.instructions[&#39;inputs&#39;].keys():
            inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
        for key in self.instructions[&#39;outputs&#39;].keys():
            if &#39;object_detection&#39; in self.tags.values():
                arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                for i in range(3):
                    outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
            else:
                outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

        yield inputs, outputs</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.PrepareDTS.generator_val"><code class="name flex">
<span>def <span class="ident">generator_val</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generator_val(self):

    inputs = {}
    outputs = {}
    for idx in self.split_sequence[&#39;val&#39;]:
        for key in self.instructions[&#39;inputs&#39;].keys():
            inputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                self.instructions[&#39;inputs&#39;][key][&#39;instructions&#39;][idx],
                **self.instructions[&#39;inputs&#39;][key][&#39;parameters&#39;])
        for key in self.instructions[&#39;outputs&#39;].keys():
            if &#39;object_detection&#39; in self.tags.values():
                arrays = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])
                for i in range(3):
                    outputs[f&#39;output_{int(key[-1])+i}&#39;] = np.array(arrays[i])
            else:
                outputs[key] = getattr(self.createarray, f&#34;create_{self.tags[key]}&#34;)(
                    self.instructions[&#39;outputs&#39;][key][&#39;instructions&#39;][idx],
                    **self.instructions[&#39;outputs&#39;][key][&#39;parameters&#39;])

        yield inputs, outputs</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.PrepareDTS.keras_datasets"><code class="name flex">
<span>def <span class="ident">keras_datasets</span></span>(<span>self, dataset: str, **options)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keras_datasets(self, dataset: str, **options):

    self.name = dataset.lower()
    tags = {&#39;mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
            &#39;fashion_mnist&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
            &#39;cifar10&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
            &#39;cifar100&#39;: {&#39;input_1&#39;: &#39;images&#39;, &#39;output_1&#39;: &#39;classification&#39;},
            &#39;imdb&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;},
            &#39;boston_housing&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;regression&#39;},
            &#39;reuters&#39;: {&#39;input_1&#39;: &#39;text&#39;, &#39;output_1&#39;: &#39;classification&#39;}}
    self.tags = tags[self.name]
    self.source = &#39;tensorflow.keras&#39;
    data = {
        &#39;mnist&#39;: mnist,
        &#39;fashion_mnist&#39;: fashion_mnist,
        &#39;cifar10&#39;: cifar10,
        &#39;cifar100&#39;: cifar100,
        &#39;imdb&#39;: imdb,
        &#39;reuters&#39;: reuters,
        &#39;boston_housing&#39;: boston_housing
    }
    (x_train, y_train), (x_val, y_val) = data[self.name].load_data()

    self.language = self._set_language(self.name)
    if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
        self.num_classes[&#39;output_1&#39;] = len(np.unique(y_train, axis=0))
        if self.name == &#39;fashion_mnist&#39;:
            self.classes_names[&#39;output_1&#39;] = [&#39;T - shirt / top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;,
                                              &#39;Shirt&#39;,
                                              &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]
        elif self.name == &#39;cifar10&#39;:
            self.classes_names[&#39;output_1&#39;] = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;,
                                              &#39;horse&#39;, &#39;ship&#39;,
                                              &#39;truck&#39;]
        else:
            self.classes_names[&#39;output_1&#39;] = [str(i) for i in range(len(np.unique(y_train, axis=0)))]
    else:
        self.num_classes[&#39;output_1&#39;] = 1

    if &#39;net&#39; in options.keys() and self.name in list(data.keys())[:4]:
        if options[&#39;net&#39;].lower() == &#39;linear&#39;:
            x_train = x_train.reshape((-1, np.prod(np.array(x_train.shape)[1:])))
            x_val = x_val.reshape((-1, np.prod(np.array(x_val.shape)[1:])))
        elif options[&#39;net&#39;].lower() == &#39;conv&#39;:
            if len(x_train.shape) == 3:
                x_train = x_train[..., None]
                x_val = x_val[..., None]

    if &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39; or \
            &#39;scaler&#39; in options.keys() and options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:

        shape_xt = x_train.shape
        shape_xv = x_val.shape
        x_train = x_train.reshape(-1, 1)
        x_val = x_val.reshape(-1, 1)

        if options[&#39;scaler&#39;] == &#39;MinMaxScaler&#39;:
            self.createarray.scaler[&#39;input_1&#39;] = MinMaxScaler()
            if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                self.createarray.scaler[&#39;output_1&#39;] = MinMaxScaler()

        elif options[&#39;scaler&#39;] == &#39;StandardScaler&#39;:
            self.createarray.scaler[&#39;input_1&#39;] = StandardScaler()
            if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
                self.createarray.scaler[&#39;output_1&#39;] = StandardScaler()

        self.createarray.scaler[&#39;input_1&#39;].fit(x_train)
        x_train = self.createarray.scaler[&#39;input_1&#39;].transform(x_train)
        x_val = self.createarray.scaler[&#39;input_1&#39;].transform(x_val)
        x_train = x_train.reshape(shape_xt)
        x_val = x_val.reshape(shape_xv)

        if &#39;classification&#39; not in self.tags[&#39;output_1&#39;]:
            shape_yt = y_train.shape
            shape_yv = y_val.shape
            y_train = y_train.reshape(-1, 1)
            y_val = y_val.reshape(-1, 1)
            self.createarray.scaler[&#39;output_1&#39;].fit(y_train)
            y_train = self.createarray.scaler[&#39;output_1&#39;].transform(y_train)
            y_val = self.createarray.scaler[&#39;output_1&#39;].transform(y_val)
            y_train = y_train.reshape(shape_yt)
            y_val = y_val.reshape(shape_yv)
    else:
        self.createarray.scaler[&#39;output_1&#39;] = None

    self.one_hot_encoding[&#39;output_1&#39;] = False
    if &#39;one_hot_encoding&#39; in options.keys() and options[&#39;one_hot_encoding&#39;] is True:
        if &#39;classification&#39; in self.tags[&#39;output_1&#39;]:
            y_train = utils.to_categorical(y_train, len(np.unique(y_train, axis=0)))
            y_val = utils.to_categorical(y_val, len(np.unique(y_val, axis=0)))
            self.one_hot_encoding[&#39;output_1&#39;] = True

    self.input_shape[&#39;input_1&#39;] = x_train.shape if len(x_train.shape) &lt; 2 else x_train.shape[1:]
    self.input_datatype = self._set_datatype(shape=x_train.shape)
    self.input_names[&#39;input_1&#39;] = &#39;Вход&#39;
    self.output_shape[&#39;output_1&#39;] = y_train.shape[1:]
    self.output_datatype[&#39;output_1&#39;] = self._set_datatype(shape=y_train.shape)
    self.output_names[&#39;input_1&#39;] = &#39;Выход&#39;

    x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, shuffle=True)
    self.X[&#39;train&#39;][&#39;input_1&#39;] = x_train
    self.X[&#39;val&#39;][&#39;input_1&#39;] = x_val
    self.X[&#39;test&#39;][&#39;input_1&#39;] = x_test
    self.Y[&#39;train&#39;][&#39;output_1&#39;] = y_train
    self.Y[&#39;val&#39;][&#39;output_1&#39;] = y_val
    self.Y[&#39;test&#39;][&#39;output_1&#39;] = y_test

    self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
    self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
    self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

    return self</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.PrepareDTS.prepare_dataset"><code class="name flex">
<span>def <span class="ident">prepare_dataset</span></span>(<span>self, dataset_name: str, source: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_dataset(self, dataset_name: str, source: str):

    def load_arrays():

        for sample in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;)):
            for arr in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample)):
                if &#39;input&#39; in arr:
                    self.X[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                        os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))
                elif &#39;output&#39; in arr:
                    self.Y[sample][arr[:arr.rfind(&#39;.&#39;)]] = joblib.load(
                        os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;arrays&#39;, sample, arr))

        pass

    def load_scalers():

        scalers = []
        folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;scalers&#39;)
        if os.path.exists(folder_path):
            for arr in os.listdir(folder_path):
                scalers.append(arr[:-3])

        for put in list(self.tags.keys()):
            if put in scalers:
                self.createarray.scaler[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
            else:
                self.createarray.scaler[put] = None

        pass

    def load_tokenizer():

        tokenizer = []
        folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;tokenizer&#39;)
        if os.path.exists(folder_path):
            for arr in os.listdir(folder_path):
                tokenizer.append(arr[:-3])

        for put in list(self.tags.keys()):
            if put in tokenizer:
                self.createarray.tokenizer[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
            else:
                self.createarray.tokenizer[put] = None

        pass

    def load_word2vec():

        word2v = []
        folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;word2vec&#39;)
        if os.path.exists(folder_path):
            for arr in os.listdir(folder_path):
                word2v.append(arr[:-3])

        for put in list(self.tags.keys()):
            if put in word2v:
                self.createarray.word2vec[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
            else:
                self.createarray.word2vec[put] = None

        pass

    def load_augmentation():

        augmentation = []
        folder_path = os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;augmentation&#39;)
        if os.path.exists(folder_path):
            for aug in os.listdir(folder_path):
                augmentation.append(aug[:-3])

        for put in list(self.tags.keys()):
            if put in augmentation:
                self.createarray.augmentation[put] = joblib.load(os.path.join(folder_path, f&#39;{put}.gz&#39;))
            else:
                self.createarray.augmentation[put] = None

        pass

    if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;, &#39;imdb&#39;, &#39;boston_housing&#39;, &#39;reuters&#39;] and \
            source != &#39;custom_dataset&#39;:
        if dataset_name in [&#39;mnist&#39;, &#39;fashion_mnist&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;]:
            self.keras_datasets(dataset_name, one_hot_encoding=True, scaler=&#39;MinMaxScaler&#39;, net=&#39;conv&#39;)
            self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
        elif dataset_name == &#39;imdb&#39;:
            self.keras_datasets(dataset_name, one_hot_encoding=True)
            self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
        elif dataset_name == &#39;reuters&#39;:
            self.keras_datasets(dataset_name)
            self.task_type[&#39;output_1&#39;] = &#39;classification&#39;
        elif dataset_name == &#39;boston_housing&#39;:
            self.keras_datasets(dataset_name, scaler=&#39;StandardScaler&#39;)
            self.task_type[&#39;output_1&#39;] = &#39;regression&#39;
    elif source == &#39;custom_dataset&#39;:
        with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;config.json&#39;), &#39;r&#39;) as cfg:
            data = json.load(cfg)
        for key, value in data.items():
            self.__dict__[key] = value
        if self.use_generator:
            if &#39;text&#39; in self.tags.values():
                with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;txt_list.json&#39;),
                          &#39;r&#39;) as txt:
                    self.createarray.txt_list = json.load(txt)

            self.dataloader = Dataloader()
            self.dataloader.load_data(strict_object=SourceData(**self.zip_params))

            with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;sequence.json&#39;),
                      &#39;r&#39;) as cfg:
                self.split_sequence = json.load(cfg)
            for inp in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                               &#39;inputs&#39;)):
                with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;inputs&#39;, inp),
                          &#39;r&#39;) as cfg:
                    data = json.load(cfg)
                self.instructions[&#39;inputs&#39;][inp[:inp.rfind(&#39;.&#39;)]] = data
            for out in os.listdir(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;,
                                               &#39;outputs&#39;)):
                with open(os.path.join(self.trds_path, f&#39;dataset {dataset_name}&#39;, &#39;instructions&#39;, &#39;outputs&#39;, out),
                          &#39;r&#39;) as cfg:
                    data = json.load(cfg)
                self.instructions[&#39;outputs&#39;][out[:out.rfind(&#39;.&#39;)]] = data
            self.createarray.file_folder = self.dataloader.file_folder

            self.dataset[&#39;train&#39;] = Dataset.from_generator(self.generator_train,
                                                           output_shapes=(self.input_shape, self.output_shape),
                                                           output_types=(self.input_dtype, self.output_dtype))
            self.dataset[&#39;val&#39;] = Dataset.from_generator(self.generator_val,
                                                         output_shapes=(self.input_shape, self.output_shape),
                                                         output_types=(self.input_dtype, self.output_dtype))
            self.dataset[&#39;test&#39;] = Dataset.from_generator(self.generator_test,
                                                          output_shapes=(self.input_shape, self.output_shape),
                                                          output_types=(self.input_dtype, self.output_dtype))
        else:
            load_arrays()

            self.dataset[&#39;train&#39;] = Dataset.from_tensor_slices((self.X[&#39;train&#39;], self.Y[&#39;train&#39;]))
            self.dataset[&#39;val&#39;] = Dataset.from_tensor_slices((self.X[&#39;val&#39;], self.Y[&#39;val&#39;]))
            self.dataset[&#39;test&#39;] = Dataset.from_tensor_slices((self.X[&#39;test&#39;], self.Y[&#39;test&#39;]))

    load_scalers()
    load_tokenizer()
    load_word2vec()
    load_augmentation()

    self.dts_prepared = True

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="terra_ai.trds.Preprocessing"><code class="flex name class">
<span>class <span class="ident">Preprocessing</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Preprocessing(object):

    def __init__(self):

        self.scaler: dict = {}
        self.tokenizer: dict = {}
        self.word2vec: dict = {}

    def create_scaler(self):

        pass

    def create_tokenizer(self, mode: str, iteration: int, **options):

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            **options: Параметры токенайзера:
                       num_words: int
                           Количество слов для токенайзера.
                       filters: str
                           Символы, подлежащие удалению.
                       lower: bool
                           Перевод заглавных букв в строчные.
                       split: str
                           Символ разделения.
                       char_level: bool
                           Учёт каждого символа в качестве отдельного токена.
                       oov_token: str
                           В случае указания этот токен будет заменять все слова, не попавшие в
                           диапазон частотности слов 0 &lt; num_words.

        Returns:
            Объект Токенайзер.

        &#34;&#34;&#34;

        self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

        pass

    def create_word2vec(self, mode: str, iteration: int, x_word: list, **options) -&gt; None:

        &#34;&#34;&#34;

        Args:
            mode: str
                Режим input/output.
            iteration: int
                Номер входа или выхода.
            x_word: list
                Список слов для обучения Word2Vec.
            **options: Параметры Word2Vec:
                       size: int
                           Dimensionality of the word vectors.
                       window: int
                           Maximum distance between the current and predicted word within a sentence.
                       min_count: int
                           Ignores all words with total frequency lower than this.
                       workers: int
                           Use these many worker threads to train the model (=faster training with multicore machines).
                       iter: int
                           Number of iterations (epochs) over the corpus.

        Returns:
            Объект Word2Vec.

        &#34;&#34;&#34;

        self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(x_word, **options)

        pass

    def inverse_data(self, put: str, array: np.ndarray):

        &#34;&#34;&#34;

        Args:
            put: str
                Рассматриваемый вход или выход (input_2, output_1);
            array: np.ndarray
                NumPy массив, подлежащий возврату в исходное состояние.

        Returns:
            Данные в исходном состоянии.

        &#34;&#34;&#34;

        inverted_data = None

        for attr in self.__dict__.keys():
            if self.__dict__[attr] and put in self.__dict__[attr].keys():
                if attr == &#39;tokenizer&#39;:
                    if array.shape[0] == self.tokenizer[put].num_words:
                        idx = 0
                        arr = []
                        for num in array:
                            if num == 1:
                                arr.append(idx)
                            idx += 1
                        array = np.array(arr)
                    inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                    inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

                elif attr == &#39;word2vec&#39;:
                    text_list = []
                    for i in range(len(array)):
                        text_list.append(
                            self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                    inverted_data = &#39; &#39;.join(text_list)

                elif attr == &#39;scaler&#39;:
                    original_shape = array.shape
                    array = array.reshape(-1, 1)
                    array = self.scaler[put].inverse_transform(array)
                    inverted_data = array.reshape(original_shape)
            break

        return inverted_data</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="terra_ai.trds.Preprocessing.create_scaler"><code class="name flex">
<span>def <span class="ident">create_scaler</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_scaler(self):

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Preprocessing.create_tokenizer"><code class="name flex">
<span>def <span class="ident">create_tokenizer</span></span>(<span>self, mode: str, iteration: int, **options)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры токенайзера:
num_words: int
Количество слов для токенайзера.
filters: str
Символы, подлежащие удалению.
lower: bool
Перевод заглавных букв в строчные.
split: str
Символ разделения.
char_level: bool
Учёт каждого символа в качестве отдельного токена.
oov_token: str
В случае указания этот токен будет заменять все слова, не попавшие в
диапазон частотности слов 0 &lt; num_words.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Токенайзер.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tokenizer(self, mode: str, iteration: int, **options):

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        **options: Параметры токенайзера:
                   num_words: int
                       Количество слов для токенайзера.
                   filters: str
                       Символы, подлежащие удалению.
                   lower: bool
                       Перевод заглавных букв в строчные.
                   split: str
                       Символ разделения.
                   char_level: bool
                       Учёт каждого символа в качестве отдельного токена.
                   oov_token: str
                       В случае указания этот токен будет заменять все слова, не попавшие в
                       диапазон частотности слов 0 &lt; num_words.

    Returns:
        Объект Токенайзер.

    &#34;&#34;&#34;

    self.tokenizer[f&#39;{mode}_{iteration}&#39;] = Tokenizer(**options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Preprocessing.create_word2vec"><code class="name flex">
<span>def <span class="ident">create_word2vec</span></span>(<span>self, mode: str, iteration: int, x_word: list, **options) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>mode</code></strong></dt>
<dd>str
Режим input/output.</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int
Номер входа или выхода.</dd>
<dt><strong><code>x_word</code></strong></dt>
<dd>list
Список слов для обучения Word2Vec.</dd>
<dt><strong><code>**options</code></strong></dt>
<dd>Параметры Word2Vec:
size: int
Dimensionality of the word vectors.
window: int
Maximum distance between the current and predicted word within a sentence.
min_count: int
Ignores all words with total frequency lower than this.
workers: int
Use these many worker threads to train the model (=faster training with multicore machines).
iter: int
Number of iterations (epochs) over the corpus.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Объект Word2Vec.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_word2vec(self, mode: str, iteration: int, x_word: list, **options) -&gt; None:

    &#34;&#34;&#34;

    Args:
        mode: str
            Режим input/output.
        iteration: int
            Номер входа или выхода.
        x_word: list
            Список слов для обучения Word2Vec.
        **options: Параметры Word2Vec:
                   size: int
                       Dimensionality of the word vectors.
                   window: int
                       Maximum distance between the current and predicted word within a sentence.
                   min_count: int
                       Ignores all words with total frequency lower than this.
                   workers: int
                       Use these many worker threads to train the model (=faster training with multicore machines).
                   iter: int
                       Number of iterations (epochs) over the corpus.

    Returns:
        Объект Word2Vec.

    &#34;&#34;&#34;

    self.word2vec[f&#39;{mode}_{iteration}&#39;] = Word2Vec(x_word, **options)

    pass</code></pre>
</details>
</dd>
<dt id="terra_ai.trds.Preprocessing.inverse_data"><code class="name flex">
<span>def <span class="ident">inverse_data</span></span>(<span>self, put: str, array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>put</code></strong></dt>
<dd>str
Рассматриваемый вход или выход (input_2, output_1);</dd>
<dt><strong><code>array</code></strong></dt>
<dd>np.ndarray
NumPy массив, подлежащий возврату в исходное состояние.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Данные в исходном состоянии.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_data(self, put: str, array: np.ndarray):

    &#34;&#34;&#34;

    Args:
        put: str
            Рассматриваемый вход или выход (input_2, output_1);
        array: np.ndarray
            NumPy массив, подлежащий возврату в исходное состояние.

    Returns:
        Данные в исходном состоянии.

    &#34;&#34;&#34;

    inverted_data = None

    for attr in self.__dict__.keys():
        if self.__dict__[attr] and put in self.__dict__[attr].keys():
            if attr == &#39;tokenizer&#39;:
                if array.shape[0] == self.tokenizer[put].num_words:
                    idx = 0
                    arr = []
                    for num in array:
                        if num == 1:
                            arr.append(idx)
                        idx += 1
                    array = np.array(arr)
                inv_tokenizer = {index: word for word, index in self.tokenizer[put].word_index.items()}
                inverted_data = &#39; &#39;.join([inv_tokenizer[seq] for seq in array])

            elif attr == &#39;word2vec&#39;:
                text_list = []
                for i in range(len(array)):
                    text_list.append(
                        self.word2vec[put].wv.most_similar(positive=np.expand_dims(array[i], axis=0), topn=1)[0][0])
                inverted_data = &#39; &#39;.join(text_list)

            elif attr == &#39;scaler&#39;:
                original_shape = array.shape
                array = array.reshape(-1, 1)
                array = self.scaler[put].inverse_transform(array)
                inverted_data = array.reshape(original_shape)
        break

    return inverted_data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="terra_ai" href="index.html">terra_ai</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="terra_ai.trds.CreateArray" href="#terra_ai.trds.CreateArray">CreateArray</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.trds.CreateArray.create_audio" href="#terra_ai.trds.CreateArray.create_audio">create_audio</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_classification" href="#terra_ai.trds.CreateArray.create_classification">create_classification</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_dataframe" href="#terra_ai.trds.CreateArray.create_dataframe">create_dataframe</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_images" href="#terra_ai.trds.CreateArray.create_images">create_images</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_object_detection" href="#terra_ai.trds.CreateArray.create_object_detection">create_object_detection</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_regression" href="#terra_ai.trds.CreateArray.create_regression">create_regression</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_scaler" href="#terra_ai.trds.CreateArray.create_scaler">create_scaler</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_segmentation" href="#terra_ai.trds.CreateArray.create_segmentation">create_segmentation</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_text" href="#terra_ai.trds.CreateArray.create_text">create_text</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_text_segmentation" href="#terra_ai.trds.CreateArray.create_text_segmentation">create_text_segmentation</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_timeseries" href="#terra_ai.trds.CreateArray.create_timeseries">create_timeseries</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_tokenizer" href="#terra_ai.trds.CreateArray.create_tokenizer">create_tokenizer</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_video" href="#terra_ai.trds.CreateArray.create_video">create_video</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.create_word2vec" href="#terra_ai.trds.CreateArray.create_word2vec">create_word2vec</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.imgaug_to_yolo" href="#terra_ai.trds.CreateArray.imgaug_to_yolo">imgaug_to_yolo</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.inverse_data" href="#terra_ai.trds.CreateArray.inverse_data">inverse_data</a></code></li>
<li><code><a title="terra_ai.trds.CreateArray.yolo_to_imgaug" href="#terra_ai.trds.CreateArray.yolo_to_imgaug">yolo_to_imgaug</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.trds.CreateDTS" href="#terra_ai.trds.CreateDTS">CreateDTS</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.trds.CreateDTS.create_dataset" href="#terra_ai.trds.CreateDTS.create_dataset">create_dataset</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_audio" href="#terra_ai.trds.CreateDTS.instructions_audio">instructions_audio</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_classification" href="#terra_ai.trds.CreateDTS.instructions_classification">instructions_classification</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_dataframe" href="#terra_ai.trds.CreateDTS.instructions_dataframe">instructions_dataframe</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_images" href="#terra_ai.trds.CreateDTS.instructions_images">instructions_images</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_object_detection" href="#terra_ai.trds.CreateDTS.instructions_object_detection">instructions_object_detection</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_regression" href="#terra_ai.trds.CreateDTS.instructions_regression">instructions_regression</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_segmentation" href="#terra_ai.trds.CreateDTS.instructions_segmentation">instructions_segmentation</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_text" href="#terra_ai.trds.CreateDTS.instructions_text">instructions_text</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_text_segmentation" href="#terra_ai.trds.CreateDTS.instructions_text_segmentation">instructions_text_segmentation</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.instructions_video" href="#terra_ai.trds.CreateDTS.instructions_video">instructions_video</a></code></li>
<li><code><a title="terra_ai.trds.CreateDTS.load_data" href="#terra_ai.trds.CreateDTS.load_data">load_data</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.trds.Dataloader" href="#terra_ai.trds.Dataloader">Dataloader</a></code></h4>
<ul class="two-column">
<li><code><a title="terra_ai.trds.Dataloader.download" href="#terra_ai.trds.Dataloader.download">download</a></code></li>
<li><code><a title="terra_ai.trds.Dataloader.load_data" href="#terra_ai.trds.Dataloader.load_data">load_data</a></code></li>
<li><code><a title="terra_ai.trds.Dataloader.load_from_google" href="#terra_ai.trds.Dataloader.load_from_google">load_from_google</a></code></li>
<li><code><a title="terra_ai.trds.Dataloader.load_from_terra" href="#terra_ai.trds.Dataloader.load_from_terra">load_from_terra</a></code></li>
<li><code><a title="terra_ai.trds.Dataloader.load_from_url" href="#terra_ai.trds.Dataloader.load_from_url">load_from_url</a></code></li>
<li><code><a title="terra_ai.trds.Dataloader.unzip" href="#terra_ai.trds.Dataloader.unzip">unzip</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.trds.PrepareDTS" href="#terra_ai.trds.PrepareDTS">PrepareDTS</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.trds.PrepareDTS.generator_test" href="#terra_ai.trds.PrepareDTS.generator_test">generator_test</a></code></li>
<li><code><a title="terra_ai.trds.PrepareDTS.generator_train" href="#terra_ai.trds.PrepareDTS.generator_train">generator_train</a></code></li>
<li><code><a title="terra_ai.trds.PrepareDTS.generator_val" href="#terra_ai.trds.PrepareDTS.generator_val">generator_val</a></code></li>
<li><code><a title="terra_ai.trds.PrepareDTS.keras_datasets" href="#terra_ai.trds.PrepareDTS.keras_datasets">keras_datasets</a></code></li>
<li><code><a title="terra_ai.trds.PrepareDTS.prepare_dataset" href="#terra_ai.trds.PrepareDTS.prepare_dataset">prepare_dataset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="terra_ai.trds.Preprocessing" href="#terra_ai.trds.Preprocessing">Preprocessing</a></code></h4>
<ul class="">
<li><code><a title="terra_ai.trds.Preprocessing.create_scaler" href="#terra_ai.trds.Preprocessing.create_scaler">create_scaler</a></code></li>
<li><code><a title="terra_ai.trds.Preprocessing.create_tokenizer" href="#terra_ai.trds.Preprocessing.create_tokenizer">create_tokenizer</a></code></li>
<li><code><a title="terra_ai.trds.Preprocessing.create_word2vec" href="#terra_ai.trds.Preprocessing.create_word2vec">create_word2vec</a></code></li>
<li><code><a title="terra_ai.trds.Preprocessing.inverse_data" href="#terra_ai.trds.Preprocessing.inverse_data">inverse_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>